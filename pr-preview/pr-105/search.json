[
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Team",
    "section": "",
    "text": "Angela Oliveira Pisco\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Director of Computational Biology \n           at \n              \n              Insitro\n              \n          \n        \n      \n      \n      \n        \n          Core Member \n           at \n              \n              Open Problems\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Dorien Roosen\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Scientist \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Dries De Maeyer\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Principal Scientist \n           at \n              \n              Janssen Pharmaceuticals\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Dries Schaumont\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Scientist \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Elizabeth Mlynarski\n      \n      Contributor\n      \n      \n      \n      \n        \n          Principal Scientist Computational Genomics \n           at \n            \n              Janssen R&D US\n            \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Isabelle Bergiers\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Scientist OMICS Technology \n           at \n              \n              Janssen Pharmaceuticals\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Jakub Majercik\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Bioinformatics Engineer \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Kai Waldrant\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Bioinformatician \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n      \n      \n        \n          Contributor \n           at \n              \n              Open Problems\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Malte D. Luecken\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Group Leader \n           at \n              \n              Helmholtz Munich\n              \n          \n        \n      \n      \n      \n        \n          Core Member \n           at \n              \n              Open Problems\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Marijke Van Moerbeke\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Statistical Consultant \n           at \n              \n              OpenAnalytics\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Matthias Beyens\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Principal Scientist \n           at \n              \n              Janssen Pharmaceuticals\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Mauro Saporita\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Lead Nextflow Developer \n           at \n              \n              Ardigen\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Povilas Gibas\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Bioinformatician \n           at \n              \n              Ardigen\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Robrecht Cannoodt\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Science Engineer \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n      \n      \n        \n          Core Member \n           at \n              \n              Open Problems\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Samuel D'Souza\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Engineer \n           at \n              \n              Chan Zuckerberg Biohub\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Toni Verbeiren\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Scientist and CEO \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Vladimir Shitov\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          PhD Candidate \n           at \n              \n              Helmholtz Munich\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Weiwei Schultz\n      \n      Contributor\n      \n      \n      \n      \n        \n          Associate Director Data Sciences \n           at \n            \n              Janssen R&D US\n            \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Xichen Wu\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Student Assistant \n           at \n              \n              Helmholtz Munich\n              \n          \n        \n      \n       \n    \n  \n  \n\n\nNo matching items"
  },
  {
    "objectID": "user_guide/parameter_lists.html",
    "href": "user_guide/parameter_lists.html",
    "title": "Parameter lists",
    "section": "",
    "text": "Using the Viash VDSL3 Nextflow platform, an optional --param_list argument can be passed to OpenPipelines workflows. The --param_list argument enables passing multiple inputs to a workflow, resulting in a multi-event nextflow channel.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Parameter lists"
    ]
  },
  {
    "objectID": "user_guide/parameter_lists.html#json-file",
    "href": "user_guide/parameter_lists.html#json-file",
    "title": "Parameter lists",
    "section": "JSON file",
    "text": "JSON file\nThe following example shows how to use a json file as a parameter list.\n$ cat param_list.json\n[\n    {\n        \"id\": \"foo\",\n        \"input\": \"foo.txt\",\n        \"event_param\": \"lorem\"\n    },\n    {\n        \"id\": \"bar\",\n        \"input\": \"bar.txt\",\n        \"event_param\": \"ipsum\"\n    }\n]\nnextflow run ... --param_list param_list.json",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Parameter lists"
    ]
  },
  {
    "objectID": "user_guide/parameter_lists.html#csv-file",
    "href": "user_guide/parameter_lists.html#csv-file",
    "title": "Parameter lists",
    "section": "CSV file",
    "text": "CSV file\nThe following example shows how to use a csv file as a parameter list.\n$ cat param_list.csv\nid,input,event_param\nfoo,foo.txt,lorem\nbar,bar.txt,ipsum\nnextflow run ... --param_list param_list.csv",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Parameter lists"
    ]
  },
  {
    "objectID": "user_guide/index.html",
    "href": "user_guide/index.html",
    "title": "User guide",
    "section": "",
    "text": "Getting started: Setting up infrastructure\n  \n  \n  \n    Running pipelines: Run a pipeline from CLI or Nextflow Tower\n  \n  \n  \n    Parameter lists: Passing multiple inputs to a workflow\n  \n  \n  \n    Ingestion: From sequencing to count tables\n  \n  \n  \n    Processing: From count tables to integrated data\n  \n  \n  \n    Downstream: Celltyping and cell-cell communication\n  \n  \n  \n    Bug reports: How to report bugs\n  \n  \n\n\nNo matching items",
    "crumbs": [
      "Fundamentals",
      "User guide"
    ]
  },
  {
    "objectID": "user_guide/bug_reports.html",
    "href": "user_guide/bug_reports.html",
    "title": "Bug reports",
    "section": "",
    "text": "Issues with Openpipelines are being tracked on Github. In order for an issue to be fixed in a timely manner, creating a complete and reproducable is essential.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Bug reports"
    ]
  },
  {
    "objectID": "user_guide/getting_started.html",
    "href": "user_guide/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "Depending on whether you plan to run the OpenPipelines workflows locally or in the cloud",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user_guide/getting_started.html#starting-workflows-locally",
    "href": "user_guide/getting_started.html#starting-workflows-locally",
    "title": "Getting started",
    "section": "Starting workflows locally",
    "text": "Starting workflows locally\nIf you want to start workflows locally, you will need to install Nextflow.\n\nInstall Docker (optional)\nDocker is a containerization platform that allows you to package your application and all its dependencies into a single image. It is used to run the analysis pipelines.\nIf you are planning on running the workflows locally, you will need to install Docker. You do not need to install Docker if the workflows will be run in the cloud using AWS Batch, Azure Batch, Google Cloud Batch, or other cloud-based compute environments.\nTo install Docker, follow the instructions here.\n\n\nInstall Java\nNextflow requires Java 11 or later. To check if Java is installed on your system, run:\njava -version\nIf Java is not installed, you can download it from here.\n\n\nInstall Nextflow\nNextflow is distributed as a single executable file. To install it, run:\ncurl -s https://get.nextflow.io | bash\nThis command will download the latest version of Nextflow and store it in the current directory.\nTo install Nextflow system-wide, move the downloaded file to a directory in your $PATH, e.g.:\nmv nextflow /usr/local/bin\n\n\nTest the installation\nTo test the installation, run:\nnextflow run hello -with-docker",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user_guide/getting_started.html#using-nextflow-tower",
    "href": "user_guide/getting_started.html#using-nextflow-tower",
    "title": "Getting started",
    "section": "Using Nextflow Tower",
    "text": "Using Nextflow Tower\nNextflow Tower is a web-based user interface for running and monitoring Nextflow pipelines. If you are planning on using Nextflow Tower, a compute environment will need to be set up.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Getting started"
    ]
  },
  {
    "objectID": "more_information/code_of_conduct.html",
    "href": "more_information/code_of_conduct.html",
    "title": "Code of conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\nOur full Code of Conduct is adapted from the Contributor Covenant, version 2.1.",
    "crumbs": [
      "Fundamentals",
      "More information",
      "Code of conduct"
    ]
  },
  {
    "objectID": "more_information/faq.html",
    "href": "more_information/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "It is possible to add additional resources such as a file containing helper functions or other resources. All you need to do is list those files under the functionality.resources section of your component and refer to them in your script using meta[\"resources_dir\"] + \"/myresource.txt\". Please visit the Viash documentation for concrete examples on how to add helper functions and other resources to your component.",
    "crumbs": [
      "Fundamentals",
      "More information",
      "FAQ"
    ]
  },
  {
    "objectID": "more_information/faq.html#how-can-i-add-an-external-resource-to-my-viash-component",
    "href": "more_information/faq.html#how-can-i-add-an-external-resource-to-my-viash-component",
    "title": "FAQ",
    "section": "",
    "text": "It is possible to add additional resources such as a file containing helper functions or other resources. All you need to do is list those files under the functionality.resources section of your component and refer to them in your script using meta[\"resources_dir\"] + \"/myresource.txt\". Please visit the Viash documentation for concrete examples on how to add helper functions and other resources to your component.",
    "crumbs": [
      "Fundamentals",
      "More information",
      "FAQ"
    ]
  },
  {
    "objectID": "more_information/faq.html#what-does-__merge__-do",
    "href": "more_information/faq.html#what-does-__merge__-do",
    "title": "FAQ",
    "section": "What does __merge__ do?",
    "text": "What does __merge__ do?\nThe __merge__ field is used to merge another YAML into a Viash config. One of its uses is in making sure that all of the components in a task has the same API.\nEach task in OpenProblems contains strict definitions of the input/output file interface of its components and the file formats of those files. These interfaces are stored as YAML files in the api subdirectory of each task.",
    "crumbs": [
      "Fundamentals",
      "More information",
      "FAQ"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html",
    "href": "components/workflows/qc/qc.html",
    "title": "Qc",
    "section": "",
    "text": "ID: qc\nNamespace: workflows/qc\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html#example-commands",
    "href": "components/workflows/qc/qc.html#example-commands",
    "title": "Qc",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/qc/qc/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\nmodality: \"rna\"\n# layer: \"raw_counts\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Mitochondrial Gene Detection\n# var_name_mitochondrial_genes: \"foo\"\n# obs_name_mitochondrial_fraction: \"foo\"\n# var_gene_names: \"gene_symbol\"\nmitochondrial_gene_regex: \"^[mM][tT]-\"\n\n# QC metrics calculation options\n# var_qc_metrics: [\"ercc,highly_variable\"]\ntop_n_vars: [50, 100, 200, 500]\noutput_obs_num_nonzero_vars: \"num_nonzero_vars\"\noutput_obs_total_counts_vars: \"total_counts\"\noutput_var_num_nonzero_obs: \"num_nonzero_obs\"\noutput_var_total_counts_obs: \"total_counts\"\noutput_var_obs_mean: \"obs_mean\"\noutput_var_pct_dropout: \"pct_dropout\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/qc/qc/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html#argument-groups",
    "href": "components/workflows/qc/qc.html#argument-groups",
    "title": "Qc",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"input.h5mu\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n--layer\nLayer to calculate qc metrics for.\nstring, example: \"raw_counts\"\n\n\n\n\n\nMitochondrial Gene Detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_name_mitochondrial_genes\nIn which .var slot to store a boolean array corresponding the mitochondrial genes.\nstring\n\n\n--obs_name_mitochondrial_fraction\n.Obs slot to store the fraction of reads found to be mitochondrial. Defaults to ‘fraction_’ suffixed by the value of –var_name_mitochondrial_genes\nstring\n\n\n--var_gene_names\n.var column name to be used to detect mitochondrial genes instead of .var_names (default if not set). Gene names matching with the regex value from –mitochondrial_gene_regex will be identified as a mitochondrial gene.\nstring, example: \"gene_symbol\"\n\n\n--mitochondrial_gene_regex\nRegex string that identifies mitochondrial genes from –var_gene_names. By default will detect human and mouse mitochondrial genes from a gene symbol.\nstring, default: \"^[mM][tT]-\"\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes. Defaults to the value from –var_name_mitochondrial_genes.\nList of string, example: \"ercc,highly_variable\", multiple_sep: \";\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \";\"\n\n\n--output_obs_num_nonzero_vars\nName of column in .obs describing, for each observation, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each row the number of columns that contain data.\nstring, default: \"num_nonzero_vars\"\n\n\n--output_obs_total_counts_vars\nName of the column for .obs describing, for each observation (row), the sum of the stored values in the columns.\nstring, default: \"total_counts\"\n\n\n--output_var_num_nonzero_obs\nName of column describing, for each feature, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each column the number of rows that contain data.\nstring, default: \"num_nonzero_obs\"\n\n\n--output_var_total_counts_obs\nName of the column in .var describing, for each feature (column), the sum of the stored values in the rows.\nstring, default: \"total_counts\"\n\n\n--output_var_obs_mean\nName of the column in .obs providing the mean of the values in each row.\nstring, default: \"obs_mean\"\n\n\n--output_var_pct_dropout\nName of the column in .obs providing for each feature the percentage of observations the feature does not appear on (i.e. is missing). Same as --output_var_num_nonzero_obs but percentage based.\nstring, default: \"pct_dropout\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html#authors",
    "href": "components/workflows/qc/qc.html#authors",
    "title": "Qc",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html#visualisation",
    "href": "components/workflows/qc/qc.html#visualisation",
    "title": "Qc",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v20(grep_annotation_column)\n    v33(mix)\n    v42(calculate_qc_metrics)\n    v62(publish)\n    v90(Output)\n    v0--&gt;v2\n    v2--&gt;v20\n    v20--&gt;v33\n    v2--&gt;v33\n    v33--&gt;v42\n    v42--&gt;v62\n    v62--&gt;v90\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v20 fill:#e3dcea,stroke:#7a4baa;\n    style v33 fill:#e3dcea,stroke:#7a4baa;\n    style v42 fill:#e3dcea,stroke:#7a4baa;\n    style v62 fill:#e3dcea,stroke:#7a4baa;\n    style v90 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html",
    "href": "components/workflows/integration/harmony_leiden.html",
    "title": "Harmony leiden",
    "section": "",
    "text": "ID: harmony_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html#example-commands",
    "href": "components/workflows/integration/harmony_leiden.html#example-commands",
    "title": "Harmony leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/integration/harmony_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Neighbour calculation\nuns_neighbors: \"harmonypy_integration_neighbors\"\nobsp_neighbor_distances: \"harmonypy_integration_distances\"\nobsp_neighbor_connectivities: \"harmonypy_integration_connectivities\"\n\n# Harmony integration options\nembedding: \"X_pca\"\nobsm_integrated: \"X_pca_integrated\"\nobs_covariates: # please fill in - example: [\"batch\", \"sample\"]\ntheta: [2.0]\n\n# Clustering options\nobs_cluster: \"harmony_integration_leiden\"\nleiden_resolution: [1.0]\n\n# Umap options\nobsm_umap: \"X_leiden_harmony_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/harmony_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html#argument-groups",
    "href": "components/workflows/integration/harmony_leiden.html#argument-groups",
    "title": "Harmony leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nNeighbour calculation\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"harmonypy_integration_neighbors\"\n\n\n--obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"harmonypy_integration_distances\"\n\n\n--obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"harmonypy_integration_connectivities\"\n\n\n\n\n\nHarmony integration options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--embedding\nEmbedding to use as input\nstring, default: \"X_pca\"\n\n\n--obsm_integrated\nIn which .obsm slot to store the resulting integrated embedding.\nstring, default: \"X_pca_integrated\"\n\n\n--obs_covariates\nThe .obs field(s) that define the covariate(s) to regress out.\nList of string, required, example: \"batch\", \"sample\", multiple_sep: \";\"\n\n\n--theta\nDiversity clustering penalty parameter. Specify for each variable in group.by.vars. theta=0 does not encourage any diversity. Larger values of theta result in more diverse clusters.”\nList of double, default: 2, multiple_sep: \";\"\n\n\n\n\n\nClustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"harmony_integration_leiden\"\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_leiden_harmony_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html#authors",
    "href": "components/workflows/integration/harmony_leiden.html#authors",
    "title": "Harmony leiden",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html#visualisation",
    "href": "components/workflows/integration/harmony_leiden.html#visualisation",
    "title": "Harmony leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(harmonypy)\n    v49(concat)\n    v38(find_neighbors)\n    v59(leiden)\n    v79(move_obsm_to_obs)\n    v92(mix)\n    v101(umap)\n    v128(Output)\n    v0--&gt;v18\n    v18--&gt;v38\n    v38--&gt;v49\n    v49--&gt;v59\n    v59--&gt;v79\n    v79--&gt;v92\n    v49--&gt;v92\n    v92--&gt;v101\n    v101--&gt;v128\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v38 fill:#e3dcea,stroke:#7a4baa;\n    style v59 fill:#e3dcea,stroke:#7a4baa;\n    style v79 fill:#e3dcea,stroke:#7a4baa;\n    style v92 fill:#e3dcea,stroke:#7a4baa;\n    style v101 fill:#e3dcea,stroke:#7a4baa;\n    style v128 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html",
    "href": "components/workflows/integration/scanorama_leiden.html",
    "title": "Scanorama leiden",
    "section": "",
    "text": "ID: scanorama_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html#example-commands",
    "href": "components/workflows/integration/scanorama_leiden.html#example-commands",
    "title": "Scanorama leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/integration/scanorama_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Neighbour calculation\nuns_neighbors: \"scanorama_integration_neighbors\"\nobsp_neighbor_distances: \"scanorama_integration_distances\"\nobsp_neighbor_connectivities: \"scanorama_integration_connectivities\"\n\n# Scanorama integration options\nobs_batch: \"sample_id\"\nobsm_input: \"X_pca\"\nobsm_output: \"X_scanorama\"\nknn: 20\nbatch_size: 5000\nsigma: 15.0\napprox: true\nalpha: 0.1\n\n# Clustering options\nobs_cluster: \"scanorama_integration_leiden\"\nleiden_resolution: [1.0]\n\n# Umap options\nobsm_umap: \"X_leiden_scanorama_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/scanorama_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html#argument-groups",
    "href": "components/workflows/integration/scanorama_leiden.html#argument-groups",
    "title": "Scanorama leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nNeighbour calculation\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"scanorama_integration_neighbors\"\n\n\n--obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"scanorama_integration_distances\"\n\n\n--obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"scanorama_integration_connectivities\"\n\n\n\n\n\nScanorama integration options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_batch\nColumn name discriminating between your batches.\nstring, default: \"sample_id\"\n\n\n--obsm_input\n.obsm slot that points to embedding to run scanorama on.\nstring, default: \"X_pca\"\n\n\n--obsm_output\nThe name of the field in adata.obsm where the integrated embeddings will be stored after running this function. Defaults to X_scanorama.\nstring, default: \"X_scanorama\"\n\n\n--knn\nNumber of nearest neighbors to use for matching.\ninteger, default: 20\n\n\n--batch_size\nThe batch size used in the alignment vector computation. Useful when integrating very large (&gt;100k samples) datasets. Set to large value that runs within available memory.\ninteger, default: 5000\n\n\n--sigma\nCorrection smoothing parameter on Gaussian kernel.\ndouble, default: 15\n\n\n--approx\nUse approximate nearest neighbors with Python annoy; greatly speeds up matching runtime.\nboolean, default: TRUE\n\n\n--alpha\nAlignment score minimum cutoff\ndouble, default: 0.1\n\n\n\n\n\nClustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions specified in ‘–leiden_resolution’.\nstring, default: \"scanorama_integration_leiden\"\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_leiden_scanorama_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html#authors",
    "href": "components/workflows/integration/scanorama_leiden.html#authors",
    "title": "Scanorama leiden",
    "section": "Authors",
    "text": "Authors\n\nMauro Saporita   (author)\nPovilas Gibas   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html#visualisation",
    "href": "components/workflows/integration/scanorama_leiden.html#visualisation",
    "title": "Scanorama leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(scanorama)\n    v49(concat)\n    v38(find_neighbors)\n    v59(leiden)\n    v79(move_obsm_to_obs)\n    v92(mix)\n    v101(umap)\n    v128(Output)\n    v0--&gt;v18\n    v18--&gt;v38\n    v38--&gt;v49\n    v49--&gt;v59\n    v59--&gt;v79\n    v79--&gt;v92\n    v49--&gt;v92\n    v92--&gt;v101\n    v101--&gt;v128\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v38 fill:#e3dcea,stroke:#7a4baa;\n    style v59 fill:#e3dcea,stroke:#7a4baa;\n    style v79 fill:#e3dcea,stroke:#7a4baa;\n    style v92 fill:#e3dcea,stroke:#7a4baa;\n    style v101 fill:#e3dcea,stroke:#7a4baa;\n    style v128 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html",
    "href": "components/workflows/gdo/gdo_singlesample.html",
    "title": "Gdo singlesample",
    "section": "",
    "text": "ID: gdo_singlesample\nNamespace: workflows/gdo\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "Gdo singlesample"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html#example-commands",
    "href": "components/workflows/gdo/gdo_singlesample.html#example-commands",
    "title": "Gdo singlesample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/gdo/gdo_singlesample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Input\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\n\n# Output\n# output: \"$id.$key.output.h5mu\"\n\n# Filtering options\n# min_counts: 200\n# max_counts: 5000000\n# min_guides_per_cell: 200\n# max_guides_per_cell: 1500000\n# min_cells_per_guide: 3\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/gdo/gdo_singlesample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "Gdo singlesample"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html#argument-groups",
    "href": "components/workflows/gdo/gdo_singlesample.html#argument-groups",
    "title": "Gdo singlesample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nInput layer to start from. By default, .X will be used.\nstring\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nFiltering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--min_counts\nMinimum number of counts captured per cell.\ninteger, example: 200\n\n\n--max_counts\nMaximum number of counts captured per cell.\ninteger, example: 5000000\n\n\n--min_guides_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--max_guides_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 1500000\n\n\n--min_cells_per_guide\nMinimum of non-zero values per gene.\ninteger, example: 3",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "Gdo singlesample"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html#authors",
    "href": "components/workflows/gdo/gdo_singlesample.html#authors",
    "title": "Gdo singlesample",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "Gdo singlesample"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html#visualisation",
    "href": "components/workflows/gdo/gdo_singlesample.html#visualisation",
    "title": "Gdo singlesample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(gdo_filter_with_counts)\n    v38(gdo_do_filter)\n    v66(Output)\n    v0--&gt;v18\n    v18--&gt;v38\n    v38--&gt;v66\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v38 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "Gdo singlesample"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html",
    "href": "components/workflows/ingestion/make_reference.html",
    "title": "Make reference",
    "section": "",
    "text": "ID: make_reference\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html#example-commands",
    "href": "components/workflows/ingestion/make_reference.html#example-commands",
    "title": "Make reference",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/ingestion/make_reference/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ngenome_fasta: # please fill in - example: \"https:/ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/GRCh38.primary_assembly.genome.fa.gz\"\ntranscriptome_gtf: # please fill in - example: \"https:/ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.annotation.gtf.gz\"\n# ercc: \"https:/assets.thermofisher.com/TFS-Assets/LSG/manuals/ERCC92.zip\"\n\n# Outputs\ntarget: [\"star\"]\n# output_fasta: \"$id.$key.output_fasta.gz\"\n# output_gtf: \"$id.$key.output_gtf.gz\"\n# output_cellranger: \"$id.$key.output_cellranger.gz\"\n# output_bd_rhapsody: \"$id.$key.output_bd_rhapsody.gz\"\n# output_star: \"$id.$key.output_star.gz\"\n\n# Arguments\n# subset_regex: \"(ERCC-00002|chr1)\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/make_reference/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html#argument-groups",
    "href": "components/workflows/ingestion/make_reference.html#argument-groups",
    "title": "Make reference",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the reference.\nstring, required, example: \"foo\"\n\n\n--genome_fasta\nReference genome fasta.\nfile, required, example: \"https:/ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/GRCh38.primary_assembly.genome.fa.gz\"\n\n\n--transcriptome_gtf\nReference transcriptome annotation.\nfile, required, example: \"https:/ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.annotation.gtf.gz\"\n\n\n--ercc\nERCC sequence and annotation file.\nfile, example: \"https:/assets.thermofisher.com/TFS-Assets/LSG/manuals/ERCC92.zip\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--target\nWhich reference indices to generate.\nList of string, default: \"star\", multiple_sep: \";\"\n\n\n--output_fasta\nOutput genome sequence fasta.\nfile, example: \"genome_sequence.fa.gz\"\n\n\n--output_gtf\nOutput transcriptome annotation gtf.\nfile, example: \"transcriptome_annotation.gtf.gz\"\n\n\n--output_cellranger\nOutput index\nfile, example: \"cellranger_index.tar.gz\"\n\n\n--output_bd_rhapsody\nOutput index\nfile, example: \"bdrhap_index.tar.gz\"\n\n\n--output_star\nOutput index\nfile, example: \"star_index.tar.gz\"\n\n\n\n\n\nArguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--subset_regex\nWill subset the reference chromosomes using the given regex.\nstring, example: \"(ERCC-00002&#124;chr1)\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html#authors",
    "href": "components/workflows/ingestion/make_reference.html#authors",
    "title": "Make reference",
    "section": "Authors",
    "text": "Authors\n\nAngela Oliveira Pisco    (author)\nRobrecht Cannoodt    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html#visualisation",
    "href": "components/workflows/ingestion/make_reference.html#visualisation",
    "title": "Make reference",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v28(concat)\n    v17(make_reference_component)\n    v39(build_cellranger_reference)\n    v98(mix)\n    v62(build_bdrhap_reference)\n    v85(star_build_reference)\n    v117(Output)\n    v0--&gt;v17\n    v17--&gt;v28\n    v28--&gt;v39\n    v39--&gt;v98\n    v28--&gt;v62\n    v62--&gt;v98\n    v28--&gt;v85\n    v85--&gt;v98\n    v98--&gt;v117\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v28 fill:#e3dcea,stroke:#7a4baa;\n    style v17 fill:#e3dcea,stroke:#7a4baa;\n    style v39 fill:#e3dcea,stroke:#7a4baa;\n    style v98 fill:#e3dcea,stroke:#7a4baa;\n    style v62 fill:#e3dcea,stroke:#7a4baa;\n    style v85 fill:#e3dcea,stroke:#7a4baa;\n    style v117 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html",
    "href": "components/workflows/ingestion/conversion.html",
    "title": "Conversion",
    "section": "",
    "text": "ID: conversion\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Conversion"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html#example-commands",
    "href": "components/workflows/ingestion/conversion.html#example-commands",
    "title": "Conversion",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/ingestion/conversion/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\ninput_type: # please fill in - example: \"foo\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Conversion from h5ad\n# modality: [\"foo\"]\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/conversion/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Conversion"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html#argument-groups",
    "href": "components/workflows/ingestion/conversion.html#argument-groups",
    "title": "Conversion",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"input.h5mu\"\n\n\n--input_type\nType of the input file\nstring, required\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nName or template for the output files.\nfile, example: \"output.h5mu\"\n\n\n\n\n\nConversion from h5ad\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--modality\nName of the modality where the h5ad is stored in the h5mu object.\nList of string, multiple_sep: \";\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Conversion"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html#authors",
    "href": "components/workflows/ingestion/conversion.html#authors",
    "title": "Conversion",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)\nDries De Maeyer   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Conversion"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html#visualisation",
    "href": "components/workflows/ingestion/conversion.html#visualisation",
    "title": "Conversion",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v19(from_10xh5_to_h5mu)\n    v78(mix)\n    v42(from_h5ad_to_h5mu)\n    v65(from_10xmtx_to_h5mu)\n    v94(Output)\n    v0--&gt;v2\n    v2--&gt;v19\n    v19--&gt;v78\n    v2--&gt;v42\n    v42--&gt;v78\n    v2--&gt;v65\n    v65--&gt;v78\n    v78--&gt;v94\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v19 fill:#e3dcea,stroke:#7a4baa;\n    style v78 fill:#e3dcea,stroke:#7a4baa;\n    style v42 fill:#e3dcea,stroke:#7a4baa;\n    style v65 fill:#e3dcea,stroke:#7a4baa;\n    style v94 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Conversion"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html",
    "href": "components/workflows/ingestion/bd_rhapsody.html",
    "title": "BD Rhapsody",
    "section": "",
    "text": "ID: bd_rhapsody\nNamespace: workflows/ingestion\n\n\n\nSource\nA wrapper for the BD Rhapsody Analysis CWL v1.10.1 pipeline.\nThis pipeline can be used for a targeted analysis (with --mode targeted) or for a whole transcriptome analysis (with --mode wta).\nThe reference_genome and transcriptome_annotation files can be generated with the make_reference pipeline. Alternatively, BD also provides standard references which can be downloaded from these locations:",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html#example-commands",
    "href": "components/workflows/ingestion/bd_rhapsody.html#example-commands",
    "title": "BD Rhapsody",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/ingestion/bd_rhapsody/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nmode: # please fill in - example: \"wta\"\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: [\"input.fastq.gz\"]\nreference: # please fill in - example: [\"reference_genome.tar.gz|reference.fasta\"]\n# transcriptome_annotation: \"transcriptome.gtf\"\n# abseq_reference: [\"abseq_reference.fasta\"]\n# supplemental_reference: [\"supplemental_reference.fasta\"]\nsample_prefix: \"sample\"\n\n# Outputs\n# output_raw: \"$id.$key.output_raw.output_raw\"\n# output_h5mu: \"$id.$key.output_h5mu.h5mu\"\n\n# Putative cell calling settings\n# putative_cell_call: \"mRNA\"\n# exact_cell_count: 10000\ndisable_putative_calling: false\n\n# Subsample arguments\n# subsample: 0.01\n# subsample_seed: 3445\n\n# Multiplex arguments\n# sample_tags_version: \"human\"\n# tag_names: [\"4-mySample\", \"9-myOtherSample\", \"6-alsoThisSample\"]\n\n# VDJ arguments\n# vdj_version: \"human\"\n\n# CWL-runner arguments\nparallel: true\ntimestamps: false\ndryrun: false\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/bd_rhapsody/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html#argument-groups",
    "href": "components/workflows/ingestion/bd_rhapsody.html#argument-groups",
    "title": "BD Rhapsody",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--mode\nWhether to run a whole transcriptome analysis (WTA) or a targeted analysis.\nstring, required, example: \"wta\"\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to your read files in the FASTQ.GZ format. You may specify as many R1/R2 read pairs as you want.\nList of file, required, example: \"input.fastq.gz\", multiple_sep: \";\"\n\n\n--reference\nRefence to map to. For --mode wta, this is the path to STAR index as a tar.gz file. For --mode targeted, this is the path to mRNA reference file for pre-designed, supplemental, or custom panel, in FASTA format\nList of file, required, example: \"reference_genome.tar.gz&#124;reference.fasta\", multiple_sep: \";\"\n\n\n--transcriptome_annotation\nPath to GTF annotation file (only for --mode wta).\nfile, example: \"transcriptome.gtf\"\n\n\n--abseq_reference\nPath to the AbSeq reference file in FASTA format. Only needed if BD AbSeq Ab-Oligos are used.\nList of file, example: \"abseq_reference.fasta\", multiple_sep: \";\"\n\n\n--supplemental_reference\nPath to the supplemental reference file in FASTA format. Only needed if there are additional transgene sequences used in the experiment (only for --mode wta).\nList of file, example: \"supplemental_reference.fasta\", multiple_sep: \";\"\n\n\n--sample_prefix\nSpecify a run name to use as the output file base name. Use only letters, numbers, or hyphens. Do not use special characters or spaces.\nstring, default: \"sample\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output_raw\nThe BD Rhapsody output folder as it comes out of the BD Rhapsody pipeline\nfile, required, example: \"output_dir\"\n\n\n--output_h5mu\nThe converted h5mu file.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nPutative cell calling settings\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--putative_cell_call\nSpecify the dataset to be used for putative cell calling. For putative cell calling using an AbSeq dataset, please provide an AbSeq_Reference fasta file above.\nstring, example: \"mRNA\"\n\n\n--exact_cell_count\nExact cell count - Set a specific number (&gt;=1) of cells as putative, based on those with the highest error-corrected read count\ninteger, example: 10000\n\n\n--disable_putative_calling\nDisable Refined Putative Cell Calling - Determine putative cells using only the basic algorithm (minimum second derivative along the cumulative reads curve). The refined algorithm attempts to remove false positives and recover false negatives, but may not be ideal for certain complex mixtures of cell types. Does not apply if Exact Cell Count is set.\nboolean_true\n\n\n\n\n\nSubsample arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--subsample\nA number &gt;1 or fraction (0 &lt; n &lt; 1) to indicate the number or percentage of reads to subsample.\ndouble, example: 0.01\n\n\n--subsample_seed\nA seed for replicating a previous subsampled run.\ninteger, example: 3445\n\n\n\n\n\nMultiplex arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--sample_tags_version\nSpecify if multiplexed run.\nstring, example: \"human\"\n\n\n--tag_names\nTag_Names (optional) - Specify the tag number followed by ‘-’ and the desired sample name to appear in Sample_Tag_Metrics.csv. Do not use the special characters: &, (), [], {}, &lt;&gt;, ?, |\nList of string, example: \"4-mySample\", \"9-myOtherSample\", \"6-alsoThisSample\", multiple_sep: \";\"\n\n\n\n\n\nVDJ arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--vdj_version\nSpecify if VDJ run.\nstring, example: \"human\"\n\n\n\n\n\nCWL-runner arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--parallel\nRun jobs in parallel.\nboolean, default: TRUE\n\n\n--timestamps\nAdd timestamps to the errors, warnings, and notifications.\nboolean_true\n\n\n--dryrun\nIf true, the output directory will only contain the CWL input files, but the pipeline itself will not be executed.\nboolean_true",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html#authors",
    "href": "components/workflows/ingestion/bd_rhapsody.html#authors",
    "title": "BD Rhapsody",
    "section": "Authors",
    "text": "Authors\n\nRobrecht Cannoodt    (maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html#visualisation",
    "href": "components/workflows/ingestion/bd_rhapsody.html#visualisation",
    "title": "BD Rhapsody",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v17(bd_rhapsody_component)\n    v38(from_bdrhap_to_h5mu)\n    v65(Output)\n    v0--&gt;v17\n    v17--&gt;v38\n    v38--&gt;v65\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v17 fill:#e3dcea,stroke:#7a4baa;\n    style v38 fill:#e3dcea,stroke:#7a4baa;\n    style v65 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html",
    "href": "components/workflows/prot/prot_singlesample.html",
    "title": "Prot singlesample",
    "section": "",
    "text": "ID: prot_singlesample\nNamespace: workflows/prot\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html#example-commands",
    "href": "components/workflows/prot/prot_singlesample.html#example-commands",
    "title": "Prot singlesample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/prot/prot_singlesample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Input\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\n\n# Output\n# output: \"$id.$key.output.h5mu\"\n\n# Filtering options\n# min_counts: 200\n# max_counts: 5000000\n# min_proteins_per_cell: 200\n# max_proteins_per_cell: 1500000\n# min_cells_per_protein: 3\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/prot/prot_singlesample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html#argument-groups",
    "href": "components/workflows/prot/prot_singlesample.html#argument-groups",
    "title": "Prot singlesample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nInput layer to start from. By default, .X will be used.\nstring\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nFiltering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--min_counts\nMinimum number of counts captured per cell.\ninteger, example: 200\n\n\n--max_counts\nMaximum number of counts captured per cell.\ninteger, example: 5000000\n\n\n--min_proteins_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--max_proteins_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 1500000\n\n\n--min_cells_per_protein\nMinimum of non-zero values per gene.\ninteger, example: 3",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html#authors",
    "href": "components/workflows/prot/prot_singlesample.html#authors",
    "title": "Prot singlesample",
    "section": "Authors",
    "text": "Authors\n\nDries De Maeyer   (author)\nRobrecht Cannoodt    (author, maintainer)\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html#visualisation",
    "href": "components/workflows/prot/prot_singlesample.html#visualisation",
    "title": "Prot singlesample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(prot_filter_with_counts)\n    v38(prot_do_filter)\n    v66(Output)\n    v0--&gt;v18\n    v18--&gt;v38\n    v38--&gt;v66\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v38 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html",
    "href": "components/workflows/multiomics/process_batches.html",
    "title": "Process batches",
    "section": "",
    "text": "ID: process_batches\nNamespace: workflows/multiomics\n\n\n\nSource\nAn input .h5mu file will first be split in order to run the multisample processing per modality. Next, the modalities are merged again and the integration setup pipeline is executed. Please note that this workflow assumes that samples from multiple pipelines are already concatenated.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html#example-commands",
    "href": "components/workflows/multiomics/process_batches.html#example-commands",
    "title": "Process batches",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/multiomics/process_batches/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: [\"input.h5mu\"]\n# rna_layer: \"foo\"\n# prot_layer: \"foo\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Highly variable features detection\nhighly_variable_features_var_output: \"filter_with_hvg\"\nhighly_variable_features_obs_batch_key: \"sample_id\"\n\n# QC metrics calculation options\nvar_qc_metrics: [\"filter_with_hvg\"]\ntop_n_vars: [50, 100, 200, 500]\n\n# PCA options\npca_overwrite: false\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/multiomics/process_batches/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html#argument-groups",
    "href": "components/workflows/multiomics/process_batches.html#argument-groups",
    "title": "Process batches",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nList of file, required, example: \"input.h5mu\", multiple_sep: \";\"\n\n\n--rna_layer\nInput layer for the gene expression modality. If not specified, .X is used.\nstring\n\n\n--prot_layer\nInput layer for the antibody capture modality. If not specified, .X is used.\nstring\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nHighly variable features detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--highly_variable_features_var_output\nIn which .var slot to store a boolean array corresponding to the highly variable genes.\nstring, default: \"filter_with_hvg\"\n\n\n--highly_variable_features_obs_batch_key\nIf specified, highly-variable genes are selected within each batch separately and merged. This simple process avoids the selection of batch-specific genes and acts as a lightweight batch correction method.\nstring, default: \"sample_id\"\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes.\nList of string, default: \"filter_with_hvg\", example: \"ercc,highly_variable\", multiple_sep: \";\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \";\"\n\n\n\n\n\nPCA options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--pca_overwrite\nAllow overwriting slots for PCA output.\nboolean_true",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html#authors",
    "href": "components/workflows/multiomics/process_batches.html#authors",
    "title": "Process batches",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html#visualisation",
    "href": "components/workflows/multiomics/process_batches.html#visualisation",
    "title": "Process batches",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v11(filter)\n    v55(flatMap)\n    v150(filter)\n    v181(mix)\n    v370(mix)\n    v275(filter)\n    v306(mix)\n    v372(mix)\n    v388(merge)\n    v404(branch)\n    v478(concat)\n    v482(branch)\n    v556(concat)\n    v565(publish)\n    v593(Output)\n    subgraph group_split_modalities_workflow [split_modalities_workflow]\n        v27(split_modalities_component)\n    end\n    subgraph group_rna_multisample [rna_multisample]\n        v78(normalize_total)\n        v98(log1p)\n        v118(delete_layer)\n        v138(highly_variable_features_scanpy)\n        v168(grep_annotation_column)\n        v190(calculate_qc_metrics)\n        v210(publish)\n    end\n    subgraph group_prot_multisample [prot_multisample]\n        v263(clr)\n        v293(grep_annotation_column)\n        v315(calculate_qc_metrics)\n        v335(publish)\n    end\n    subgraph group_dimensionality_reduction_rna [dimensionality_reduction_rna]\n        v417(pca)\n        v437(find_neighbors)\n        v457(umap)\n    end\n    subgraph group_dimensionality_reduction_prot [dimensionality_reduction_prot]\n        v495(pca)\n        v515(find_neighbors)\n        v535(umap)\n    end\n    v370--&gt;v372\n    v404--&gt;v478\n    v482--&gt;v556\n    v0--&gt;v11\n    v11--&gt;v27\n    v27--&gt;v55\n    v55--&gt;v78\n    v78--&gt;v98\n    v98--&gt;v118\n    v118--&gt;v138\n    v138--&gt;v150\n    v150--&gt;v168\n    v168--&gt;v181\n    v150--&gt;v181\n    v181--&gt;v190\n    v190--&gt;v210\n    v210--&gt;v370\n    v55--&gt;v263\n    v263--&gt;v275\n    v275--&gt;v293\n    v293--&gt;v306\n    v275--&gt;v306\n    v306--&gt;v315\n    v315--&gt;v335\n    v335--&gt;v370\n    v55--&gt;v372\n    v372--&gt;v388\n    v388--&gt;v404\n    v404--&gt;v417\n    v417--&gt;v437\n    v437--&gt;v457\n    v457--&gt;v478\n    v478--&gt;v482\n    v482--&gt;v495\n    v495--&gt;v515\n    v515--&gt;v535\n    v535--&gt;v556\n    v556--&gt;v565\n    v565--&gt;v593\n    style group_split_modalities_workflow fill:#F0F0F0,stroke:#969696;\n    style group_rna_multisample fill:#F0F0F0,stroke:#969696;\n    style group_prot_multisample fill:#F0F0F0,stroke:#969696;\n    style group_dimensionality_reduction_rna fill:#F0F0F0,stroke:#969696;\n    style group_dimensionality_reduction_prot fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v11 fill:#e3dcea,stroke:#7a4baa;\n    style v27 fill:#e3dcea,stroke:#7a4baa;\n    style v55 fill:#e3dcea,stroke:#7a4baa;\n    style v78 fill:#e3dcea,stroke:#7a4baa;\n    style v98 fill:#e3dcea,stroke:#7a4baa;\n    style v118 fill:#e3dcea,stroke:#7a4baa;\n    style v138 fill:#e3dcea,stroke:#7a4baa;\n    style v150 fill:#e3dcea,stroke:#7a4baa;\n    style v168 fill:#e3dcea,stroke:#7a4baa;\n    style v181 fill:#e3dcea,stroke:#7a4baa;\n    style v190 fill:#e3dcea,stroke:#7a4baa;\n    style v210 fill:#e3dcea,stroke:#7a4baa;\n    style v370 fill:#e3dcea,stroke:#7a4baa;\n    style v263 fill:#e3dcea,stroke:#7a4baa;\n    style v275 fill:#e3dcea,stroke:#7a4baa;\n    style v293 fill:#e3dcea,stroke:#7a4baa;\n    style v306 fill:#e3dcea,stroke:#7a4baa;\n    style v315 fill:#e3dcea,stroke:#7a4baa;\n    style v335 fill:#e3dcea,stroke:#7a4baa;\n    style v372 fill:#e3dcea,stroke:#7a4baa;\n    style v388 fill:#e3dcea,stroke:#7a4baa;\n    style v404 fill:#e3dcea,stroke:#7a4baa;\n    style v478 fill:#e3dcea,stroke:#7a4baa;\n    style v417 fill:#e3dcea,stroke:#7a4baa;\n    style v437 fill:#e3dcea,stroke:#7a4baa;\n    style v457 fill:#e3dcea,stroke:#7a4baa;\n    style v482 fill:#e3dcea,stroke:#7a4baa;\n    style v556 fill:#e3dcea,stroke:#7a4baa;\n    style v495 fill:#e3dcea,stroke:#7a4baa;\n    style v515 fill:#e3dcea,stroke:#7a4baa;\n    style v535 fill:#e3dcea,stroke:#7a4baa;\n    style v565 fill:#e3dcea,stroke:#7a4baa;\n    style v593 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html",
    "href": "components/workflows/multiomics/dimensionality_reduction.html",
    "title": "Dimensionality reduction",
    "section": "",
    "text": "ID: dimensionality_reduction\nNamespace: workflows/multiomics\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html#example-commands",
    "href": "components/workflows/multiomics/dimensionality_reduction.html#example-commands",
    "title": "Dimensionality reduction",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/multiomics/dimensionality_reduction/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# PCA options\nobsm_pca: \"X_pca\"\n# var_pca_feature_selection: \"foo\"\npca_overwrite: false\n\n# Neighbour calculation\nuns_neighbors: \"neighbors\"\nobsp_neighbor_distances: \"distances\"\nobsp_neighbor_connectivities: \"connectivities\"\n\n# Umap options\nobsm_umap: \"X_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/multiomics/dimensionality_reduction/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html#argument-groups",
    "href": "components/workflows/multiomics/dimensionality_reduction.html#argument-groups",
    "title": "Dimensionality reduction",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nPCA options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_pca\nIn which .obsm slot to store the resulting PCA embedding.\nstring, default: \"X_pca\"\n\n\n--var_pca_feature_selection\nColumn name in .var matrix that will be used to select which genes to run the PCA on.\nstring\n\n\n--pca_overwrite\nAllow overwriting slots for PCA output.\nboolean_true\n\n\n\n\n\nNeighbour calculation\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"neighbors\"\n\n\n--obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"distances\"\n\n\n--obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"connectivities\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html#authors",
    "href": "components/workflows/multiomics/dimensionality_reduction.html#authors",
    "title": "Dimensionality reduction",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html#visualisation",
    "href": "components/workflows/multiomics/dimensionality_reduction.html#visualisation",
    "title": "Dimensionality reduction",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(pca)\n    v38(find_neighbors)\n    v58(umap)\n    v85(Output)\n    v0--&gt;v18\n    v18--&gt;v38\n    v38--&gt;v58\n    v58--&gt;v85\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v38 fill:#e3dcea,stroke:#7a4baa;\n    style v58 fill:#e3dcea,stroke:#7a4baa;\n    style v85 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html",
    "href": "components/workflows/rna/rna_singlesample.html",
    "title": "Rna singlesample",
    "section": "",
    "text": "ID: rna_singlesample\nNamespace: workflows/rna\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html#example-commands",
    "href": "components/workflows/rna/rna_singlesample.html#example-commands",
    "title": "Rna singlesample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/rna/rna_singlesample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Input\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\n\n# Output\n# output: \"$id.$key.output.h5mu\"\n\n# Filtering options\n# min_counts: 200\n# max_counts: 5000000\n# min_genes_per_cell: 200\n# max_genes_per_cell: 1500000\n# min_cells_per_gene: 3\n# min_fraction_mito: 0.0\n# max_fraction_mito: 0.2\n\n# Mitochondrial gene detection\n# var_name_mitochondrial_genes: \"foo\"\n# obs_name_mitochondrial_fraction: \"foo\"\n# var_gene_names: \"gene_symbol\"\nmitochondrial_gene_regex: \"^[mM][tT]-\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/rna/rna_singlesample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html#argument-groups",
    "href": "components/workflows/rna/rna_singlesample.html#argument-groups",
    "title": "Rna singlesample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nInput layer to start from. By default, .X will be used.\nstring\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nFiltering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--min_counts\nMinimum number of counts captured per cell.\ninteger, example: 200\n\n\n--max_counts\nMaximum number of counts captured per cell.\ninteger, example: 5000000\n\n\n--min_genes_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--max_genes_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 1500000\n\n\n--min_cells_per_gene\nMinimum of non-zero values per gene.\ninteger, example: 3\n\n\n--min_fraction_mito\nMinimum fraction of UMIs that are mitochondrial. Requires –obs_name_mitochondrial_fraction.\ndouble, example: 0\n\n\n--max_fraction_mito\nMaximum fraction of UMIs that are mitochondrial. Requires –obs_name_mitochondrial_fraction.\ndouble, example: 0.2\n\n\n\n\n\nMitochondrial gene detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_name_mitochondrial_genes\nIn which .var slot to store a boolean array corresponding the mitochondrial genes.\nstring\n\n\n--obs_name_mitochondrial_fraction\nWhen specified, write the fraction of counts originating from mitochondrial genes (based on –mitochondrial_gene_regex) to an .obs column with the specified name. Requires –var_name_mitochondrial_genes.\nstring\n\n\n--var_gene_names\n.var column name to be used to detect mitochondrial genes instead of .var_names (default if not set). Gene names matching with the regex value from –mitochondrial_gene_regex will be identified as a mitochondrial gene.\nstring, example: \"gene_symbol\"\n\n\n--mitochondrial_gene_regex\nRegex string that identifies mitochondrial genes from –var_gene_names. By default will detect human and mouse mitochondrial genes from a gene symbol.\nstring, default: \"^[mM][tT]-\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html#authors",
    "href": "components/workflows/rna/rna_singlesample.html#authors",
    "title": "Rna singlesample",
    "section": "Authors",
    "text": "Authors\n\nDries De Maeyer   (author)\nRobrecht Cannoodt    (author, maintainer)\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html#visualisation",
    "href": "components/workflows/rna/rna_singlesample.html#visualisation",
    "title": "Rna singlesample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v11(filter)\n    v42(mix)\n    v97(branch)\n    v113(concat)\n    v102(delimit_fraction)\n    v122(rna_filter_with_counts)\n    v142(rna_do_filter)\n    v162(filter_with_scrublet)\n    v189(Output)\n    subgraph group_qc [qc]\n        v29(grep_annotation_column)\n        v51(calculate_qc_metrics)\n        v71(publish)\n    end\n    v97--&gt;v113\n    v97--&gt;v102\n    v0--&gt;v11\n    v11--&gt;v29\n    v29--&gt;v42\n    v11--&gt;v42\n    v42--&gt;v51\n    v51--&gt;v71\n    v71--&gt;v97\n    v102--&gt;v113\n    v113--&gt;v122\n    v122--&gt;v142\n    v142--&gt;v162\n    v162--&gt;v189\n    style group_qc fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v11 fill:#e3dcea,stroke:#7a4baa;\n    style v29 fill:#e3dcea,stroke:#7a4baa;\n    style v42 fill:#e3dcea,stroke:#7a4baa;\n    style v51 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v97 fill:#e3dcea,stroke:#7a4baa;\n    style v113 fill:#e3dcea,stroke:#7a4baa;\n    style v102 fill:#e3dcea,stroke:#7a4baa;\n    style v122 fill:#e3dcea,stroke:#7a4baa;\n    style v142 fill:#e3dcea,stroke:#7a4baa;\n    style v162 fill:#e3dcea,stroke:#7a4baa;\n    style v189 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/index.html",
    "href": "components/index.html",
    "title": "Reference",
    "section": "",
    "text": "Order By\n       Default\n         \n          Name\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nNamespace\n\n\nDescription\n\n\n\n\n\n\nBD Rhapsody\n\n\nWorkflows/ingestion\n\n\nA generic pipeline for running BD Rhapsody WTA or Targeted mapping, with support for AbSeq, VDJ and/or SMK.\n\n\n\n\nBbknn leiden\n\n\nWorkflows/integration\n\n\nRun bbknn followed by leiden clustering and run umap on the result.\n\n\n\n\nCell Ranger mapping\n\n\nWorkflows/ingestion\n\n\nA pipeline for running Cell Ranger mapping.\n\n\n\n\nCell Ranger multi\n\n\nWorkflows/ingestion\n\n\nA pipeline for running Cell Ranger multi.\n\n\n\n\nCell Ranger post-processing\n\n\nWorkflows/ingestion\n\n\nPost-processing Cell Ranger datasets.\n\n\n\n\nConversion\n\n\nWorkflows/ingestion\n\n\nA pipeline to convert different file formats to .h5mu.\n\n\n\n\nDemux\n\n\nWorkflows/ingestion\n\n\nA generic pipeline for running bcl2fastq, bcl-convert or Cell Ranger mkfastq.\n\n\n\n\nDimensionality reduction\n\n\nWorkflows/multiomics\n\n\nRun calculations that output information required for most integration methods: PCA, nearest neighbour and UMAP.\n\n\n\n\nGdo singlesample\n\n\nWorkflows/gdo\n\n\nProcessing unimodal single-sample guide-derived oligonucleotide (GDO) data.\n\n\n\n\nHarmony leiden\n\n\nWorkflows/integration\n\n\nRun harmony integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nMake reference\n\n\nWorkflows/ingestion\n\n\nBuild a transcriptomics reference into one of many formats\n\n\n\n\nProcess batches\n\n\nWorkflows/multiomics\n\n\nThis workflow serves as an entrypoint into the ‘full_pipeline’ in order to re-run the multisample processing and the integration setup.\n\n\n\n\nProcess samples\n\n\nWorkflows/multiomics\n\n\nA pipeline to analyse multiple multiomics samples.\n\n\n\n\nProt multisample\n\n\nWorkflows/prot\n\n\nProcessing unimodal multi-sample ADT data.\n\n\n\n\nProt singlesample\n\n\nWorkflows/prot\n\n\nProcessing unimodal single-sample CITE-seq data.\n\n\n\n\nQc\n\n\nWorkflows/qc\n\n\nA pipeline to add basic qc statistics to a MuData\n\n\n\n\nRna multisample\n\n\nWorkflows/rna\n\n\nProcessing unimodal multi-sample RNA transcriptomics data.\n\n\n\n\nRna singlesample\n\n\nWorkflows/rna\n\n\nProcessing unimodal single-sample RNA transcriptomics data.\n\n\n\n\nScanorama leiden\n\n\nWorkflows/integration\n\n\nRun scanorama integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nScvi leiden\n\n\nWorkflows/integration\n\n\nRun scvi integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nSplit modalities\n\n\nWorkflows/multiomics\n\n\nA pipeline to split a multimodal mudata files into several unimodal mudata files.\n\n\n\n\nTotalvi leiden\n\n\nWorkflows/integration\n\n\nRun totalVI integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "components/index.html#workflows",
    "href": "components/index.html#workflows",
    "title": "Reference",
    "section": "",
    "text": "Order By\n       Default\n         \n          Name\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nNamespace\n\n\nDescription\n\n\n\n\n\n\nBD Rhapsody\n\n\nWorkflows/ingestion\n\n\nA generic pipeline for running BD Rhapsody WTA or Targeted mapping, with support for AbSeq, VDJ and/or SMK.\n\n\n\n\nBbknn leiden\n\n\nWorkflows/integration\n\n\nRun bbknn followed by leiden clustering and run umap on the result.\n\n\n\n\nCell Ranger mapping\n\n\nWorkflows/ingestion\n\n\nA pipeline for running Cell Ranger mapping.\n\n\n\n\nCell Ranger multi\n\n\nWorkflows/ingestion\n\n\nA pipeline for running Cell Ranger multi.\n\n\n\n\nCell Ranger post-processing\n\n\nWorkflows/ingestion\n\n\nPost-processing Cell Ranger datasets.\n\n\n\n\nConversion\n\n\nWorkflows/ingestion\n\n\nA pipeline to convert different file formats to .h5mu.\n\n\n\n\nDemux\n\n\nWorkflows/ingestion\n\n\nA generic pipeline for running bcl2fastq, bcl-convert or Cell Ranger mkfastq.\n\n\n\n\nDimensionality reduction\n\n\nWorkflows/multiomics\n\n\nRun calculations that output information required for most integration methods: PCA, nearest neighbour and UMAP.\n\n\n\n\nGdo singlesample\n\n\nWorkflows/gdo\n\n\nProcessing unimodal single-sample guide-derived oligonucleotide (GDO) data.\n\n\n\n\nHarmony leiden\n\n\nWorkflows/integration\n\n\nRun harmony integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nMake reference\n\n\nWorkflows/ingestion\n\n\nBuild a transcriptomics reference into one of many formats\n\n\n\n\nProcess batches\n\n\nWorkflows/multiomics\n\n\nThis workflow serves as an entrypoint into the ‘full_pipeline’ in order to re-run the multisample processing and the integration setup.\n\n\n\n\nProcess samples\n\n\nWorkflows/multiomics\n\n\nA pipeline to analyse multiple multiomics samples.\n\n\n\n\nProt multisample\n\n\nWorkflows/prot\n\n\nProcessing unimodal multi-sample ADT data.\n\n\n\n\nProt singlesample\n\n\nWorkflows/prot\n\n\nProcessing unimodal single-sample CITE-seq data.\n\n\n\n\nQc\n\n\nWorkflows/qc\n\n\nA pipeline to add basic qc statistics to a MuData\n\n\n\n\nRna multisample\n\n\nWorkflows/rna\n\n\nProcessing unimodal multi-sample RNA transcriptomics data.\n\n\n\n\nRna singlesample\n\n\nWorkflows/rna\n\n\nProcessing unimodal single-sample RNA transcriptomics data.\n\n\n\n\nScanorama leiden\n\n\nWorkflows/integration\n\n\nRun scanorama integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nScvi leiden\n\n\nWorkflows/integration\n\n\nRun scvi integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nSplit modalities\n\n\nWorkflows/multiomics\n\n\nA pipeline to split a multimodal mudata files into several unimodal mudata files.\n\n\n\n\nTotalvi leiden\n\n\nWorkflows/integration\n\n\nRun totalVI integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "components/index.html#modules",
    "href": "components/index.html#modules",
    "title": "Reference",
    "section": "Modules",
    "text": "Modules\n\n\n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nNamespace\n\n\nDescription\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "OpenPipelines.bio next release",
    "section": "",
    "text": "Add descriptions to all pages and add listings to index pages.\nUpdate documentation on creating components for developers.\nUpdate getting started page for developers\nUpdate project structure.\nUpdate information on running tests.\nUpdate “More information” pages\nWrite getting started page for user guide\nDocument how to run workflows\nDocument parameter lists"
  },
  {
    "objectID": "CHANGELOG.html#major-changes",
    "href": "CHANGELOG.html#major-changes",
    "title": "OpenPipelines.bio next release",
    "section": "",
    "text": "Add descriptions to all pages and add listings to index pages.\nUpdate documentation on creating components for developers.\nUpdate getting started page for developers\nUpdate project structure.\nUpdate information on running tests.\nUpdate “More information” pages\nWrite getting started page for user guide\nDocument how to run workflows\nDocument parameter lists"
  },
  {
    "objectID": "CHANGELOG.html#major-changes-1",
    "href": "CHANGELOG.html#major-changes-1",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.12.1 release."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-2",
    "href": "CHANGELOG.html#major-changes-2",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.12.0 release."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-3",
    "href": "CHANGELOG.html#major-changes-3",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.11.0 release."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-4",
    "href": "CHANGELOG.html#major-changes-4",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.10.0 release.\nAdd documentation for OpenPipelines architecture."
  },
  {
    "objectID": "CHANGELOG.html#minor-changes",
    "href": "CHANGELOG.html#minor-changes",
    "title": "OpenPipelines.bio next release",
    "section": "MINOR CHANGES",
    "text": "MINOR CHANGES\n\nAlso generate documentation for the multiple_sep values of component arguments."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-5",
    "href": "CHANGELOG.html#major-changes-5",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.9.0 release."
  },
  {
    "objectID": "CHANGELOG.html#minor-changes-1",
    "href": "CHANGELOG.html#minor-changes-1",
    "title": "OpenPipelines.bio next release",
    "section": "MINOR CHANGES",
    "text": "MINOR CHANGES\n\nUpdate to Viash actions 0.4.0."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-6",
    "href": "CHANGELOG.html#major-changes-6",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.8.0 release.\nUse git submodule to access openpipeline repo.\nPropose new website structure.\nUpdate author page."
  },
  {
    "objectID": "fundamentals/roadmap.html",
    "href": "fundamentals/roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "flowchart LR\n  classDef done fill:#a3f6cf,stroke:#000000;\n  classDef wip fill:#f4cb93,stroke:#000000;\n  classDef unprocessed fill:#afadff,stroke:#000000;\n\n  Raw1[Sample 1] --&gt; Split1[/Split\\nmodalities/]:::done --&gt; ProcGEX1 & ProcRNAV1 & ProcADT1 & ProcATAC1 & ProcVDJ1\n  ProcGEX1[/Process GEX\\nprofile/]:::done --&gt; ConcatGEX[/Concatenate\\nprofiles/]:::done --&gt; ProcGEX[/Process GEX\\nprofiles/]:::done\n  ProcRNAV1[/Process RNAV\\nprofile/]:::wip --&gt; ConcatRNAV[/Concatenate\\nprofiles/]:::done --&gt; ProcRNAV[/Process RNAV\\nprofiles/]:::wip\n  ProcADT1[/Process ADT\\nprofile/]:::done --&gt; ConcatADT[/Concatenate\\nprofiles/]:::done --&gt; ProcADT[/Process ADT\\nprofiles/]:::done\n  ProcATAC1[/Process ATAC\\nprofile/]:::unprocessed --&gt; ConcatATAC[/Concatenate\\nprofiles/]:::done --&gt; ProcATAC[/Process ATAC\\nprofiles/]:::unprocessed\n  ProcVDJ1[/Process VDJ\\nprofile/]:::unprocessed --&gt; ConcatVDJ[/Concatenate\\nprofiles/]:::done --&gt; ProcVDJ[/Process VDJ\\nprofiles/]:::unprocessed\n  ProcGEX & ProcRNAV & ProcADT & ProcATAC & ProcVDJ --&gt; Merge[/Merge\\nmodalities/]:::done --&gt; SetupIntegration[/Setup\\nintegration/]:::done --&gt; Integration[/Integration/]:::done\n\n\n\n\nFigure 1: Status of implemented components. Green: implemented, orange: work in progress, purple: modality included in output but unprocessed,\nGEX: Gene-expression. RNAV: RNA Velocity. ADT: Antibody-Derived Tags. ATAC: Assay for Transposase-Accessible Chromatin.\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  subgraph ingestion\n    direction TB\n    subgraph cellranger_multi\n      direction TB\n      mapping_10x --&gt; convert_to_h5mu_10x\n      mapping_10x[mapping]:::done\n      convert_to_h5mu_10x[convert_to_h5mu]:::done\n    end\n    subgraph bdrhap_v1\n      direction TB\n      mapping_bd1 --&gt; convert_to_h5mu_bd1\n      mapping_bd1[mapping]:::done\n      convert_to_h5mu_bd1[convert_to_h5mu]:::done\n    end\n    subgraph bdrhap_v2\n      direction TB\n      mapping_bd2 --&gt; convert_to_h5mu_bd2\n      mapping_bd2[mapping]:::wip\n      convert_to_h5mu_bd2[convert_to_h5mu]:::wip\n    end\n    cellranger_multi:::subwf\n    bdrhap_v1:::subwf\n    bdrhap_v2:::subwf\n  end\n  ingestion:::wf\n  subgraph process_samples\n    split_modalities --&gt; rna_singlesample & prot_singlesample & gdo_singlesample & atac_singlesample & other_modalities --&gt; concat --&gt; process_batches\n    split_modalities:::done\n    rna_singlesample:::done\n    prot_singlesample:::done\n    gdo_singlesample:::done\n    atac_singlesample:::todo\n    other_modalities:::done\n    concat:::done\n    subgraph process_batches\n      direction LR\n      split_modalities2 --&gt; rna_multisample & prot_multisample & atac_multisample & other_modalities2 --&gt; merge\n      split_modalities2[split_modalities]:::done\n      other_modalities2[other_modalities]:::done\n      rna_multisample:::done\n      prot_multisample:::done\n      merge:::done\n    end\n    process_batches:::subwf\n    atac_multisample:::todo\n  end\n  process_samples:::wf\n  raw_counts --- ingestion --&gt; raw_h5mu\n  raw_h5mu --- process_samples --&gt; processed_h5mu\n  subgraph integration\n    direction LR\n    integration_method --&gt; find_neighbors --&gt; leiden --&gt; umap\n    integration_method -.- intmeth\n    integration_method:::done\n    find_neighbors:::done\n    leiden:::done\n    umap:::done\n    subgraph intmeth [integration_method]\n      bbknn:::done\n      harmony:::done\n      scanorama:::done\n      scvi:::done\n      totalvi:::done\n      scgpt_integration:::wip\n    end\n    intmeth:::info\n  end\n  integration:::wf\n  processed_h5mu --- integration ---&gt; integrated_h5mu\n  subgraph celltype_annotation\n    direction TB\n    integration_method2[integration_method]:::done\n    celltypist:::wip\n    scanvi:::wip\n    scgpt_annotation:::wip\n    onclass:::todo\n    svm:::todo\n    randomforest:::todo\n    pynndescent_knn:::wip\n    consensus_voting:::todo\n    integration_method2 --&gt; pynndescent_knn --&gt; consensus_voting\n    celltypist & scanvi & scgpt_annotation & onclass & svm & randomforest --&gt; consensus_voting\n  end\n  reference_atlas --&gt; celltype_annotation\n  celltype_annotation:::wf\n  integrated_h5mu --- celltype_annotation --&gt; annotated_h5mu\n\n  classDef done fill:#ccebc5,stroke:#4daf4a\n  classDef wip fill:#fed9a6,stroke:#ff7f00\n  classDef todo fill:#fbb4ae,stroke:#e41a1c\n  classDef wf fill:#f0f0f0,stroke:#525252\n  classDef subwf fill:#d9d9d9,stroke:#525252\n  classDef info fill:#f0f0f0,stroke:#525252,stroke-dasharray: 4 4\n  \n  subgraph Legend\n    done[Done]:::done\n    wip[Work in progress]:::wip\n    todo[To do]:::todo\n  end\n  Legend:::info\n\n\n\n\nFigure 2",
    "crumbs": [
      "Fundamentals",
      "Roadmap"
    ]
  },
  {
    "objectID": "fundamentals/philosophy.html",
    "href": "fundamentals/philosophy.html",
    "title": "Philosophy",
    "section": "",
    "text": "Mission\nOpenPipelines are best-practice living workflows for single-cell uni- and multi-omics data. Building a best-practice pipeline requires knowledge and time that not one single person can provide, but rather requires input from a community. Additionally, a best-pratice pipeline needs constant maintenance to keep up to date with the latest standards, ideally sourcing input from a ‘living’ benchmark. Continuous improvement necessitates a robust system for sourcing and applying community input both from a technical and organisational standpoint.\n\n\n\n\n\ngraph TB\n  ben[\"🌱📈 Living benchmarks\"]\n  pra[\"🌱📖 Living best practices\"]\n  pip[\"🌱⚙️ Living reproducible pipelines\"]\n  ben --&gt; pra --&gt; pip",
    "crumbs": [
      "Fundamentals",
      "Philosophy"
    ]
  },
  {
    "objectID": "contributing/project_structure.html",
    "href": "contributing/project_structure.html",
    "title": "Project structure",
    "section": "",
    "text": "The root of the repository contains two main folders:\n\nsrc, which contains the source code for components and workflows.\n(optionally) the target folder\n\nEach subfolder from src contains a Viash namespace, a logical grouping of pipeline components that perform a similar function. Within each namespace, subfolders designate individual pipeline components. For example ./src/convert/from_bdrhap_to_h5ad contains the implementation for a component from_bdrhap_to_h5ad which is grouped together with other components such as from_10xmtx_to_h5mu into a namespace convert. In a similar manner as grouping components into namespaces, pipelines are grouped together into folders. However, these are not component namespaces and as such do not interact with viash ns commands.\nAs will become apparent later on, Viash not only provides commands to perform operations on individual components, but also on groups of components in a namespace and all components in a project. As a rule of thumb, the basic Viash commands (like viash test) are designated for running commands on individual components, while ns commands are (viash ns test) are for namespaces. When cloning a fresh repository, there will be no target folder present. This is because the target folder will only be created after components have been build.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Project structure"
    ]
  },
  {
    "objectID": "contributing/project_structure.html#sec-project-structure",
    "href": "contributing/project_structure.html#sec-project-structure",
    "title": "Project structure",
    "section": "",
    "text": "The root of the repository contains two main folders:\n\nsrc, which contains the source code for components and workflows.\n(optionally) the target folder\n\nEach subfolder from src contains a Viash namespace, a logical grouping of pipeline components that perform a similar function. Within each namespace, subfolders designate individual pipeline components. For example ./src/convert/from_bdrhap_to_h5ad contains the implementation for a component from_bdrhap_to_h5ad which is grouped together with other components such as from_10xmtx_to_h5mu into a namespace convert. In a similar manner as grouping components into namespaces, pipelines are grouped together into folders. However, these are not component namespaces and as such do not interact with viash ns commands.\nAs will become apparent later on, Viash not only provides commands to perform operations on individual components, but also on groups of components in a namespace and all components in a project. As a rule of thumb, the basic Viash commands (like viash test) are designated for running commands on individual components, while ns commands are (viash ns test) are for namespaces. When cloning a fresh repository, there will be no target folder present. This is because the target folder will only be created after components have been build.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Project structure"
    ]
  },
  {
    "objectID": "contributing/project_structure.html#sec-versioning",
    "href": "contributing/project_structure.html#sec-versioning",
    "title": "Project structure",
    "section": "Versioning and branching strategy",
    "text": "Versioning and branching strategy\nOpenPipeline tries to use of semantic versioning to govern changes between versions. An release of openpipelines uses a version number in the format MAJOR.MINOR.PATCH. Currenly, openpipelines is still at major version 0.x.y, meaning that public-facing breaking changes are possible on MINOR releases. These breaking changes will be documented in a dedicated section of the CHANGELOG that is published with each release. A PATCH release (i.e. a release where the MAJOR and MINOR version number stay the same), is used to resolve bugs with the pipeline but should not introduce breaking changes. Keep in mind that patches might introduce behavioral changes that may look breaking but are actually rectifying changes that were inadvertently introduced previously (and were in fact also ‘breaking changes’). In this case, a bug can also be released without changing the MINOR version, in a PATH release.\nBetween releases, development progress is tracked on Git branches. A git branch represents a snapshot of a codebase in time, to which changes can be added (i.e. committed). Eventually, all new feature or bugfixes must be reconsiled into a single branch so that a new release can be created. This process is called merging and the process of requesting the merging of two branches is called a pull request. Openpipelines follows the convention that the target branch for all pull requests is the main branch. Thus, the main branch contains the latest changes for the code and it can be considered the development branch.\nOnce a pull request has been approved and merged, Github Actions CI will automatically build all components (creating the target directory) and push the result to the main_build branch. In essence, the main_build branch is a copy of the main branch, but also containing the build components. Once it is time to create a openpipelines release, the Github CI release workflow is manually triggered, the components on the main branch will be build and tested. Then, the result will be pushed to the release branch and the integration tests will be run. If all tests succeeded, a new github tag and release can be created manually from the release branch.\n\n\n\n\n\n%%{init: { 'logLevel': 'debug', 'theme': 'default'} } }%%\ngitGraph\n  commit id: \"initial commit\"\n  branch main_build\n  commit id: \"CI build\"\n  checkout main\n  commit\n  checkout main_build\n  merge main\n  checkout main\n  branch feature_a\n  branch feature_b\n  checkout feature_a\n  commit\n  commit\n  checkout main\n  commit id: \"#release 0.1\" type: HIGHLIGHT\n  checkout main_build\n  merge main\n  checkout main\n  branch release\n  commit tag: \"0.1\"\n  checkout main\n  commit\n  checkout feature_b\n  commit\n  commit\n  checkout feature_a\n  commit\n  checkout main\n  merge feature_a\n  checkout main_build\n  merge main\n  checkout main\n  checkout feature_b\n  commit\n  checkout main\n  merge feature_b\n  checkout main_build\n  merge main\n  checkout release\n  merge main tag: \"0.2\"",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Project structure"
    ]
  },
  {
    "objectID": "contributing/index.html",
    "href": "contributing/index.html",
    "title": "Contributing",
    "section": "",
    "text": "Getting started: Install dependencies and fetch test resources\n  \n  \n  \n    Project structure: The structure of OpenPipelines\n  \n  \n  \n    Creating components: A guide on how to create new components\n  \n  \n  \n    Creating pipelines: A guide on how to create new workflows\n  \n  \n  \n    Running tests: How to run component and integration tests.\n  \n  \n  \n    Publishing your changes: How to create a pull request\n  \n  \n\n\nNo matching items",
    "crumbs": [
      "Fundamentals",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing/getting_started.html",
    "href": "contributing/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "The OpenPipelines code is hosted on GitHub. To start working on OpenPipelines, you should create your own copy of the repository by forking it. Visit the OpenPipelines repository here and use the ‘Fork’ button on the top right hand side of the page. After you are done forking, you can clone the repository to a local directory on your computer using git clone. You can choose between using an SSH key to log in to GitHub or username and password (HTTPS) to connect to github.\n\nHTTPSSSH\n\n\ngit clone https://github.com/&lt;YOUR USERNAME&gt;/openpipeline.git\ncd openpipeline\ngit remote add upstream https://github.com/openpipeline-bio/openpipeline.git\n\n\ngit clone git@github.com:&lt;YOUR USERNAME&gt;/openpipeline.git\ncd openpipeline\ngit remote add upstream https://github.com/openpipeline-bio/openpipeline.git",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Getting started"
    ]
  },
  {
    "objectID": "contributing/getting_started.html#forking-the-code-and-cloning-the-repository",
    "href": "contributing/getting_started.html#forking-the-code-and-cloning-the-repository",
    "title": "Getting started",
    "section": "",
    "text": "The OpenPipelines code is hosted on GitHub. To start working on OpenPipelines, you should create your own copy of the repository by forking it. Visit the OpenPipelines repository here and use the ‘Fork’ button on the top right hand side of the page. After you are done forking, you can clone the repository to a local directory on your computer using git clone. You can choose between using an SSH key to log in to GitHub or username and password (HTTPS) to connect to github.\n\nHTTPSSSH\n\n\ngit clone https://github.com/&lt;YOUR USERNAME&gt;/openpipeline.git\ncd openpipeline\ngit remote add upstream https://github.com/openpipeline-bio/openpipeline.git\n\n\ngit clone git@github.com:&lt;YOUR USERNAME&gt;/openpipeline.git\ncd openpipeline\ngit remote add upstream https://github.com/openpipeline-bio/openpipeline.git",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Getting started"
    ]
  },
  {
    "objectID": "contributing/getting_started.html#sec-install-viash-nextflow",
    "href": "contributing/getting_started.html#sec-install-viash-nextflow",
    "title": "Getting started",
    "section": "Install viash and nextflow",
    "text": "Install viash and nextflow\nTo start contributing to OpenPipelines, you will need at Java 11 (or higher) and Docker installed on your system.\nOpenPipelines is being developed in Viash and Nextflow. If you are unfamiliar with either one of these platforms, you can check out their respective documentation pages.\nYou can check if is installed correctly by running the following commands.\nnextflow run hello -with-docker\nviash --version",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Getting started"
    ]
  },
  {
    "objectID": "contributing/getting_started.html#fetch-test-resources",
    "href": "contributing/getting_started.html#fetch-test-resources",
    "title": "Getting started",
    "section": "Fetch test resources",
    "text": "Fetch test resources\nOpenPipelines uses a number of test resources to test the pipelines. If everything is installed correctly, you should be able to fetch these resources by running the following command.\nviash run src/download/sync_test_resources/config.vsh.yaml",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Getting started"
    ]
  },
  {
    "objectID": "contributing/creating_components.html",
    "href": "contributing/creating_components.html",
    "title": "Creating components",
    "section": "",
    "text": "One of the core principals of OpenPipelines is to use MuData as a common data format troughout the whole pipeline. See the concepts page for more information on openpipelines uses MuData to store single-cell data.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#a-common-file-format",
    "href": "contributing/creating_components.html#a-common-file-format",
    "title": "Creating components",
    "section": "",
    "text": "One of the core principals of OpenPipelines is to use MuData as a common data format troughout the whole pipeline. See the concepts page for more information on openpipelines uses MuData to store single-cell data.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#component-location",
    "href": "contributing/creating_components.html#component-location",
    "title": "Creating components",
    "section": "Component location",
    "text": "Component location\nAs discussed in the project structure, components in the repository are stored within src. Additionally, components are grouped into namespaces, according to a common functionality. An example of such a namespace is the dimensionality reduction namespace (dimred), of which the components pca and umap are members. This means that within src, the namespace folders can be found that stores the components that belong to these namespaces.\nIn order to create a new component in OpenPipelines, you will need to create a new folder that will contain the different elements of the component:\nmkdir src/my_namespace/my_component\n\n\n\n\n\n\nTip\n\n\n\nTake a look at the components that are already in src/! There might be a component that already does something similar to what you need.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#the-elements-of-a-component",
    "href": "contributing/creating_components.html#the-elements-of-a-component",
    "title": "Creating components",
    "section": "The elements of a component",
    "text": "The elements of a component\nA component consists of one or more scripts that provide the functionality of the component together with metadata of the component in a configuration file. The Viash config contains metadata of your dataset, which script is used to run it, and the required dependencies. An in-depth guide on how to create components is available on the viash website, but a few specifics and guidelines will be discussed here.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#the-config",
    "href": "contributing/creating_components.html#the-config",
    "title": "Creating components",
    "section": "The config",
    "text": "The config\nfunctionality:\n  name: \"my_component\"\n  namespace: \"my_namespace\"\n  description: \"My new custom component\"\n  authors:\n    - __merge__: ../../authors/my_name.yaml\n      roles: [ author ]\n  arguments:\n    - name: \"--output\"\n      type: file\n      example: \"output_file.h5mu\"\n      description: \"Location were the output file should be written to.\"\n      direction: \"output\"\n  resources:\n    - type: python_script\n      path: script.py\nplatforms:\n  - type: docker\n    image: python:3.11\n    setup:\n      - type: python\n        packages: mudata~=0.2.3\n  - type: nextflow\n    directives:\n      label: [highcpu, midmem]\n\nBasic information\nEach component should have the name, a namespace, a description and author information defined in the config. Because a single author can contribute to multiple components, the author information is often duplicated across components, which was causing issues with the author information being out of date and not easy to maintain. Therefore, it was decided to move author information to ./src/authors. Each author has a yaml file containing the author information, and the viash __merge__ property is used to merge this information into the viash configs.\nBasic information checklist:\n\nGive the component a name\nAdd the component to an appropriate namespace\nAdd a description\nAdd author information\n\n\n\nArguments and argument groups\nIf you component requires arguments, they should be defined in arguments or argument_groups. Try tro group individual arguments into argument_groups when the number of arguments become too larg (10 or more as a rule of thumb).\nArgument checklist:\n\nAdd a description and name\nEach argument should have the appropriate type.\nInput and output files should be of type file instead of string and use the appropriate direction:\nIf possible: add an example\nIf the argument can accept multiple values, add multiple: true\nIf the possible input for an argument is limited to certain set of values, use choices:\n\n\n\n(Test)resources\nResources define files that are required for a component to perform its function. These can be scripts, but also additional files like settings for tools you might require. Defining resources is both a necessity because viash needs to know what code to execute, but defining resources also has the added benefit that these resources are automatically made available, regardless of the build environment. For example: resources are automatically mounted within a running docker container.\nThere is a difference between defining resources and test_resources. While resources are required for a component to function, test_resources only need to be included when testing the component (with for example viash test) in addition to the regular resources. Having a look at the example above, resources are defined using the resources: property. It takes a list of multiple files or folders.\nIn openpipelines, it was decided to not use a service like git lfs to include large resources into the repository. Instead, if large resources are required, there are two possibilities: * Large resources required for testing are to uploaded into an s3 bucket that is synced automatically before running tests (both locally and on github). Please ping a maintainer when you open a PR and ask them to upload the files for you. * Other large resources that are not needed for testing can be considered as input. This means that an argument of type: file needs to be created. The downside of this method is that viash is not able to natively support remote files f\nResources checklist: - Script resources are located next to the config and added to the config with the correct type (python_script, r_script, …) - Small resources (&lt;50MB) that are not scripts can also be checked in into the repo, next to the\n\n\nThe script file\nTODO\n\n\nAuthor information\nTODO",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#adding-dependencies",
    "href": "contributing/creating_components.html#adding-dependencies",
    "title": "Creating components",
    "section": "Adding dependencies",
    "text": "Adding dependencies\nTODO",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#building-components-from-their-source",
    "href": "contributing/creating_components.html#building-components-from-their-source",
    "title": "Creating components",
    "section": "Building components from their source",
    "text": "Building components from their source\nWhen running or testing individual components, it is not necessary to execute an extra command to run the build step, viash test and viash run will build the component on the fly. However, before integrating components into a pipeline, you will need to build the components. More specifically, openpipelines uses Nextflow to combine components into pipelines, so we need to have at least the components build for nextflow platform as target. The easiest method to build the components is to use:\nviash ns build --parallel --setup cachedbuild\nAfter using viash ns build, the target folder will be populated with three subfolders, corresponding to the build platforms that viash supports: native, docker and nextflow.\nBuilding an individual component can still be useful, for example when debugging a component for which the build fails or if you want to create a standalone executable for a component to execute it without the need to use viash. To build an individual component, viash build can be used. Note that the default build directory of this viash base command is output, which is not the location where build components will be imported from when integrating them in pipelines. Using the --output argument, you can set it to any directory you want, for example:\nviash build src/filter/do_filter/config.vsh.yaml -o target/native/filter/do_filter/ -p native",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#containerization",
    "href": "contributing/creating_components.html#containerization",
    "title": "Creating components",
    "section": "Containerization",
    "text": "Containerization\nOne of the key benefits of using Viash is that containers can be created that gather dependencies per component, which avoids building one container that has to encorporate all dependencies for a pipeline together. The containers for a single component can be reduced in size, defining the minimal requirements to run the component. That being said, building containers from scratch can be labour intensive and error prone, with base containers from reputable publishers often benefiting from improved reliability and security. Hence, a balance has to be made between reducing the container’s size and adding many dependencies to a small base container.\nThe preferred containerization setup in OpenPipelines uses the following guidelines:\n\nChoose a base container from a reputable source and use its latest version\nDo not use base containers that have not been updated in a while\nUse package managers to install dependencies as much as possible\nAvoid building depdencies from source.\n\nExamples of base containers that are currently being used are:\n\npython:3.11 for python environments\nubuntu:focal for general linux environments and bash scripts\neddelbuettel/r2u:22.04 for R\nnvcr.io/nvidia/pytorch:22.09-py3 for using GPU accelerated calculations using pytorch in python",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/running_tests.html",
    "href": "contributing/running_tests.html",
    "title": "Running tests",
    "section": "",
    "text": "The input data that is needed to run the tests will need to be downloaded from the openpipelines Amazon AWS s3 bucket first. To do so, the download/sync_test_resource component can be used, which will download the data to the correct location (resources_test) by default.\nviash run src/download/sync_test_resources/config.vsh.yaml",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Running tests"
    ]
  },
  {
    "objectID": "contributing/running_tests.html#fetch-the-test-data",
    "href": "contributing/running_tests.html#fetch-the-test-data",
    "title": "Running tests",
    "section": "",
    "text": "The input data that is needed to run the tests will need to be downloaded from the openpipelines Amazon AWS s3 bucket first. To do so, the download/sync_test_resource component can be used, which will download the data to the correct location (resources_test) by default.\nviash run src/download/sync_test_resources/config.vsh.yaml",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Running tests"
    ]
  },
  {
    "objectID": "contributing/running_tests.html#run-component-tests",
    "href": "contributing/running_tests.html#run-component-tests",
    "title": "Running tests",
    "section": "Run component tests",
    "text": "Run component tests\nTo build and run tests for individual component that you are working on, use viash test with the config.vsh.yaml of the component you would like to test. For example:\nviash test src/convert/from_10xh5_to_h5mu/config.vsh.yaml\nKeep in mind that when no platform is passed to viash test, it will use the first platform that is specified in the config, which is docker for most of the components in openpipelines. Use -p native for example if you do not want to use docker.\nIt is also possible to execute the tests for all components in each namespace using:\nviash ns test --parallel -q convert",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Running tests"
    ]
  },
  {
    "objectID": "contributing/running_tests.html#run-integration-tests",
    "href": "contributing/running_tests.html#run-integration-tests",
    "title": "Running tests",
    "section": "Run integration tests",
    "text": "Run integration tests\nIndividual integration tests can be run by using the integration_test.sh scripts for a pipeline, located next to the main.nf in the src/workflows folder.\nsrc/workflows/ingestion/cellranger_demux/integration_test.sh",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Running tests"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html",
    "href": "contributing/pull_requests.html",
    "title": "Publishing your changes",
    "section": "",
    "text": "After ensuring that the implemented changes pass all relevant tests and meets the contribution guidelines, you can create a pull request following the steps below.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html#step-1-merge-upstream-repository",
    "href": "contributing/pull_requests.html#step-1-merge-upstream-repository",
    "title": "Publishing your changes",
    "section": "Step 1: Merge upstream repository",
    "text": "Step 1: Merge upstream repository\nBefore you contribute your changes need to merge the upstream main branch into your fork. This ensures that your changes are based on the latest version of the code.\nTo do this, enter the following commands adapted from Syncing a Fork in your terminal or command prompt:\n# add the upstream repository to your local repository\ngit remote add upstream https://github.com/openpipelines-bio/openpipeline.git\n# download the changes from the openpipelines repo\ngit fetch upstream\n# change your current branch to the branch of the pull request\ngit checkout &lt;feature_branch&gt;\n# merge the changes from upstream into your branch\ngit merge upstream/main\n# push the updates, your pull request will also be updated\ngit push",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html#step-2-edit-changelog",
    "href": "contributing/pull_requests.html#step-2-edit-changelog",
    "title": "Publishing your changes",
    "section": "Step 2: Edit changelog",
    "text": "Step 2: Edit changelog\nAdd an entry to the CHANGELOG.md file describing the proposed changes.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html#step-3-create-pull-request",
    "href": "contributing/pull_requests.html#step-3-create-pull-request",
    "title": "Publishing your changes",
    "section": "Step 3: Create pull request",
    "text": "Step 3: Create pull request\nThe following steps were adapted from Creating a pull request from a fork\n\nGo to https://github.com/openpipelines-bio/openpipeline/pulls.\nClick on the New pull request button.\nOn the compare page click on the link compare across forks below the title. \nOn the right side in the head section select your fork repo and the correct branch you want to merge.\nClick on Create pull request.\nConstruct your PR by giving it a title and description.\nMake sure you select the box below the description Allow edits from maintainers.\nIf the PR is ready for review click the button Create Pull Request. Otherwise you can click the arrow next to the button and select Create Draft Pull Request and click the button when it changes.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html#next-steps",
    "href": "contributing/pull_requests.html#next-steps",
    "title": "Publishing your changes",
    "section": "Next steps",
    "text": "Next steps\n\nGithub Actions\nWhenever a Pull Request (including draft) is created a github workflow will perform checks. These checks need to be succesful as a minimum requirement before a merge can be done. When there are errors in the checks, try to fix them while waiting on a review. If it is not possible to fix the error, add a comment to the PR to let the reviewers know.\n\n\nReview\nYour PR will be reviewed by maintainers of OpenPipelines. During the review, you can be asked for changes to the code.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html",
    "href": "fundamentals/architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "OpenPipeline is a pipeline for the processing of multimodal single-cell data that scales to a great many of samples. Covering the architecture requires us to explain many angles, including: what the expected inputs and outputs are for each workflow are, how do the workflows relate to each other, and what the state of the data is at each step of the pipeline. Here is an overview of the general steps involved in processing sequencing data into a single integrated object. We will discuss each of the steps further below.\nflowchart TD  \n  ingest[\"Ingestion\"] --&gt; split --&gt; unimodalsinglesample[\"Unimodal Single Sample Processing\"] --&gt; concat --&gt; unimodalmultisample[\"Unimodal Multi Sample Processing\"] --&gt; merging --&gt; integation_setup[\"Integration Setup\"] --&gt; integration[\"Integration\"]  --&gt; downstreamprocessing[\"Downstream Processing\"]\n\n\n\n\nFigure 1: Overview of the steps included in OpenPipeline for the analysis of single cell multiomics data.\nflowchart TB\n  subgraph ingestion\n    direction TB\n    subgraph cellranger_multi\n      direction TB\n      mapping_10x --&gt; convert_to_h5mu_10x\n      mapping_10x[mapping]\n      convert_to_h5mu_10x[convert_to_h5mu]\n    end\n    subgraph bdrhap_v1\n      direction TB\n      mapping_bd1 --&gt; convert_to_h5mu_bd1\n      mapping_bd1[mapping]\n      convert_to_h5mu_bd1[convert_to_h5mu]\n    end\n    subgraph bdrhap_v2\n      direction TB\n      mapping_bd2 --&gt; convert_to_h5mu_bd2\n      mapping_bd2[mapping]\n      convert_to_h5mu_bd2[convert_to_h5mu]\n    end\n    cellranger_multi:::subwf\n    bdrhap_v1:::subwf\n    bdrhap_v2:::subwf\n  end\n  ingestion:::wf\n  subgraph process_samples\n    split_modalities --&gt; rna_singlesample & prot_singlesample & gdo_singlesample & atac_singlesample & other_modalities --&gt; concat --&gt; process_batches\n    split_modalities\n    rna_singlesample\n    prot_singlesample\n    gdo_singlesample\n    atac_singlesample\n    other_modalities\n    concat\n    subgraph process_batches\n      direction LR\n      split_modalities2 --&gt; rna_multisample & prot_multisample & atac_multisample & other_modalities2 --&gt; merge\n      split_modalities2[split_modalities]\n      other_modalities2[other_modalities]\n      rna_multisample\n      prot_multisample\n      merge\n    end\n    process_batches:::subwf\n    atac_multisample\n  end\n  process_samples:::wf\n  raw_counts --- ingestion --&gt; raw_h5mu\n  raw_h5mu --- process_samples --&gt; processed_h5mu\n  subgraph integration\n    direction LR\n    integration_method --&gt; find_neighbors --&gt; leiden --&gt; umap\n    integration_method -.- intmeth\n    integration_method\n    find_neighbors\n    leiden\n    umap\n    subgraph intmeth [integration_method]\n      bbknn\n      harmony\n      scanorama\n      scvi\n      totalvi\n      scgpt_integration\n    end\n    intmeth:::info\n  end\n  integration:::wf\n  processed_h5mu --- integration ---&gt; integrated_h5mu\n  subgraph celltype_annotation\n    direction TB\n    integration_method2[integration_method]\n    celltypist\n    scanvi\n    scgpt_annotation\n    onclass\n    svm\n    randomforest\n    pynndescent_knn\n    consensus_voting\n    integration_method2 --&gt; pynndescent_knn --&gt; consensus_voting\n    celltypist & scanvi & scgpt_annotation & onclass & svm & randomforest --&gt; consensus_voting\n  end\n  reference_atlas --&gt; celltype_annotation\n  celltype_annotation:::wf\n  integrated_h5mu --- celltype_annotation --&gt; annotated_h5mu\n\n  classDef wf fill:#f0f0f0,stroke:#525252\n  classDef subwf fill:#d9d9d9,stroke:#525252\n  classDef info fill:#f0f0f0,stroke:#525252,stroke-dasharray: 4 4",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#ingestion-workflows",
    "href": "fundamentals/architecture.html#ingestion-workflows",
    "title": "Architecture",
    "section": "Ingestion workflows",
    "text": "Ingestion workflows\nAll of the following workflows from the ingestion namespace have been discussed in more detail in the ingestion section:\n\ningestion/bd_rhapsody\ningestion/cellranger_mapping\ningestion/cellranger_multi\ningestion/demux\ningestion/make_reference",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#multiomics-workflows",
    "href": "fundamentals/architecture.html#multiomics-workflows",
    "title": "Architecture",
    "section": "Multiomics workflows",
    "text": "Multiomics workflows\nThere exists no singlesample workflow. However, the prot_singlesample and rna_singlesample pipelines do exist and they map identically to the functionality described in the single-sample antibody capture processing and single-sample gene expression processing sections respectively. If you would like to process your samples as described in the unimodal single sample processing section, you can execute both workflows in tandem for the two modalities.\nContrary to the workflows for single sample processing, there exists a multiomics/multisample workflow. However this workflow is not just the multiomics/prot_multisample and multiomics/rna_multisample workflows that have been combined. Instead, it combines the multiomics/prot_multisample, multiomics/rna_multisample and multiomics/integration/initialize_integration workflows. The purpose of this pipeline is to provide an extra ‘entrypoint’ into the full pipeline that skips the singlesample processing, allowing reprocessing samples that have already been processed before. A popular usecase is to manually select one or more celltypes which need to be processed again or the integration of observations from multiple experiments into a single dataset. Keep in mind that concatenation is not included in the multisample pipeline, so when multiple input files are specified they are processed in parallel. If you would like to integrate multiple experiments, you need to first concatenate them in a seperate step:",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#the-full-pipeline",
    "href": "fundamentals/architecture.html#the-full-pipeline",
    "title": "Architecture",
    "section": "The “full” pipeline",
    "text": "The “full” pipeline\nThe name of this pipeline is a bit of a misnomer, because it does not include all the steps from ingestion to integration. As will be discussed in the ingestion section, which ingestion strategy you need is dependant on your technology provider and the chosen platform. For integration, there exist many methods and combination of methods, and you may wish to choose which integration methods are applicable for your usecase. As a consequence, these two stages in the analysis of single-cell need to be executed seperatly and not as part of a single unified pipeline. All other steps outlined below on the other hand are included into the “full” pipeline, which can therefore be summarized in the following figure:\n\n\n\n\n\n\nflowchart TD  \n  split --&gt; unimodalsinglesample[\"Unimodal Single Sample Processing\"] --&gt; concat --&gt; unimodalmultisample[\"Unimodal Multi Sample Processing\"] --&gt; merging --&gt; integation_setup\n\n\n\n\nFigure 2: Overview of the steps included in the full pipelines from OpenPipeline.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#integration-workflows",
    "href": "fundamentals/architecture.html#integration-workflows",
    "title": "Architecture",
    "section": "Integration workflows",
    "text": "Integration workflows\nFor each of the integration methods (and their optional combination with other tools), a seperate pipeline is defined. More information for each of the pipelines is available in the integration methods section.\n\nmultiomics/integration/bbknn_leiden\nmultiomics/integration/harmony_leiden\nmultiomics/integration/scanorama_leiden\nmultiomics/integration/scvi_leiden\nmultiomics/integration/totalvi_leiden\nmultiomics/integration/initialize_integration",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-splitting",
    "href": "fundamentals/architecture.html#sec-splitting",
    "title": "Architecture",
    "section": "Splitting modalities",
    "text": "Splitting modalities\nWe refer to splitting modalities when multimodal MuData file is split into several unimodal MuData files. The number of output files is equal to the number of modalities present in the input file. Splitting the modalities works on MuData files containing data for multiple samples or for single-sample files.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-merging",
    "href": "fundamentals/architecture.html#sec-merging",
    "title": "Architecture",
    "section": "Merging of modalities",
    "text": "Merging of modalities\nMerging refers to combining multiple files with data for one modality into a single output file that contains all input modalities. It is the inverse operation of splitting the modalities.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#concatenation-of-samples",
    "href": "fundamentals/architecture.html#concatenation-of-samples",
    "title": "Architecture",
    "section": "Concatenation of samples",
    "text": "Concatenation of samples\nJoining of observations for different samples, stored in their respective MuData file, into a single MuData file for all samples together is called sample concatenation. In practice, this operation is performed for each modality separately. An extra column (with default name sample_id) is added to the annotation of the observations (.obs) to indicate where each observation originated from.\n\nSpecial care must be taken when considering annotations for observations and features while concatenating the samples. Indeed, the data from different samples can contain conflicting information. Openpipeline’s concat component provides an argument other_axis_mode that allows a user to specify what happens when conflicting information is found. The move option for this argument is the default behavior. In this mode, each annotation column (from .obs and .var) is compared across samples. When no conflicts are found or the column is unique for a sample, the column is added output object. When a conflict does occur, all of the columns are gathered from the samples and stored into a dataframe. This dataframe is then stored into .obsm for annotations for the observations and .varm for feature annotations. This way, a user can have a look at the conflicts and decide what to do with them.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#creating-a-transcriptomics-reference",
    "href": "fundamentals/architecture.html#creating-a-transcriptomics-reference",
    "title": "Architecture",
    "section": "Creating a transcriptomics reference",
    "text": "Creating a transcriptomics reference\nMapping reads from the FASTQ files to features requires a reference that needs to be provided to the mapping component. Depending on the usecase, you might even need to provide references specific for the modalities that you are trying to analyze. For gene expression data, the reference is a reference genome, together with its appropriate gene annotation. A genome reference is often indexed in order to improve the mapping speed. Additionally, some mapping frameworks provided by the single-cell technology providers require extra preprocessing of the reference before they can be used with their worklow. OpenPipelines provides a make_reference that allows you to create references in many formats which can be used to map your reads to.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-single-sample-gex",
    "href": "fundamentals/architecture.html#sec-single-sample-gex",
    "title": "Architecture",
    "section": "Single-sample Gene Expression Processing",
    "text": "Single-sample Gene Expression Processing\nSingle-sample gene expression processing involves two steps: removing cells based on count statistics and flagging observations originating from doublets.\nThe removal of cells based on basic count statistics is split up into two parts: first, cells are flagged for removal by filter_with_counts. It flags observations based on several thresholds:\n\nThe number of genes that have a least a single count. Both a maximum and minimum number of genes for a cell to be removed can be specified.\nThe percentage of read counts that originated from a mitochodrial genes. Cells can be filtered based on both a maximum or minimum fraction of mitochondrial genes.\nThe minimum or maximum total number of counts captured per cell. Cells with 0 total counts are always removed.\n\nFlagging cells for removal involved adding a boolean column to the .obs dataframe. After the cells have been flagged for removal, the cells are actually filtered using do_filter, which reads the values in .obs and removed the cells labeled True. This applies the general phylosophy of “separation of concerns”: one component is responsible for labeling the cells, another for removing them. This keeps the codebase for a single component small and its functionality testable.\nThe next and final step in the single-sample gene expression processing is doublet detection using filter_with_scrublet. Like filter_with_counts, it will not remove cells but add a column to .obs (which have the name filter_with_scrublet by default). The single-sample GEX workflow will not remove not be removed during the processing (hence no do_filter). However, you can choose to remove them yourself before doing your analyses by applying a filter with the column in .obs yourself.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-single-sample-adt",
    "href": "fundamentals/architecture.html#sec-single-sample-adt",
    "title": "Architecture",
    "section": "Single-sample Antibody Capture Processing",
    "text": "Single-sample Antibody Capture Processing\nThe process of filtering antibody capture data is similar to the filtering in the single-sample gene-expression processing, but without doublet detection. In some particular cases you can use your ADT data to perform doublet detection using for example cell-type maskers. More information can be found in the single-cell best practices book.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#multisample-gene-expression-processing",
    "href": "fundamentals/architecture.html#multisample-gene-expression-processing",
    "title": "Architecture",
    "section": "Multisample Gene Expression Processing",
    "text": "Multisample Gene Expression Processing\nProcessing multisample gene expression involved the following steps:\n\nNormalization: Normalization aims to adjust the raw counts in the dataset for variable sampling effects by scaling the observable variance to a specified range. There are different ways to transform the data, but the normalization method is to make sure each observation (cell) has a total count equal to the median of total counts over all genes for observations (cells) before normalization.\nLog transformation: Calculates \\(X = ln(X + 1)\\), which converts multiplicative relative changes to additive differences. This allows for interpreting the gene expression in terms of relative, rather than absolute, abundances of genes.\nHighly variable gene detection: Detects genes that have a large change in expression between samples. By default, OpenPipeline uses the method from Seurat (Satija et al.). As with other “filtering” components, the filter_with_hvg component does not remove features, but rather annotates genes of interest by adding a boolean column to .var.\nQC metric calculations",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#multisample-antibody-capture-processing",
    "href": "fundamentals/architecture.html#multisample-antibody-capture-processing",
    "title": "Architecture",
    "section": "Multisample Antibody Capture Processing",
    "text": "Multisample Antibody Capture Processing\nProcessing the ADT modality for multiple samples",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-dimensionality-reduction",
    "href": "fundamentals/architecture.html#sec-dimensionality-reduction",
    "title": "Architecture",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nscRNA-seq is a high-throughput sequencing technology that produces datasets with high dimensions in the number of cells and genes. It is true that the data should provide more information, but it also contains more noise and redudant information, making it harder to distill the usefull information. The number of genes and cells can already reduced by gene filtering, but further reduction is a necessity for downstream analysis. Dimensionality reduction projects high-dimensional data into a lower dimensional space (like taking a photo (2D) of some 3D structure). The lower dimensional representation still captures the underlying information of the data, while having fewer dimensions.\nSeveral dimensionality reduction methods have been developed and applied to single-cell data analysis. Two of which are being applied in OpenPipeline:\n\nPrincipal Component Analysis (PCA): PCA reduces the dimension of a dataset by creating a new set of variables (principal components, PCs) from a linear combination of the original features in such a way that they are as uncorrelated as possible. The PCs can be ranked in the order by which they explain the largest variability in the original dataset. By keeping the top n PCs, the PCs with the lowest variance are discarded to effectively reduce the dimensionality of the data without losing information.\nUniform manifold approximation and projection (UMAP): a non-linear dimensionality technique. It constructs a high dimensional graph representation of the dataset and optimizes the low-dimensional graph representation to be structurally as similar as possible to the original graph. In a review by Xiang et al., 2021 it showed the highest stability and separates best the original cell populations.\nt-SNE is another popular non-linear, graph based dimensionality technique which is very similar to UMAP, but it has not yet been implemented in OpenPipeline.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-initializing-integration",
    "href": "fundamentals/architecture.html#sec-initializing-integration",
    "title": "Architecture",
    "section": "Initializing integration",
    "text": "Initializing integration\nAs will be descibed in more details later on, many integration methods exist and therefore there is no single integration which is executed by default. However, there are common tasks which are run before integration either because they provide required input for many downstream integration methods or because they popular steps that would otherwise be done manually. These operations are executed by default when using the “full pipeline” as part of the initialize_integration subworkflow.\nPCA is used to reduce the dimensionality of the dataset as described previously. Find Neighbors and Leiden Clustering are useful for the identification of cell types or states in the data. Here we apply a popular method to accomplish this is to first calculate a neighborhood graph on a low dimensinonal representation of the data and then cluster the data based on similarity between data points. Finally, UMAP allows us to visualise the clusters by reducing the dimensionality of the data while still providing an accurate representation of the underlying cell population structure.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-integration-methods",
    "href": "fundamentals/architecture.html#sec-integration-methods",
    "title": "Architecture",
    "section": "Integration Methods",
    "text": "Integration Methods\nIntegration is the alignment of cell types across samples. There exist three different types of integration methods, based on the degree of integration across modalities:\n\nUnimodal integration across batches. For example: scVI, scanorama, harmony\nMultimodal integration across batches and modalities. Can be used to integrate joint-profiling data where multiple modalities are measured. For example: totalVI\nMosaic integration: data integration across batches and modalities where not all cells are profiled in all modalities and it may be the case that no cells contain profiles in all integrated modalities. Mosaic integration methods have not been made available in OpenPipeline yet. An example of a tool that performs mosaic integration is StabMap.\n\nIn either of the three cases, concatenated samples are required, and merged modalities preferred. A plethora of integration methods exist, which in turn interact with other functionality (like clustering and dimensionality reduction methods) to generate a large number of possible usecases which one pipeline cannot cover in an easy manner. Therefore, there is no single integration step that is part of a global pipeline which is executed by default. Instead, a user can choose from the integration workflows provided, and ‘stack’ integration methods by adding the outputs to different output slots of the MuData object. The following sections will descibe the integration workflows that are available in OpenPipeline.\n\nUnimodal integration\nFor unimodal integration, scVI, scanorama and harmony have been added to the scvi_leiden, scanorama_leiden, and harmony_leiden workflows respectively. After executing the integration methods themselves, Find Neighbors and Leiden Clustering are run the results of the integration as wel as UMAP in order to be able to visualise the results. The functioning of these components has already been described here.\n\n\n\nMultimodal Integration\nA single multimodal integration method is currently avaiable in OpenPipeline: totalVI. It allows using information from both the gene-expression data and the antibody-capture data together to integrate the cell types. As with the other integration workflows, after running totalVI, Find Neighbors, Leiden Clustering and UMAP are run on the result. However in this case the three components are executed on both of the integrated modalities.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/index.html",
    "href": "fundamentals/index.html",
    "title": "Fundamentals",
    "section": "",
    "text": "Philosophy: Our approach and mission\n  \n  \n  \n    Concepts: The core concepts behind this project\n  \n  \n  \n    Architecture: Structure of the project\n  \n  \n  \n    Roadmap: Development roadmap\n  \n  \n\n\nNo matching items",
    "crumbs": [
      "Fundamentals"
    ]
  },
  {
    "objectID": "fundamentals/concepts.html",
    "href": "fundamentals/concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Goals\nOpenPipelines strives to provide easy ways to interact with the pipeline and/or codebase for three types of users:\n\nPipeline executor: runs the pipeline from a GUI side\nPipeline editor: adapts pipelines with existing components for specific projects\nComponent developer: develops novel components and or pipelines\n\nThis means that openpipelines must be:\n\nUsable by non-experts\nEasy to deploy\nProvide reproducable results\nScalable\nEasy to maintain and adapt\n\n\n\nRequirements\nTo meet these demands, the following concepts have been implemented at the core of Openpipeline:\n\n🌍 A language independent framework\n💾 A versitile storage solution\n🔳 Modularity\n🔀 A best-practice pipeline layout\n⌛ Versioning\n✅ Automatic testing\n💬 Community input\n📺 A graphical interace\n\n\n\nA common file format: AnnData and MuData 💾\nOne of the core principals of OpenPipelines is to use MuData as a common data format throughout the whole pipeline. This means that the input and output for most components and workflows will be a MuData file and converters from and to other common data formats are provided to improve compatibility with up-and downstream applications. Choosing a common data format greatly diminishes the development complexity because it facilitates interfacing between different tools in a pipeline without needing to convert multiple times.\nMuData is a format to store annotated multimodal data. It is derived from the AnnData format. If you are unfamiliar with AnnData or MuData, it is recommended to read up on AnnData first as it is the unimodal counterpart of MuData. MuData can be roughly described as collection of several AnnData objects (stored as a associative array in the .mod attribute). MuData provides a hierarchical way to store the data:\nMuData\n├─ .mod\n│  ├─ modality_1 (AnnData Object)\n│     ├─ .X\n│     ├─ .layers\n│         ├─ layer_1 \n│         ├─ ...\n│     ├─ .var\n│     ├─ .obs\n│     ├─ .obsm\n│     ├─ .varm\n│     ├─ .uns\n│  ├─ modality_2 (AnnData Object)\n├─ .var\n├─ .obs\n├─ .obms\n├─ .varm\n├─ .uns\n\n.mod: an associative array of AnnData objects. Used in OpenPipelines to store the different modalities (CITE-seq, RNA abundance, …)\n.X and .layers: matrices storing the measurements with the columns being the variables measured and the rows being the observations (cells in most cases).\n.var: metadata for the variables (i.e. annotation for the columns of .X or any matrix in .layers). The number of rows in the .var datafame (or the length of each entry in the dictionairy) is equal to the number of columns in the measurement matrices.\n.obs: metadata for the observations (i.e. annotation for the rows of .X or any matrix in .layers). The number of rows in the .obs datafame (or the length of each entry in the dictionairy) is equal to the number of rows in the measurement matrices.\nvarm: the multi-dimensional variable annotation. A key-dataframe mapping where the number of rows in each dataframe is equal to the number of columns in the measurement matrices.\nobsm: the multi-dimensional observation annotation. A key-dataframe mapping where the number of rows in each dataframe is equal to the number of rows in the measurement matrices.\n.uns: A mapping where no restrictions are enforced on the dimensions of the data.\n\n\n\nModularity and a language independent framework 🔳\nTODO\n\n\nA graphical interface 📺\nTODO",
    "crumbs": [
      "Fundamentals",
      "Concepts"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenPipelines",
    "section": "",
    "text": "Reusable components built with Viash for flexible pipeline construction.\n\n\n\n\nLeverage Nextflow for execution on various platforms (local, cloud, HPC).\n\n\n\n\nContainerized components and unit testing ensure reliable analyses.\n\n\n\n\n\n\nA framework for sharing and integrating components to foster teamwork.\n\n\n\n\nAdapts to evolving single-cell analysis by incorporating current best practices and novel methods.\n\n\n\n\nSupports diverse single-cell data types (RNA, protein, VDJ, ATAC) for multi-omics analyses."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "OpenPipelines",
    "section": "",
    "text": "Reusable components built with Viash for flexible pipeline construction.\n\n\n\n\nLeverage Nextflow for execution on various platforms (local, cloud, HPC).\n\n\n\n\nContainerized components and unit testing ensure reliable analyses.\n\n\n\n\n\n\nA framework for sharing and integrating components to foster teamwork.\n\n\n\n\nAdapts to evolving single-cell analysis by incorporating current best practices and novel methods.\n\n\n\n\nSupports diverse single-cell data types (RNA, protein, VDJ, ATAC) for multi-omics analyses."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "OpenPipelines",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\nflowchart LR\n  %% Ingestion\n  subgraph ingestion[Step 1: Ingestion]\n    direction LR\n    10x_ingestion[10x Ingestion]:::subwf\n    bd_ingestion[BD Rhapsody\\nIngestion]:::subwf\n    own_h5mu[Own H5MU]:::subwf\n  end\n  ingestion:::info\n\n  %% Process samples\n  subgraph process_samples[Step 2: Process Samples]\n    direction LR\n    gex[GEX]:::subwf\n    atac[ATAC]:::subwf\n    adt[ADT]:::subwf\n    vdj[VDJ]:::subwf\n    other[Other]:::subwf\n  end\n  process_samples:::info\n\n  %% Integration and downstream\n  subgraph integration[Step 3: Integration]\n    direction LR\n    harmony[Harmony]:::subwf\n    scvi[scVI]:::subwf\n    scanvi[scanVI]:::subwf\n    etc[...]:::subwf\n  end\n  integration[Integration]:::info\n  \n  subgraph downstream[Step 4: Downstream]\n    direction LR\n    celltype_annotation[Cell Type\\nAnnotation]:::subwf\n    markergenes[Marker Genes\\nAnalysis]:::subwf\n    differential[Differential\\nExpression]:::subwf\n    gene_signature_analysis[Gene Signature\\nAnalysis]:::subwf\n    ccc[Cell-Cell\\nCommunication]:::subwf\n  end\n\n  ingestion --&gt; process_samples --&gt; integration --&gt; downstream\n\n  classDef wf fill:#f0f0f0,stroke:#525252\n  classDef subwf fill:#d9d9d9,stroke:#525252\n  classDef info fill:#f0f0f0,stroke:#525252,stroke-dasharray: 4 4"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "OpenPipelines",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\n\n\n  \n  Fundamentals\n  \n    \n  \n    \n        \n  \n  Philosophy\n  \n    \n    \n        \n  \n  Concepts\n  \n    \n    \n        \n  \n  Architecture\n  \n    \n    \n        \n  \n  Roadmap\n  \n    \n    \n  \n    \n\n\nNo matching items\n\n\n\n\n\n\n  \n  User guide\n  \n    \n  \n    \n        \n  \n  Getting started\n  \n    \n    \n        \n  \n  Running pipelines\n  \n    \n    \n        \n  \n  Parameter lists\n  \n    \n    \n        \n  \n  Ingestion\n  \n    \n    \n        \n  \n  Processing\n  \n    \n    \n        \n  \n  Downstream\n  \n    \n    \n        \n  \n  Bug reports\n  \n    \n    \n  \n    \n\n\nNo matching items\n\n\n\n\n\n\n  \n  Contributing\n  \n    \n  \n    \n        \n  \n  Getting started\n  \n    \n    \n        \n  \n  Project structure\n  \n    \n    \n        \n  \n  Creating components\n  \n    \n    \n        \n  \n  Creating pipelines\n  \n    \n    \n        \n  \n  Running tests\n  \n    \n    \n        \n  \n  Publishing your changes\n  \n    \n    \n  \n    \n\n\nNo matching items\n\n\n\n\n\n\n  \n  More information\n  \n    \n  \n    \n        \n  \n  Cheat sheets\n  \n    \n    \n        \n  \n  Code of conduct\n  \n    \n    \n        \n  \n  FAQ\n  \n    \n    \n  \n    \n\n\nNo matching items"
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html",
    "href": "components/workflows/rna/rna_multisample.html",
    "title": "Rna multisample",
    "section": "",
    "text": "ID: rna_multisample\nNamespace: workflows/rna\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html#example-commands",
    "href": "components/workflows/rna/rna_multisample.html#example-commands",
    "title": "Rna multisample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/rna/rna_multisample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"concatenated\"\ninput: # please fill in - example: \"dataset.h5mu\"\nmodality: \"rna\"\n# layer: \"foo\"\n\n# Output\n# output: \"$id.$key.output.h5mu\"\n\n# Filtering highly variable features\nhighly_variable_features_var_output: \"filter_with_hvg\"\nhighly_variable_features_obs_batch_key: \"sample_id\"\nhighly_variable_features_flavor: \"seurat\"\n# highly_variable_features_n_top_features: 123\n\n# QC metrics calculation options\nvar_qc_metrics: [\"filter_with_hvg\"]\ntop_n_vars: [50, 100, 200, 500]\noutput_obs_num_nonzero_vars: \"num_nonzero_vars\"\noutput_obs_total_counts_vars: \"total_counts\"\noutput_var_num_nonzero_obs: \"num_nonzero_obs\"\noutput_var_total_counts_obs: \"total_counts\"\noutput_var_obs_mean: \"obs_mean\"\noutput_var_pct_dropout: \"pct_dropout\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/rna/rna_multisample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html#argument-groups",
    "href": "components/workflows/rna/rna_multisample.html#argument-groups",
    "title": "Rna multisample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the concatenated file\nstring, required, example: \"concatenated\"\n\n\n--input\nPath to the samples.\nfile, required, example: \"dataset.h5mu\"\n\n\n--modality\nModality to process.\nstring, default: \"rna\"\n\n\n--layer\nInput layer to use. If not specified, .X is used.\nstring\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nFiltering highly variable features\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--highly_variable_features_var_output\nIn which .var slot to store a boolean array corresponding to the highly variable features.\nstring, default: \"filter_with_hvg\"\n\n\n--highly_variable_features_obs_batch_key\nIf specified, highly-variable features are selected within each batch separately and merged. This simple process avoids the selection of batch-specific features and acts as a lightweight batch correction method. For all flavors, featues are first sorted by how many batches they are highly variable. For dispersion-based flavors ties are broken by normalized dispersion. If flavor = ‘seurat_v3’, ties are broken by the median (across batches) rank based on within-batch normalized variance.\nstring, default: \"sample_id\"\n\n\n--highly_variable_features_flavor\nChoose the flavor for identifying highly variable features. For the dispersion based methods in their default workflows, Seurat passes the cutoffs whereas Cell Ranger passes n_top_features.\nstring, default: \"seurat\"\n\n\n--highly_variable_features_n_top_features\nNumber of highly-variable features to keep. Mandatory if filter_with_hvg_flavor is set to ‘seurat_v3’.\ninteger\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes.\nList of string, default: \"filter_with_hvg\", example: \"ercc,highly_variable\", multiple_sep: \";\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \";\"\n\n\n--output_obs_num_nonzero_vars\nName of column in .obs describing, for each observation, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each row the number of columns that contain data.\nstring, default: \"num_nonzero_vars\"\n\n\n--output_obs_total_counts_vars\nName of the column for .obs describing, for each observation (row), the sum of the stored values in the columns.\nstring, default: \"total_counts\"\n\n\n--output_var_num_nonzero_obs\nName of column describing, for each feature, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each column the number of rows that contain data.\nstring, default: \"num_nonzero_obs\"\n\n\n--output_var_total_counts_obs\nName of the column in .var describing, for each feature (column), the sum of the stored values in the rows.\nstring, default: \"total_counts\"\n\n\n--output_var_obs_mean\nName of the column in .obs providing the mean of the values in each row.\nstring, default: \"obs_mean\"\n\n\n--output_var_pct_dropout\nName of the column in .obs providing for each feature the percentage of observations the feature does not appear on (i.e. is missing). Same as --num_nonzero_obs but percentage based.\nstring, default: \"pct_dropout\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html#authors",
    "href": "components/workflows/rna/rna_multisample.html#authors",
    "title": "Rna multisample",
    "section": "Authors",
    "text": "Authors\n\nDries De Maeyer   (author)\nRobrecht Cannoodt    (author, maintainer)\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html#visualisation",
    "href": "components/workflows/rna/rna_multisample.html#visualisation",
    "title": "Rna multisample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(normalize_total)\n    v38(log1p)\n    v58(delete_layer)\n    v78(highly_variable_features_scanpy)\n    v90(filter)\n    v121(mix)\n    v189(Output)\n    subgraph group_rna_qc [rna_qc]\n        v108(grep_annotation_column)\n        v130(calculate_qc_metrics)\n        v150(publish)\n    end\n    v0--&gt;v18\n    v18--&gt;v38\n    v38--&gt;v58\n    v58--&gt;v78\n    v78--&gt;v90\n    v90--&gt;v108\n    v108--&gt;v121\n    v90--&gt;v121\n    v121--&gt;v130\n    v130--&gt;v150\n    v150--&gt;v189\n    style group_rna_qc fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v38 fill:#e3dcea,stroke:#7a4baa;\n    style v58 fill:#e3dcea,stroke:#7a4baa;\n    style v78 fill:#e3dcea,stroke:#7a4baa;\n    style v90 fill:#e3dcea,stroke:#7a4baa;\n    style v108 fill:#e3dcea,stroke:#7a4baa;\n    style v121 fill:#e3dcea,stroke:#7a4baa;\n    style v130 fill:#e3dcea,stroke:#7a4baa;\n    style v150 fill:#e3dcea,stroke:#7a4baa;\n    style v189 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html",
    "href": "components/workflows/multiomics/process_samples.html",
    "title": "Process samples",
    "section": "",
    "text": "ID: process_samples\nNamespace: workflows/multiomics\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html#example-commands",
    "href": "components/workflows/multiomics/process_samples.html#example-commands",
    "title": "Process samples",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/multiomics/process_samples/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\n# rna_layer: \"foo\"\n# prot_layer: \"foo\"\n# gdo_layer: \"foo\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Sample ID options\nadd_id_to_obs: true\nadd_id_obs_output: \"sample_id\"\nadd_id_make_observation_keys_unique: true\n\n# RNA filtering options\n# rna_min_counts: 200\n# rna_max_counts: 5000000\n# rna_min_genes_per_cell: 200\n# rna_max_genes_per_cell: 1500000\n# rna_min_cells_per_gene: 3\n# rna_min_fraction_mito: 0.0\n# rna_max_fraction_mito: 0.2\n\n# CITE-seq filtering options\n# prot_min_counts: 3\n# prot_max_counts: 5000000\n# prot_min_proteins_per_cell: 200\n# prot_max_proteins_per_cell: 100000000\n# prot_min_cells_per_protein: 3\n\n# GDO filtering options\n# gdo_min_counts: 3\n# gdo_max_counts: 5000000\n# gdo_min_guides_per_cell: 200\n# gdo_max_guides_per_cell: 100000000\n# gdo_min_cells_per_guide: 3\n\n# Highly variable features detection\nhighly_variable_features_var_output: \"filter_with_hvg\"\nhighly_variable_features_obs_batch_key: \"sample_id\"\n\n# Mitochondrial Gene Detection\n# var_name_mitochondrial_genes: \"foo\"\n# obs_name_mitochondrial_fraction: \"foo\"\n# var_gene_names: \"gene_symbol\"\nmitochondrial_gene_regex: \"^[mM][tT]-\"\n\n# QC metrics calculation options\n# var_qc_metrics: [\"ercc,highly_variable\"]\ntop_n_vars: [50, 100, 200, 500]\n\n# PCA options\npca_overwrite: false\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/multiomics/process_samples/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html#argument-groups",
    "href": "components/workflows/multiomics/process_samples.html#argument-groups",
    "title": "Process samples",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"input.h5mu\"\n\n\n--rna_layer\nInput layer for the gene expression modality. If not specified, .X is used.\nstring\n\n\n--prot_layer\nInput layer for the antibody capture modality. If not specified, .X is used.\nstring\n\n\n--gdo_layer\nInput layer for the guide-derived oligonucleotide (GDO) data. If not specified, .X is used.\nstring\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nSample ID options\nOptions for adding the id to .obs on the MuData object. Having a sample id present in a requirement of several components for this pipeline.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--add_id_to_obs\nAdd the value passed with –id to .obs.\nboolean, default: TRUE\n\n\n--add_id_obs_output\n.Obs column to add the sample IDs to. Required and only used when –add_id_to_obs is set to ‘true’\nstring, default: \"sample_id\"\n\n\n--add_id_make_observation_keys_unique\nJoin the id to the .obs index (.obs_names). Only used when –add_id_to_obs is set to ‘true’.\nboolean, default: TRUE\n\n\n\n\n\nRNA filtering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_min_counts\nMinimum number of counts captured per cell.\ninteger, example: 200\n\n\n--rna_max_counts\nMaximum number of counts captured per cell.\ninteger, example: 5000000\n\n\n--rna_min_genes_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--rna_max_genes_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 1500000\n\n\n--rna_min_cells_per_gene\nMinimum of non-zero values per gene.\ninteger, example: 3\n\n\n--rna_min_fraction_mito\nMinimum fraction of UMIs that are mitochondrial.\ndouble, example: 0\n\n\n--rna_max_fraction_mito\nMaximum fraction of UMIs that are mitochondrial.\ndouble, example: 0.2\n\n\n\n\n\nCITE-seq filtering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--prot_min_counts\nMinimum number of counts per cell.\ninteger, example: 3\n\n\n--prot_max_counts\nMinimum number of counts per cell.\ninteger, example: 5000000\n\n\n--prot_min_proteins_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--prot_max_proteins_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 100000000\n\n\n--prot_min_cells_per_protein\nMinimum of non-zero values per protein.\ninteger, example: 3\n\n\n\n\n\nGDO filtering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--gdo_min_counts\nMinimum number of counts per cell.\ninteger, example: 3\n\n\n--gdo_max_counts\nMinimum number of counts per cell.\ninteger, example: 5000000\n\n\n--gdo_min_guides_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--gdo_max_guides_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 100000000\n\n\n--gdo_min_cells_per_guide\nMinimum of non-zero values per guide.\ninteger, example: 3\n\n\n\n\n\nHighly variable features detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--highly_variable_features_var_output\nIn which .var slot to store a boolean array corresponding to the highly variable genes.\nstring, default: \"filter_with_hvg\"\n\n\n--highly_variable_features_obs_batch_key\nIf specified, highly-variable genes are selected within each batch separately and merged. This simple process avoids the selection of batch-specific genes and acts as a lightweight batch correction method.\nstring, default: \"sample_id\"\n\n\n\n\n\nMitochondrial Gene Detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_name_mitochondrial_genes\nIn which .var slot to store a boolean array corresponding the mitochondrial genes.\nstring\n\n\n--obs_name_mitochondrial_fraction\nWhen specified, write the fraction of counts originating from mitochondrial genes (based on –mitochondrial_gene_regex) to an .obs column with the specified name. Requires –var_name_mitochondrial_genes.\nstring\n\n\n--var_gene_names\n.var column name to be used to detect mitochondrial genes instead of .var_names (default if not set). Gene names matching with the regex value from –mitochondrial_gene_regex will be identified as a mitochondrial gene.\nstring, example: \"gene_symbol\"\n\n\n--mitochondrial_gene_regex\nRegex string that identifies mitochondrial genes from –var_gene_names. By default will detect human and mouse mitochondrial genes from a gene symbol.\nstring, default: \"^[mM][tT]-\"\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes. Defaults to the combined values specified for –var_name_mitochondrial_genes and –highly_variable_features_var_output.\nList of string, example: \"ercc,highly_variable\", multiple_sep: \";\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \";\"\n\n\n\n\n\nPCA options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--pca_overwrite\nAllow overwriting slots for PCA output.\nboolean_true",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html#authors",
    "href": "components/workflows/multiomics/process_samples.html#authors",
    "title": "Process samples",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html#visualisation",
    "href": "components/workflows/multiomics/process_samples.html#visualisation",
    "title": "Process samples",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v20(add_id)\n    v32(filter)\n    v76(flatMap)\n    v90(filter)\n    v121(mix)\n    v176(branch)\n    v192(concat)\n    v388(mix)\n    v390(mix)\n    v405(concatenate_h5mu)\n    v429(filter)\n    v473(flatMap)\n    v568(filter)\n    v599(mix)\n    v788(mix)\n    v693(filter)\n    v724(mix)\n    v790(mix)\n    v822(branch)\n    v896(concat)\n    v900(branch)\n    v974(concat)\n    v1022(Output)\n    subgraph group_split_modalities_workflow [split_modalities_workflow]\n        v48(split_modalities_component)\n    end\n    subgraph group_rna_singlesample [rna_singlesample]\n        v108(grep_annotation_column)\n        v130(calculate_qc_metrics)\n        v150(publish)\n        v181(delimit_fraction)\n        v201(rna_filter_with_counts)\n        v221(rna_do_filter)\n        v241(filter_with_scrublet)\n    end\n    subgraph group_prot_singlesample [prot_singlesample]\n        v282(prot_filter_with_counts)\n        v302(prot_do_filter)\n    end\n    subgraph group_gdo_singlesample [gdo_singlesample]\n        v344(gdo_filter_with_counts)\n        v364(gdo_do_filter)\n    end\n    subgraph group_process_batches [process_batches]\n        v445(split_modalities_component)\n        v806(merge)\n        v835(pca)\n        v855(find_neighbors)\n        v875(umap)\n        v913(pca)\n        v933(find_neighbors)\n        v953(umap)\n        v983(publish)\n        subgraph group_rna_multisample [rna_multisample]\n            v496(normalize_total)\n            v516(log1p)\n            v536(delete_layer)\n            v556(highly_variable_features_scanpy)\n            v586(grep_annotation_column)\n            v608(calculate_qc_metrics)\n            v628(publish)\n        end\n        subgraph group_prot_multisample [prot_multisample]\n            v681(clr)\n            v711(grep_annotation_column)\n            v733(calculate_qc_metrics)\n            v753(publish)\n        end\n    end\n    v176--&gt;v192\n    v388--&gt;v390\n    v788--&gt;v790\n    v822--&gt;v896\n    v900--&gt;v974\n    v176--&gt;v181\n    v0--&gt;v20\n    v20--&gt;v32\n    v32--&gt;v48\n    v48--&gt;v76\n    v76--&gt;v90\n    v90--&gt;v108\n    v108--&gt;v121\n    v90--&gt;v121\n    v121--&gt;v130\n    v130--&gt;v150\n    v150--&gt;v176\n    v181--&gt;v192\n    v192--&gt;v201\n    v201--&gt;v221\n    v221--&gt;v241\n    v241--&gt;v388\n    v76--&gt;v282\n    v282--&gt;v302\n    v302--&gt;v388\n    v76--&gt;v344\n    v344--&gt;v364\n    v364--&gt;v388\n    v76--&gt;v390\n    v390--&gt;v405\n    v405--&gt;v429\n    v429--&gt;v445\n    v445--&gt;v473\n    v473--&gt;v496\n    v496--&gt;v516\n    v516--&gt;v536\n    v536--&gt;v556\n    v556--&gt;v568\n    v568--&gt;v586\n    v586--&gt;v599\n    v568--&gt;v599\n    v599--&gt;v608\n    v608--&gt;v628\n    v628--&gt;v788\n    v473--&gt;v681\n    v681--&gt;v693\n    v693--&gt;v711\n    v711--&gt;v724\n    v693--&gt;v724\n    v724--&gt;v733\n    v733--&gt;v753\n    v753--&gt;v788\n    v473--&gt;v790\n    v790--&gt;v806\n    v806--&gt;v822\n    v822--&gt;v835\n    v835--&gt;v855\n    v855--&gt;v875\n    v875--&gt;v896\n    v896--&gt;v900\n    v900--&gt;v913\n    v913--&gt;v933\n    v933--&gt;v953\n    v953--&gt;v974\n    v974--&gt;v983\n    v983--&gt;v1022\n    style group_split_modalities_workflow fill:#F0F0F0,stroke:#969696;\n    style group_rna_singlesample fill:#F0F0F0,stroke:#969696;\n    style group_prot_singlesample fill:#F0F0F0,stroke:#969696;\n    style group_gdo_singlesample fill:#F0F0F0,stroke:#969696;\n    style group_process_batches fill:#F0F0F0,stroke:#969696;\n    style group_rna_multisample fill:#D9D9D9,stroke:#737373;\n    style group_prot_multisample fill:#D9D9D9,stroke:#737373;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v20 fill:#e3dcea,stroke:#7a4baa;\n    style v32 fill:#e3dcea,stroke:#7a4baa;\n    style v48 fill:#e3dcea,stroke:#7a4baa;\n    style v76 fill:#e3dcea,stroke:#7a4baa;\n    style v90 fill:#e3dcea,stroke:#7a4baa;\n    style v108 fill:#e3dcea,stroke:#7a4baa;\n    style v121 fill:#e3dcea,stroke:#7a4baa;\n    style v130 fill:#e3dcea,stroke:#7a4baa;\n    style v150 fill:#e3dcea,stroke:#7a4baa;\n    style v176 fill:#e3dcea,stroke:#7a4baa;\n    style v192 fill:#e3dcea,stroke:#7a4baa;\n    style v181 fill:#e3dcea,stroke:#7a4baa;\n    style v201 fill:#e3dcea,stroke:#7a4baa;\n    style v221 fill:#e3dcea,stroke:#7a4baa;\n    style v241 fill:#e3dcea,stroke:#7a4baa;\n    style v388 fill:#e3dcea,stroke:#7a4baa;\n    style v282 fill:#e3dcea,stroke:#7a4baa;\n    style v302 fill:#e3dcea,stroke:#7a4baa;\n    style v344 fill:#e3dcea,stroke:#7a4baa;\n    style v364 fill:#e3dcea,stroke:#7a4baa;\n    style v390 fill:#e3dcea,stroke:#7a4baa;\n    style v405 fill:#e3dcea,stroke:#7a4baa;\n    style v429 fill:#e3dcea,stroke:#7a4baa;\n    style v445 fill:#e3dcea,stroke:#7a4baa;\n    style v473 fill:#e3dcea,stroke:#7a4baa;\n    style v496 fill:#e3dcea,stroke:#7a4baa;\n    style v516 fill:#e3dcea,stroke:#7a4baa;\n    style v536 fill:#e3dcea,stroke:#7a4baa;\n    style v556 fill:#e3dcea,stroke:#7a4baa;\n    style v568 fill:#e3dcea,stroke:#7a4baa;\n    style v586 fill:#e3dcea,stroke:#7a4baa;\n    style v599 fill:#e3dcea,stroke:#7a4baa;\n    style v608 fill:#e3dcea,stroke:#7a4baa;\n    style v628 fill:#e3dcea,stroke:#7a4baa;\n    style v788 fill:#e3dcea,stroke:#7a4baa;\n    style v681 fill:#e3dcea,stroke:#7a4baa;\n    style v693 fill:#e3dcea,stroke:#7a4baa;\n    style v711 fill:#e3dcea,stroke:#7a4baa;\n    style v724 fill:#e3dcea,stroke:#7a4baa;\n    style v733 fill:#e3dcea,stroke:#7a4baa;\n    style v753 fill:#e3dcea,stroke:#7a4baa;\n    style v790 fill:#e3dcea,stroke:#7a4baa;\n    style v806 fill:#e3dcea,stroke:#7a4baa;\n    style v822 fill:#e3dcea,stroke:#7a4baa;\n    style v896 fill:#e3dcea,stroke:#7a4baa;\n    style v835 fill:#e3dcea,stroke:#7a4baa;\n    style v855 fill:#e3dcea,stroke:#7a4baa;\n    style v875 fill:#e3dcea,stroke:#7a4baa;\n    style v900 fill:#e3dcea,stroke:#7a4baa;\n    style v974 fill:#e3dcea,stroke:#7a4baa;\n    style v913 fill:#e3dcea,stroke:#7a4baa;\n    style v933 fill:#e3dcea,stroke:#7a4baa;\n    style v953 fill:#e3dcea,stroke:#7a4baa;\n    style v983 fill:#e3dcea,stroke:#7a4baa;\n    style v1022 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html",
    "href": "components/workflows/multiomics/split_modalities.html",
    "title": "Split modalities",
    "section": "",
    "text": "ID: split_modalities\nNamespace: workflows/multiomics\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html#example-commands",
    "href": "components/workflows/multiomics/split_modalities.html#example-commands",
    "title": "Split modalities",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/multiomics/split_modalities/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\n\n# Outputs\n# output: \"$id.$key.output.output\"\n# output_types: \"$id.$key.output_types.csv\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/multiomics/split_modalities/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html#argument-groups",
    "href": "components/workflows/multiomics/split_modalities.html#argument-groups",
    "title": "Split modalities",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"input.h5mu\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nOutput directory containing multiple h5mu files.\nfile, required, example: \"/path/to/output\"\n\n\n--output_types\nA csv containing the base filename and modality type per output file.\nfile, required, example: \"types.csv\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html#authors",
    "href": "components/workflows/multiomics/split_modalities.html#authors",
    "title": "Split modalities",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html#visualisation",
    "href": "components/workflows/multiomics/split_modalities.html#visualisation",
    "title": "Split modalities",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v18(split_modalities_component)\n    v51(Output)\n    v0--&gt;v2\n    v2--&gt;v18\n    v18--&gt;v51\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v51 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html",
    "href": "components/workflows/prot/prot_multisample.html",
    "title": "Prot multisample",
    "section": "",
    "text": "ID: prot_multisample\nNamespace: workflows/prot\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html#example-commands",
    "href": "components/workflows/prot/prot_multisample.html#example-commands",
    "title": "Prot multisample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/prot/prot_multisample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"concatenated\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# QC metrics calculation options\n# var_qc_metrics: [\"ercc,highly_variable\"]\ntop_n_vars: [50, 100, 200, 500]\noutput_obs_num_nonzero_vars: \"num_nonzero_vars\"\noutput_obs_total_counts_vars: \"total_counts\"\noutput_var_num_nonzero_obs: \"num_nonzero_obs\"\noutput_var_total_counts_obs: \"total_counts\"\noutput_var_obs_mean: \"obs_mean\"\noutput_var_pct_dropout: \"pct_dropout\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/prot/prot_multisample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html#argument-groups",
    "href": "components/workflows/prot/prot_multisample.html#argument-groups",
    "title": "Prot multisample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the concatenated file\nstring, required, example: \"concatenated\"\n\n\n--input\nPath to the samples.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nInput layer to use. If not specified, .X is used.\nstring\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes. Defaults to the value from –var_name_mitochondrial_genes.\nList of string, example: \"ercc,highly_variable\", multiple_sep: \";\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \";\"\n\n\n--output_obs_num_nonzero_vars\nName of column in .obs describing, for each observation, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each row the number of columns that contain data.\nstring, default: \"num_nonzero_vars\"\n\n\n--output_obs_total_counts_vars\nName of the column for .obs describing, for each observation (row), the sum of the stored values in the columns.\nstring, default: \"total_counts\"\n\n\n--output_var_num_nonzero_obs\nName of column describing, for each feature, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each column the number of rows that contain data.\nstring, default: \"num_nonzero_obs\"\n\n\n--output_var_total_counts_obs\nName of the column in .var describing, for each feature (column), the sum of the stored values in the rows.\nstring, default: \"total_counts\"\n\n\n--output_var_obs_mean\nName of the column in .obs providing the mean of the values in each row.\nstring, default: \"obs_mean\"\n\n\n--output_var_pct_dropout\nName of the column in .obs providing for each feature the percentage of observations the feature does not appear on (i.e. is missing). Same as --output_var_num_nonzero_obs but percentage based.\nstring, default: \"pct_dropout\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html#authors",
    "href": "components/workflows/prot/prot_multisample.html#authors",
    "title": "Prot multisample",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html#visualisation",
    "href": "components/workflows/prot/prot_multisample.html#visualisation",
    "title": "Prot multisample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(clr)\n    v30(filter)\n    v61(mix)\n    v129(Output)\n    subgraph group_prot_qc [prot_qc]\n        v48(grep_annotation_column)\n        v70(calculate_qc_metrics)\n        v90(publish)\n    end\n    v0--&gt;v18\n    v18--&gt;v30\n    v30--&gt;v48\n    v48--&gt;v61\n    v30--&gt;v61\n    v61--&gt;v70\n    v70--&gt;v90\n    v90--&gt;v129\n    style group_prot_qc fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v30 fill:#e3dcea,stroke:#7a4baa;\n    style v48 fill:#e3dcea,stroke:#7a4baa;\n    style v61 fill:#e3dcea,stroke:#7a4baa;\n    style v70 fill:#e3dcea,stroke:#7a4baa;\n    style v90 fill:#e3dcea,stroke:#7a4baa;\n    style v129 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html",
    "href": "components/workflows/ingestion/cellranger_mapping.html",
    "title": "Cell Ranger mapping",
    "section": "",
    "text": "ID: cellranger_mapping\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html#example-commands",
    "href": "components/workflows/ingestion/cellranger_mapping.html#example-commands",
    "title": "Cell Ranger mapping",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_mapping/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: [\"sample_S1_L001_R1_001.fastq.gz\", \"sample_S1_L001_R2_001.fastq.gz\"]\nreference: # please fill in - example: \"reference.tar.gz\"\n\n# Outputs\n# output_raw: \"$id.$key.output_raw.output_raw\"\n# output_h5mu: \"$id.$key.output_h5mu.h5mu\"\nuns_metrics: \"metrics_summary\"\noutput_type: \"raw\"\n\n# Cell Ranger arguments\n# expect_cells: 3000\nchemistry: \"auto\"\nsecondary_analysis: false\ngenerate_bam: true\ninclude_introns: true\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_mapping/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html#argument-groups",
    "href": "components/workflows/ingestion/cellranger_mapping.html#argument-groups",
    "title": "Cell Ranger mapping",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nThe fastq.gz files to align. Can also be a single directory containing fastq.gz files.\nList of file, required, example: \"sample_S1_L001_R1_001.fastq.gz\", \"sample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--reference\nThe path to Cell Ranger reference tar.gz file.\nfile, required, example: \"reference.tar.gz\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output_raw\nLocation where the output folder from Cell Ranger will be stored.\nfile, required, example: \"output_dir\"\n\n\n--output_h5mu\nThe output from Cell Ranger, converted to h5mu.\nfile, required, example: \"output.h5mu\"\n\n\n--uns_metrics\nName of the .uns slot under which to QC metrics (if any).\nstring, default: \"metrics_summary\"\n\n\n--output_type\nWhich Cell Ranger output to use for converting to h5mu.\nstring, default: \"raw\"\n\n\n\n\n\nCell Ranger arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--expect_cells\nExpected number of recovered cells, used as input to cell calling algorithm.\ninteger, example: 3000\n\n\n--chemistry\nAssay configuration. - auto: autodetect mode - threeprime: Single Cell 3’ - fiveprime: Single Cell 5’ - SC3Pv1: Single Cell 3’ v1 - SC3Pv2: Single Cell 3’ v2 - SC3Pv3: Single Cell 3’ v3 - SC3Pv3LT: Single Cell 3’ v3 LT - SC3Pv3HT: Single Cell 3’ v3 HT - SC5P-PE: Single Cell 5’ paired-end - SC5P-R2: Single Cell 5’ R2-only - SC-FB: Single Cell Antibody-only 3’ v2 or 5’ See https://kb.10xgenomics.com/hc/en-us/articles/115003764132-How-does-Cell-Ranger-auto-detect-chemistry- for more information.\nstring, default: \"auto\"\n\n\n--secondary_analysis\nWhether or not to run the secondary analysis e.g. clustering.\nboolean, default: FALSE\n\n\n--generate_bam\nWhether to generate a BAM file.\nboolean, default: TRUE\n\n\n--include_introns\nInclude intronic reads in count (default=true unless –target-panel is specified in which case default=false)\nboolean, default: TRUE",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html#authors",
    "href": "components/workflows/ingestion/cellranger_mapping.html#authors",
    "title": "Cell Ranger mapping",
    "section": "Authors",
    "text": "Authors\n\nAngela Oliveira Pisco    (author)\nRobrecht Cannoodt    (author, maintainer)\nDries De Maeyer   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html#visualisation",
    "href": "components/workflows/ingestion/cellranger_mapping.html#visualisation",
    "title": "Cell Ranger mapping",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v17(cellranger_count)\n    v37(cellranger_count_split)\n    v57(from_10xh5_to_h5mu)\n    v84(Output)\n    v0--&gt;v17\n    v17--&gt;v37\n    v37--&gt;v57\n    v57--&gt;v84\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v17 fill:#e3dcea,stroke:#7a4baa;\n    style v37 fill:#e3dcea,stroke:#7a4baa;\n    style v57 fill:#e3dcea,stroke:#7a4baa;\n    style v84 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html",
    "title": "Cell Ranger post-processing",
    "section": "",
    "text": "ID: cellranger_postprocessing\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html#example-commands",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html#example-commands",
    "title": "Cell Ranger post-processing",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_postprocessing/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\n\n# Outputs\n# output: \"$id.$key.output.output\"\n\n# Correction arguments\nperform_correction: false\ncellbender_epochs: 150\n\n# Filtering arguments\n# min_genes: 100\n# min_counts: 1000\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_postprocessing/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html#argument-groups",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html#argument-groups",
    "title": "Cell Ranger post-processing",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nInput h5mu file created by running Cell Ranger and converting its output to h5mu.\nfile, required, example: \"input.h5mu\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nThe converted h5mu file.\nfile\n\n\n\n\n\nCorrection arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--perform_correction\nWhether or not to run CellBender to perform count correction.\nboolean_true\n\n\n--cellbender_epochs\nNumber of epochs to run CellBender for.\ninteger, default: 150\n\n\n\n\n\nFiltering arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--min_genes\nMinimum number of counts required for a cell to pass filtering.\ninteger, example: 100\n\n\n--min_counts\nMinimum number of genes expressed required for a cell to pass filtering.\ninteger, example: 1000",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html#authors",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html#authors",
    "title": "Cell Ranger post-processing",
    "section": "Authors",
    "text": "Authors\n\nAngela Oliveira Pisco    (author)\nRobrecht Cannoodt    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html#visualisation",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html#visualisation",
    "title": "Cell Ranger post-processing",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v18(cellbender_remove_background)\n    v31(mix)\n    v41(filter_with_counts)\n    v54(mix)\n    v63(publish)\n    v90(Output)\n    v0--&gt;v2\n    v2--&gt;v18\n    v18--&gt;v31\n    v2--&gt;v31\n    v31--&gt;v41\n    v41--&gt;v54\n    v31--&gt;v54\n    v54--&gt;v63\n    v63--&gt;v90\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v31 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v54 fill:#e3dcea,stroke:#7a4baa;\n    style v63 fill:#e3dcea,stroke:#7a4baa;\n    style v90 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html",
    "href": "components/workflows/ingestion/cellranger_multi.html",
    "title": "Cell Ranger multi",
    "section": "",
    "text": "ID: cellranger_multi\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html#example-commands",
    "href": "components/workflows/ingestion/cellranger_multi.html#example-commands",
    "title": "Cell Ranger multi",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_multi/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\n# input: [\"sample_S1_L001_R1_001.fastq.gz\", \"sample_S1_L001_R2_001.fastq.gz\"]\ngex_reference: # please fill in - example: \"reference_genome.tar.gz\"\n# vdj_reference: \"reference_vdj.tar.gz\"\n# feature_reference: \"feature_reference.csv\"\n# vdj_inner_enrichment_primers: \"enrichment_primers.txt\"\n\n# Outputs\n# output_raw: \"$id.$key.output_raw.output_raw\"\n# output_h5mu: \"$id.$key.output_h5mu.h5mu\"\nuns_metrics: \"metrics_cellranger\"\n\n# Feature type-specific input files\n# gex_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# abc_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# cgc_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# mux_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# vdj_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# vdj_t_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# vdj_t_gd_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# vdj_b_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# agc_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n\n# Cell multiplexing parameters\n# cell_multiplex_sample_id: \"foo\"\n# cell_multiplex_oligo_ids: \"foo\"\n# cell_multiplex_description: \"foo\"\n\n# Gene expression arguments\n# gex_expect_cells: 3000\ngex_chemistry: \"auto\"\ngex_secondary_analysis: false\ngex_generate_bam: true\ngex_include_introns: true\n\n# Library arguments\n# library_id: [\"mysample1\"]\n# library_type: [\"Gene Expression\"]\n# library_subsample: [\"0.5\"]\n# library_lanes: [\"1-4\"]\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_multi/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html#argument-groups",
    "href": "components/workflows/ingestion/cellranger_multi.html#argument-groups",
    "title": "Cell Ranger multi",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nThe fastq.gz files to align. Can also be a single directory containing fastq.gz files.\nList of file, example: \"sample_S1_L001_R1_001.fastq.gz\", \"sample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--gex_reference\nGenome refence index built by Cell Ranger mkref.\nfile, required, example: \"reference_genome.tar.gz\"\n\n\n--vdj_reference\nVDJ refence index built by Cell Ranger mkref.\nfile, example: \"reference_vdj.tar.gz\"\n\n\n--feature_reference\nPath to the Feature reference CSV file, declaring Feature Barcode constructs and associated barcodes. Required only for Antibody Capture or CRISPR Guide Capture libraries. See https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/feature-bc-analysis#feature-ref for more information.\nfile, example: \"feature_reference.csv\"\n\n\n--vdj_inner_enrichment_primers\nV(D)J Immune Profiling libraries: if inner enrichment primers other than those provided in the 10x Genomics kits are used, they need to be specified here as a text file with one primer per line.\nfile, example: \"enrichment_primers.txt\"\n\n\n\n\n\nFeature type-specific input files\nHelper functionality to allow feature type-specific input files, without the need to specify library_type or library_id. The library_id will be inferred from the input paths.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--gex_input\nThe FASTQ files to be analyzed for Gene Expression. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--abc_input\nThe FASTQ files to be analyzed for Antibody Capture. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--cgc_input\nThe FASTQ files to be analyzed for CRISPR Guide Capture. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--mux_input\nThe FASTQ files to be analyzed for Multiplexing Capture. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--vdj_input\nThe FASTQ files to be analyzed for VDJ. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--vdj_t_input\nThe FASTQ files to be analyzed for VDJ-T. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--vdj_t_gd_input\nThe FASTQ files to be analyzed for VDJ-T-GD. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--vdj_b_input\nThe FASTQ files to be analyzed for VDJ-B. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--agc_input\nThe FASTQ files to be analyzed for Antigen Capture. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output_raw\nThe raw output folder.\nfile, required, example: \"output_dir\"\n\n\n--output_h5mu\nThe converted h5mu file.\nfile, required, example: \"output.h5mu\"\n\n\n--uns_metrics\nName of the .uns slot under which to QC metrics (if any).\nstring, default: \"metrics_cellranger\"\n\n\n\n\n\nCell multiplexing parameters\nArguments related to cell multiplexing.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--cell_multiplex_sample_id\nA name to identify a multiplexed sample. Must be alphanumeric with hyphens and/or underscores, and less than 64 characters. Required for Cell Multiplexing libraries.\nstring\n\n\n--cell_multiplex_oligo_ids\nThe Cell Multiplexing oligo IDs used to multiplex this sample. If multiple CMOs were used for a sample, separate IDs with a pipe (e.g., CMO301|CMO302). Required for Cell Multiplexing libraries.\nstring\n\n\n--cell_multiplex_description\nA description for the sample.\nstring\n\n\n\n\n\nGene expression arguments\nArguments relevant to the analysis of gene expression data.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--gex_expect_cells\nExpected number of recovered cells, used as input to cell calling algorithm.\ninteger, example: 3000\n\n\n--gex_chemistry\nAssay configuration. - auto: autodetect mode - threeprime: Single Cell 3’ - fiveprime: Single Cell 5’ - SC3Pv1: Single Cell 3’ v1 - SC3Pv2: Single Cell 3’ v2 - SC3Pv3: Single Cell 3’ v3 - SC3Pv3LT: Single Cell 3’ v3 LT - SC3Pv3HT: Single Cell 3’ v3 HT - SC5P-PE: Single Cell 5’ paired-end - SC5P-R2: Single Cell 5’ R2-only - SC-FB: Single Cell Antibody-only 3’ v2 or 5’ See https://kb.10xgenomics.com/hc/en-us/articles/115003764132-How-does-Cell-Ranger-auto-detect-chemistry- for more information.\nstring, default: \"auto\"\n\n\n--gex_secondary_analysis\nWhether or not to run the secondary analysis e.g. clustering.\nboolean, default: FALSE\n\n\n--gex_generate_bam\nWhether to generate a BAM file.\nboolean, default: TRUE\n\n\n--gex_include_introns\nInclude intronic reads in count (default=true unless –target-panel is specified in which case default=false)\nboolean, default: TRUE\n\n\n\n\n\nLibrary arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--library_id\nThe Illumina sample name to analyze. This must exactly match the ‘Sample Name’ part of the FASTQ files specified in the --input argument.\nList of string, example: \"mysample1\", multiple_sep: \";\"\n\n\n--library_type\nThe underlying feature type of the library. Possible values: “Gene Expression”, “VDJ”, “VDJ-T”, “VDJ-B”, “Antibody Capture”, “CRISPR Guide Capture”, “Multiplexing Capture”\nList of string, example: \"Gene Expression\", multiple_sep: \";\"\n\n\n--library_subsample\nOptional. The rate at which reads from the provided FASTQ files are sampled. Must be strictly greater than 0 and less than or equal to 1.\nList of string, example: \"0.5\", multiple_sep: \";\"\n\n\n--library_lanes\nLanes associated with this sample. Defaults to using all lanes.\nList of string, example: \"1-4\", multiple_sep: \";\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html#authors",
    "href": "components/workflows/ingestion/cellranger_multi.html#authors",
    "title": "Cell Ranger multi",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html#visualisation",
    "href": "components/workflows/ingestion/cellranger_multi.html#visualisation",
    "title": "Cell Ranger multi",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v17(cellranger_multi_component)\n    v37(from_cellranger_multi_to_h5mu)\n    v64(Output)\n    v0--&gt;v17\n    v17--&gt;v37\n    v37--&gt;v64\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v17 fill:#e3dcea,stroke:#7a4baa;\n    style v37 fill:#e3dcea,stroke:#7a4baa;\n    style v64 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html",
    "href": "components/workflows/ingestion/demux.html",
    "title": "Demux",
    "section": "",
    "text": "ID: demux\nNamespace: workflows/ingestion\n\n\n\nSource\nConvert .bcl files to .fastq files using bcl2fastq, bcl-convert or Cell Ranger mkfastq.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html#example-commands",
    "href": "components/workflows/ingestion/demux.html#example-commands",
    "title": "Demux",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/ingestion/demux/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Arguments\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"bcl_dir\"\nsample_sheet: # please fill in - example: \"bcl_dir\"\ndemultiplexer: \"bcl2fastq\"\n# ignore_missing: true\n# output_fastq: \"$id.$key.output_fastq.output_fastq\"\n# output_fastqc: \"$id.$key.output_fastqc.output_fastqc\"\n# output_multiqc: \"$id.$key.output_multiqc.output_multiqc\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/demux/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html#argument-group",
    "href": "components/workflows/ingestion/demux.html#argument-group",
    "title": "Demux",
    "section": "Argument group",
    "text": "Argument group\n\nArguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nInput run directory\nfile, required, example: \"bcl_dir\"\n\n\n--sample_sheet\nPointer to the sample sheet\nfile, required, example: \"bcl_dir\"\n\n\n--demultiplexer\nThe multiplexer to use, one of bclconvert or mkfastq\nstring, default: \"bcl2fastq\"\n\n\n--ignore_missing\nShould the demultiplexer ignore missing entities (filter, …)\nboolean\n\n\n--output_fastq\nOutput directory containig fastq files\nfile, required, example: \"fastq_dir\"\n\n\n--output_fastqc\nReports directory produced by FastQC\nfile, example: \"reports_dir\"\n\n\n--output_multiqc\nReports directory produced by MultiQC\nfile, example: \"reports_dir\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html#authors",
    "href": "components/workflows/ingestion/demux.html#authors",
    "title": "Demux",
    "section": "Authors",
    "text": "Authors\n\nToni Verbeiren   (author, maintainer)\nMarijke Van Moerbeke    (author)\nAngela Oliveira Pisco    (author)\nSamuel D’Souza   (author)\nRobrecht Cannoodt    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html#visualisation",
    "href": "components/workflows/ingestion/demux.html#visualisation",
    "title": "Demux",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v19(cellranger_mkfastq)\n    v78(mix)\n    v42(bcl_convert)\n    v65(bcl2fastq)\n    v87(fastqc)\n    v107(multiqc)\n    v135(Output)\n    v0--&gt;v2\n    v2--&gt;v19\n    v19--&gt;v78\n    v2--&gt;v42\n    v42--&gt;v78\n    v2--&gt;v65\n    v65--&gt;v78\n    v78--&gt;v87\n    v87--&gt;v107\n    v107--&gt;v135\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v19 fill:#e3dcea,stroke:#7a4baa;\n    style v78 fill:#e3dcea,stroke:#7a4baa;\n    style v42 fill:#e3dcea,stroke:#7a4baa;\n    style v65 fill:#e3dcea,stroke:#7a4baa;\n    style v87 fill:#e3dcea,stroke:#7a4baa;\n    style v107 fill:#e3dcea,stroke:#7a4baa;\n    style v135 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html",
    "href": "components/workflows/integration/bbknn_leiden.html",
    "title": "Bbknn leiden",
    "section": "",
    "text": "ID: bbknn_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html#example-commands",
    "href": "components/workflows/integration/bbknn_leiden.html#example-commands",
    "title": "Bbknn leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/integration/bbknn_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Bbknn\nobsm_input: \"X_pca\"\nobs_batch: \"sample_id\"\nuns_output: \"bbknn_integration_neighbors\"\nobsp_distances: \"bbknn_integration_distances\"\nobsp_connectivities: \"bbknn_integration_connectivities\"\nn_neighbors_within_batch: 3\nn_pcs: 50\n# n_trim: 123\n\n# Clustering options\nobs_cluster: \"bbknn_integration_leiden\"\nleiden_resolution: [1.0]\n\n# UMAP options\nobsm_umap: \"X_leiden_bbknn_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/bbknn_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html#argument-groups",
    "href": "components/workflows/integration/bbknn_leiden.html#argument-groups",
    "title": "Bbknn leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nBbknn\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_input\nThe dimensionality reduction in .obsm to use for neighbour detection. Defaults to X_pca.\nstring, default: \"X_pca\"\n\n\n--obs_batch\n.obs column name discriminating between your batches.\nstring, default: \"sample_id\"\n\n\n--uns_output\nMandatory .uns slot to store various neighbor output objects.\nstring, default: \"bbknn_integration_neighbors\"\n\n\n--obsp_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"bbknn_integration_distances\"\n\n\n--obsp_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"bbknn_integration_connectivities\"\n\n\n--n_neighbors_within_batch\nHow many top neighbours to report for each batch; total number of neighbours in the initial k-nearest-neighbours computation will be this number times the number of batches.\ninteger, default: 3\n\n\n--n_pcs\nHow many dimensions (in case of PCA, principal components) to use in the analysis.\ninteger, default: 50\n\n\n--n_trim\nTrim the neighbours of each cell to these many top connectivities. May help with population independence and improve the tidiness of clustering. The lower the value the more independent the individual populations, at the cost of more conserved batch effect. If None (default), sets the parameter value automatically to 10 times neighbors_within_batch times the number of batches. Set to 0 to skip.\ninteger\n\n\n\n\n\nClustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"bbknn_integration_leiden\"\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUMAP options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_leiden_bbknn_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html#authors",
    "href": "components/workflows/integration/bbknn_leiden.html#authors",
    "title": "Bbknn leiden",
    "section": "Authors",
    "text": "Authors\n\nMauro Saporita   (author)\nPovilas Gibas   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html#visualisation",
    "href": "components/workflows/integration/bbknn_leiden.html#visualisation",
    "title": "Bbknn leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v29(concat)\n    v18(bbknn)\n    v39(leiden)\n    v59(move_obsm_to_obs)\n    v72(mix)\n    v81(umap)\n    v108(Output)\n    v0--&gt;v18\n    v18--&gt;v29\n    v29--&gt;v39\n    v39--&gt;v59\n    v59--&gt;v72\n    v29--&gt;v72\n    v72--&gt;v81\n    v81--&gt;v108\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v29 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v39 fill:#e3dcea,stroke:#7a4baa;\n    style v59 fill:#e3dcea,stroke:#7a4baa;\n    style v72 fill:#e3dcea,stroke:#7a4baa;\n    style v81 fill:#e3dcea,stroke:#7a4baa;\n    style v108 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html",
    "href": "components/workflows/integration/scvi_leiden.html",
    "title": "Scvi leiden",
    "section": "",
    "text": "ID: scvi_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html#example-commands",
    "href": "components/workflows/integration/scvi_leiden.html#example-commands",
    "title": "Scvi leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/integration/scvi_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n# output_model: \"$id.$key.output_model.output_model\"\n\n# Neighbour calculation\nuns_neighbors: \"scvi_integration_neighbors\"\nobsp_neighbor_distances: \"scvi_integration_distances\"\nobsp_neighbor_connectivities: \"scvi_integration_connectivities\"\n\n# Scvi integration options\nobs_batch: # please fill in - example: \"foo\"\nobsm_output: \"X_scvi_integrated\"\n# var_input: \"foo\"\n# early_stopping: true\nearly_stopping_monitor: \"elbo_validation\"\nearly_stopping_patience: 45\nearly_stopping_min_delta: 0.0\n# max_epochs: 123\nreduce_lr_on_plateau: true\nlr_factor: 0.6\nlr_patience: 30.0\n\n# Clustering options\nobs_cluster: \"scvi_integration_leiden\"\nleiden_resolution: [1.0]\n\n# Umap options\nobsm_umap: \"X_scvi_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/scvi_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html#argument-groups",
    "href": "components/workflows/integration/scvi_leiden.html#argument-groups",
    "title": "Scvi leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n--output_model\nFolder where the state of the trained model will be saved to.\nfile, required, example: \"output_dir\"\n\n\n\n\n\nNeighbour calculation\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"scvi_integration_neighbors\"\n\n\n--obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"scvi_integration_distances\"\n\n\n--obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"scvi_integration_connectivities\"\n\n\n\n\n\nScvi integration options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_batch\nColumn name discriminating between your batches.\nstring, required\n\n\n--obsm_output\nIn which .obsm slot to store the resulting integrated embedding.\nstring, default: \"X_scvi_integrated\"\n\n\n--var_input\n.var column containing highly variable genes. By default, do not subset genes.\nstring\n\n\n--early_stopping\nWhether to perform early stopping with respect to the validation set.\nboolean\n\n\n--early_stopping_monitor\nMetric logged during validation set epoch.\nstring, default: \"elbo_validation\"\n\n\n--early_stopping_patience\nNumber of validation epochs with no improvement after which training will be stopped.\ninteger, default: 45\n\n\n--early_stopping_min_delta\nMinimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\ndouble, default: 0\n\n\n--max_epochs\nNumber of passes through the dataset, defaults to (20000 / number of cells) * 400 or 400; whichever is smallest.\ninteger\n\n\n--reduce_lr_on_plateau\nWhether to monitor validation loss and reduce learning rate when validation set lr_scheduler_metric plateaus.\nboolean, default: TRUE\n\n\n--lr_factor\nFactor to reduce learning rate.\ndouble, default: 0.6\n\n\n--lr_patience\nNumber of epochs with no improvement after which learning rate will be reduced.\ndouble, default: 30\n\n\n\n\n\nClustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"scvi_integration_leiden\"\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_scvi_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html#authors",
    "href": "components/workflows/integration/scvi_leiden.html#authors",
    "title": "Scvi leiden",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html#visualisation",
    "href": "components/workflows/integration/scvi_leiden.html#visualisation",
    "title": "Scvi leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(scvi)\n    v49(concat)\n    v38(find_neighbors)\n    v59(leiden)\n    v79(move_obsm_to_obs)\n    v92(mix)\n    v101(umap)\n    v128(Output)\n    v0--&gt;v18\n    v18--&gt;v38\n    v38--&gt;v49\n    v49--&gt;v59\n    v59--&gt;v79\n    v79--&gt;v92\n    v49--&gt;v92\n    v92--&gt;v101\n    v101--&gt;v128\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v38 fill:#e3dcea,stroke:#7a4baa;\n    style v59 fill:#e3dcea,stroke:#7a4baa;\n    style v79 fill:#e3dcea,stroke:#7a4baa;\n    style v92 fill:#e3dcea,stroke:#7a4baa;\n    style v101 fill:#e3dcea,stroke:#7a4baa;\n    style v128 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html",
    "href": "components/workflows/integration/totalvi_leiden.html",
    "title": "Totalvi leiden",
    "section": "",
    "text": "ID: totalvi_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html#example-commands",
    "href": "components/workflows/integration/totalvi_leiden.html#example-commands",
    "title": "Totalvi leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -main-script target/nextflow/workflows/integration/totalvi_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\nprot_modality: \"prot\"\nreference: # please fill in - example: \"path/to/file\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n# reference_model_path: \"$id.$key.reference_model_path.reference_model_path\"\n# query_model_path: \"$id.$key.query_model_path.query_model_path\"\n\n# General TotalVI Options\nobs_batch: \"sample_id\"\nmax_epochs: 400\nmax_query_epochs: 200\nweight_decay: 0.0\nforce_retrain: false\n# var_input: \"foo\"\n\n# TotalVI integration options RNA\nrna_reference_modality: \"rna\"\nrna_obsm_output: \"X_totalvi\"\n\n# TotalVI integration options ADT\nprot_reference_modality: \"prot\"\nprot_obsm_output: \"X_totalvi\"\n\n# Neighbour calculation RNA\nrna_uns_neighbors: \"totalvi_integration_neighbors\"\nrna_obsp_neighbor_distances: \"totalvi_integration_distances\"\nrna_obsp_neighbor_connectivities: \"totalvi_integration_connectivities\"\n\n# Neighbour calculation ADT\nprot_uns_neighbors: \"totalvi_integration_neighbors\"\nprot_obsp_neighbor_distances: \"totalvi_integration_distances\"\nprot_obsp_neighbor_connectivities: \"totalvi_integration_connectivities\"\n\n# Clustering options RNA\nrna_obs_cluster: \"totalvi_integration_leiden\"\nrna_leiden_resolution: [1.0]\n\n# Clustering options ADT\nprot_obs_cluster: \"totalvi_integration_leiden\"\nprot_leiden_resolution: [1.0]\n\n# Umap options\nobsm_umap: \"X_totalvi_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 1.0.5 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/totalvi_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html#argument-groups",
    "href": "components/workflows/integration/totalvi_leiden.html#argument-groups",
    "title": "Totalvi leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n--prot_modality\nWhich modality to process.\nstring, default: \"prot\"\n\n\n--reference\nInput h5mu file with reference data to train the TOTALVI model.\nfile, required\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n--reference_model_path\nDirectory with the reference model. If not exists, trained model will be saved there\nfile, default: \"totalvi_model_reference\"\n\n\n--query_model_path\nDirectory, where the query model will be saved\nfile, default: \"totalvi_model_query\"\n\n\n\n\n\nGeneral TotalVI Options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_batch\n.Obs column name discriminating between your batches.\nstring, default: \"sample_id\"\n\n\n--max_epochs\nNumber of passes through the dataset\ninteger, default: 400\n\n\n--max_query_epochs\nNumber of passes through the dataset, when fine-tuning model for query\ninteger, default: 200\n\n\n--weight_decay\nWeight decay, when fine-tuning model for query\ndouble, default: 0\n\n\n--force_retrain\nIf true, retrain the model and save it to reference_model_path\nboolean_true\n\n\n--var_input\nBoolean .var column to subset data with (e.g. containing highly variable genes). By default, do not subset genes.\nstring\n\n\n\n\n\nTotalVI integration options RNA\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_reference_modality\n\nstring, default: \"rna\"\n\n\n--rna_obsm_output\nIn which .obsm slot to store the normalized RNA from TOTALVI.\nstring, default: \"X_totalvi\"\n\n\n\n\n\nTotalVI integration options ADT\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--prot_reference_modality\nName of the modality containing proteins in the reference\nstring, default: \"prot\"\n\n\n--prot_obsm_output\nIn which .obsm slot to store the normalized protein data from TOTALVI.\nstring, default: \"X_totalvi\"\n\n\n\n\n\nNeighbour calculation RNA\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"totalvi_integration_neighbors\"\n\n\n--rna_obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"totalvi_integration_distances\"\n\n\n--rna_obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"totalvi_integration_connectivities\"\n\n\n\n\n\nNeighbour calculation ADT\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--prot_uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"totalvi_integration_neighbors\"\n\n\n--prot_obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"totalvi_integration_distances\"\n\n\n--prot_obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"totalvi_integration_connectivities\"\n\n\n\n\n\nClustering options RNA\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"totalvi_integration_leiden\"\n\n\n--rna_leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nClustering options ADT\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--prot_obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"totalvi_integration_leiden\"\n\n\n--prot_leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_totalvi_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html#authors",
    "href": "components/workflows/integration/totalvi_leiden.html#authors",
    "title": "Totalvi leiden",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html#visualisation",
    "href": "components/workflows/integration/totalvi_leiden.html#visualisation",
    "title": "Totalvi leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v18(totalvi)\n    v50(concat)\n    v39(find_neighbors)\n    v60(leiden)\n    v80(move_obsm_to_obs)\n    v93(mix)\n    v102(umap)\n    v134(concat)\n    v177(mix)\n    v206(publish)\n    v233(Output)\n    subgraph group_totalvi_leiden [totalvi_leiden]\n        v123(find_neighbors)\n        v144(leiden)\n        v164(move_obsm_to_obs)\n        v186(umap)\n    end\n    v0--&gt;v18\n    v18--&gt;v39\n    v39--&gt;v50\n    v50--&gt;v60\n    v60--&gt;v80\n    v80--&gt;v93\n    v50--&gt;v93\n    v93--&gt;v102\n    v102--&gt;v123\n    v123--&gt;v134\n    v134--&gt;v144\n    v144--&gt;v164\n    v164--&gt;v177\n    v134--&gt;v177\n    v177--&gt;v186\n    v186--&gt;v206\n    v206--&gt;v233\n    style group_totalvi_leiden fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v50 fill:#e3dcea,stroke:#7a4baa;\n    style v39 fill:#e3dcea,stroke:#7a4baa;\n    style v60 fill:#e3dcea,stroke:#7a4baa;\n    style v80 fill:#e3dcea,stroke:#7a4baa;\n    style v93 fill:#e3dcea,stroke:#7a4baa;\n    style v102 fill:#e3dcea,stroke:#7a4baa;\n    style v134 fill:#e3dcea,stroke:#7a4baa;\n    style v123 fill:#e3dcea,stroke:#7a4baa;\n    style v144 fill:#e3dcea,stroke:#7a4baa;\n    style v164 fill:#e3dcea,stroke:#7a4baa;\n    style v177 fill:#e3dcea,stroke:#7a4baa;\n    style v186 fill:#e3dcea,stroke:#7a4baa;\n    style v206 fill:#e3dcea,stroke:#7a4baa;\n    style v233 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "more_information/cheat_sheets.html",
    "href": "more_information/cheat_sheets.html",
    "title": "Cheat sheets",
    "section": "",
    "text": "Figure 1: Cheat sheet for developing modular pipeline components with Viash, including a sample Viash component (left) and common commands used throughout the various stages of a development cycle (right).",
    "crumbs": [
      "Fundamentals",
      "More information",
      "Cheat sheets"
    ]
  },
  {
    "objectID": "more_information/cheat_sheets.html#viash",
    "href": "more_information/cheat_sheets.html#viash",
    "title": "Cheat sheets",
    "section": "",
    "text": "Figure 1: Cheat sheet for developing modular pipeline components with Viash, including a sample Viash component (left) and common commands used throughout the various stages of a development cycle (right).",
    "crumbs": [
      "Fundamentals",
      "More information",
      "Cheat sheets"
    ]
  },
  {
    "objectID": "more_information/index.html",
    "href": "more_information/index.html",
    "title": "More information",
    "section": "",
    "text": "Cheat sheets: Cheat sheets for various tools\n  \n  \n  \n    Code of conduct: Our DEI values\n  \n  \n  \n    FAQ: Frequently Asked Questions\n  \n  \n\n\nNo matching items",
    "crumbs": [
      "Fundamentals",
      "More information"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html",
    "href": "user_guide/running_pipelines.html",
    "title": "Running pipelines",
    "section": "",
    "text": "Dependening on whether you want to run a workflow locally or on cloud infrastructure, using Nextflow Tower or not, you will need to use different commands.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html#run-locally-from-the-cli",
    "href": "user_guide/running_pipelines.html#run-locally-from-the-cli",
    "title": "Running pipelines",
    "section": "Run locally from the CLI",
    "text": "Run locally from the CLI\nYou can run a workflow from the command line using the following command:\nnextflow run openpipelines-bio/openpipeline \\\n  -main-script target/nextflow/workflows/integration/multimodal_integration/main.nf \\\n  -revision 0.12.1 \\\n  -latest \\\n  -profile docker \\\n  --publish_dir foo/ \\\n  --input \"bar\" \\\n  --output \"test.txt\"\nDoing so will run the workflow locally using a Docker container.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html#on-cloud-infrastructure",
    "href": "user_guide/running_pipelines.html#on-cloud-infrastructure",
    "title": "Running pipelines",
    "section": "On cloud infrastructure",
    "text": "On cloud infrastructure\nYou can use a similar command to run the workflow on cloud infrastructure, such as AWS Batch or Google Cloud Platform. However, this requires you to create a separate Nextflow config file for each cloud provider. See the Nextflow documentation for more information.\nnextflow run openpipelines-bio/openpipeline \\\n  -main-script target/nextflow/workflows/integration/multimodal_integration/main.nf \\\n  -revision 0.12.1 \\\n  -latest \\\n  --publish_dir foo/ \\\n  --input \"bar\" \\\n  --output \"test.txt\" \\\n  -c configs/my_hpc.config",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html#using-the-nextflow-tower-cli",
    "href": "user_guide/running_pipelines.html#using-the-nextflow-tower-cli",
    "title": "Running pipelines",
    "section": "Using the Nextflow Tower CLI",
    "text": "Using the Nextflow Tower CLI\nIf you have access to a Nextflow Tower instance in which a Compute Environment has already been set up, you can run a workflow from the Tower CLI. The command is very similar to the command to run a workflow from the CLI, but you need to: * Use tw launch instead of nextflow run * Specify the workspace ID and compute environment ID * Rename arguments: -revision to --revision, -latest to --pull-latest, -main-script to --main-script, -c to --config * Store workflow arguments in a separate yaml file (if this was not already the case).\nExample:\ntw launch openpipelines-bio/openpipeline \\\n  --revision 0.12.1 \\\n  --pull-latest \\\n  --main-script target/nextflow/workflows/integration/multimodal_integration/main.nf \\\n  --workspace &lt;your workspace id&gt; \\\n  --compute-env &lt;your compute environment id&gt; \\\n  --params-file params.yaml \\\n  --config configs/my_hpc.config",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html#using-the-nextflow-tower-web-ui",
    "href": "user_guide/running_pipelines.html#using-the-nextflow-tower-web-ui",
    "title": "Running pipelines",
    "section": "Using the Nextflow Tower Web UI",
    "text": "Using the Nextflow Tower Web UI\nIf you have access to a Nextflow Tower instance in which a Compute Environment has already been set up, you can run a workflow from the Tower UI. To do so, go to the “Launchpad” and click on the button “launch a run without configuration”.\n\nNext, fill in the required fields and click on “Launch run”.\n\nCompute environment: Select the compute environment you want to run the workflow on.\nPipeline to launch: Fill in openpipelines-bio/openpipeline.\nRevision number: The release number of the pipeline you want to run, e.g. 0.12.1. You can find the release number on the GitHub releases page.\nWork directory: The bucket path where the scratch data is stored.\nPipeline parameters: The YAML or JSON of the parameters that are passed to the pipeline. See the Components page for more information about the parameters of each pipeline.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  }
]