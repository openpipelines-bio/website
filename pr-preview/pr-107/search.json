[
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Team",
    "section": "",
    "text": "Angela Oliveira Pisco\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Director of Computational Biology \n           at \n              \n              Insitro\n              \n          \n        \n      \n      \n      \n        \n          Core Member \n           at \n              \n              Open Problems\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Dorien Roosen\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Scientist \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Dries De Maeyer\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Principal Scientist \n           at \n              \n              Janssen Pharmaceuticals\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Dries Schaumont\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Scientist \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Elizabeth Mlynarski\n      \n      Contributor\n      \n      \n      \n      \n        \n          Principal Scientist Computational Genomics \n           at \n            \n              Janssen R&D US\n            \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Isabelle Bergiers\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Scientist OMICS Technology \n           at \n              \n              Janssen Pharmaceuticals\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Jakub Majercik\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Bioinformatics Engineer \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Kai Waldrant\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Bioinformatician \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n      \n      \n        \n          Contributor \n           at \n              \n              Open Problems\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Malte D. Luecken\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Group Leader \n           at \n              \n              Helmholtz Munich\n              \n          \n        \n      \n      \n      \n        \n          Core Member \n           at \n              \n              Open Problems\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Marijke Van Moerbeke\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Statistical Consultant \n           at \n              \n              OpenAnalytics\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Matthias Beyens\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Principal Scientist \n           at \n              \n              Janssen Pharmaceuticals\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Mauro Saporita\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Lead Nextflow Developer \n           at \n              \n              Ardigen\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Povilas Gibas\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Bioinformatician \n           at \n              \n              Ardigen\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Robrecht Cannoodt\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Science Engineer \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n      \n      \n        \n          Core Member \n           at \n              \n              Open Problems\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Samuel D'Souza\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Engineer \n           at \n              \n              Chan Zuckerberg Biohub\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Toni Verbeiren\n      \n      Core Team Member\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Data Scientist and CEO \n           at \n              \n              Data Intuitive\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Vladimir Shitov\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          PhD Candidate \n           at \n              \n              Helmholtz Munich\n              \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Weiwei Schultz\n      \n      Contributor\n      \n      \n      \n      \n        \n          Associate Director Data Sciences \n           at \n            \n              Janssen R&D US\n            \n          \n        \n      \n       \n    \n  \n  \n  \n    \n    \n    \n    \n      Xichen Wu\n      \n      Contributor\n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n      \n      \n      \n        \n          Student Assistant \n           at \n              \n              Helmholtz Munich\n              \n          \n        \n      \n       \n    \n  \n  \n\n\nNo matching items"
  },
  {
    "objectID": "user_guide/parameter_lists.html",
    "href": "user_guide/parameter_lists.html",
    "title": "Parameter lists",
    "section": "",
    "text": "Using the Viash VDSL3 Nextflow platform, an optional --param_list argument can be passed to OpenPipelines workflows. The --param_list argument enables passing multiple inputs to a workflow, resulting in a multi-event nextflow channel.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Parameter lists"
    ]
  },
  {
    "objectID": "user_guide/parameter_lists.html#json-file",
    "href": "user_guide/parameter_lists.html#json-file",
    "title": "Parameter lists",
    "section": "JSON file",
    "text": "JSON file\nThe following example shows how to use a json file as a parameter list.\n$ cat param_list.json\n[\n    {\n        \"id\": \"foo\",\n        \"input\": \"foo.txt\",\n        \"event_param\": \"lorem\"\n    },\n    {\n        \"id\": \"bar\",\n        \"input\": \"bar.txt\",\n        \"event_param\": \"ipsum\"\n    }\n]\nnextflow run ... --param_list param_list.json",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Parameter lists"
    ]
  },
  {
    "objectID": "user_guide/parameter_lists.html#csv-file",
    "href": "user_guide/parameter_lists.html#csv-file",
    "title": "Parameter lists",
    "section": "CSV file",
    "text": "CSV file\nThe following example shows how to use a csv file as a parameter list.\n$ cat param_list.csv\nid,input,event_param\nfoo,foo.txt,lorem\nbar,bar.txt,ipsum\nnextflow run ... --param_list param_list.csv",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Parameter lists"
    ]
  },
  {
    "objectID": "user_guide/index.html",
    "href": "user_guide/index.html",
    "title": "User guide",
    "section": "",
    "text": "Getting started: Setting up infrastructure\n  \n  \n  \n    Running pipelines: Run a pipeline from CLI or Nextflow Tower\n  \n  \n  \n    Parameter lists: Passing multiple inputs to a workflow\n  \n  \n  \n    Ingestion: From sequencing to count tables\n  \n  \n  \n    Processing: From count tables to integrated data\n  \n  \n  \n    Downstream: Celltyping and cell-cell communication\n  \n  \n  \n    Bug reports: How to report bugs\n  \n  \n\n\nNo matching items",
    "crumbs": [
      "Fundamentals",
      "User guide"
    ]
  },
  {
    "objectID": "user_guide/bug_reports.html",
    "href": "user_guide/bug_reports.html",
    "title": "Bug reports",
    "section": "",
    "text": "Issues with Openpipelines are being tracked on Github. In order for an issue to be fixed in a timely manner, creating a complete and reproducable is essential.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Bug reports"
    ]
  },
  {
    "objectID": "user_guide/getting_started.html",
    "href": "user_guide/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "Depending on whether you plan to run the OpenPipelines workflows locally or in the cloud",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user_guide/getting_started.html#starting-workflows-locally",
    "href": "user_guide/getting_started.html#starting-workflows-locally",
    "title": "Getting started",
    "section": "Starting workflows locally",
    "text": "Starting workflows locally\nIf you want to start workflows locally, you will need to install Nextflow.\n\nInstall Docker (optional)\nDocker is a containerization platform that allows you to package your application and all its dependencies into a single image. It is used to run the analysis pipelines.\nIf you are planning on running the workflows locally, you will need to install Docker. You do not need to install Docker if the workflows will be run in the cloud using AWS Batch, Azure Batch, Google Cloud Batch, or other cloud-based compute environments.\nTo install Docker, follow the instructions here.\n\n\nInstall Java\nNextflow requires Java 11 or later. To check if Java is installed on your system, run:\njava -version\nIf Java is not installed, you can download it from here.\n\n\nInstall Nextflow\nNextflow is distributed as a single executable file. To install it, run:\ncurl -s https://get.nextflow.io | bash\nThis command will download the latest version of Nextflow and store it in the current directory.\nTo install Nextflow system-wide, move the downloaded file to a directory in your $PATH, e.g.:\nmv nextflow /usr/local/bin\n\n\nTest the installation\nTo test the installation, run:\nnextflow run hello -with-docker",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user_guide/getting_started.html#using-nextflow-tower",
    "href": "user_guide/getting_started.html#using-nextflow-tower",
    "title": "Getting started",
    "section": "Using Nextflow Tower",
    "text": "Using Nextflow Tower\nNextflow Tower is a web-based user interface for running and monitoring Nextflow pipelines. If you are planning on using Nextflow Tower, a compute environment will need to be set up.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Getting started"
    ]
  },
  {
    "objectID": "more_information/code_of_conduct.html",
    "href": "more_information/code_of_conduct.html",
    "title": "Code of conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\nOur full Code of Conduct is adapted from the Contributor Covenant, version 2.1.",
    "crumbs": [
      "Fundamentals",
      "More information",
      "Code of conduct"
    ]
  },
  {
    "objectID": "more_information/faq.html",
    "href": "more_information/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "It is possible to add additional resources such as a file containing helper functions or other resources. All you need to do is list those files under the functionality.resources section of your component and refer to them in your script using meta[\"resources_dir\"] + \"/myresource.txt\". Please visit the Viash documentation for concrete examples on how to add helper functions and other resources to your component.",
    "crumbs": [
      "Fundamentals",
      "More information",
      "FAQ"
    ]
  },
  {
    "objectID": "more_information/faq.html#how-can-i-add-an-external-resource-to-my-viash-component",
    "href": "more_information/faq.html#how-can-i-add-an-external-resource-to-my-viash-component",
    "title": "FAQ",
    "section": "",
    "text": "It is possible to add additional resources such as a file containing helper functions or other resources. All you need to do is list those files under the functionality.resources section of your component and refer to them in your script using meta[\"resources_dir\"] + \"/myresource.txt\". Please visit the Viash documentation for concrete examples on how to add helper functions and other resources to your component.",
    "crumbs": [
      "Fundamentals",
      "More information",
      "FAQ"
    ]
  },
  {
    "objectID": "more_information/faq.html#what-does-__merge__-do",
    "href": "more_information/faq.html#what-does-__merge__-do",
    "title": "FAQ",
    "section": "What does __merge__ do?",
    "text": "What does __merge__ do?\nThe __merge__ field is used to merge another YAML into a Viash config. One of its uses is in making sure that all of the components in a task has the same API.\nEach task in OpenProblems contains strict definitions of the input/output file interface of its components and the file formats of those files. These interfaces are stored as YAML files in the api subdirectory of each task.",
    "crumbs": [
      "Fundamentals",
      "More information",
      "FAQ"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html",
    "href": "components/workflows/qc/qc.html",
    "title": "Qc",
    "section": "",
    "text": "ID: qc\nNamespace: workflows/qc\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html#example-commands",
    "href": "components/workflows/qc/qc.html#example-commands",
    "title": "Qc",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/qc/qc/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\nmodality: \"rna\"\n# layer: \"raw_counts\"\n\n# Mitochondrial Gene Detection\n# var_name_mitochondrial_genes: \"foo\"\n# obs_name_mitochondrial_fraction: \"foo\"\n# var_gene_names: \"gene_symbol\"\nmitochondrial_gene_regex: \"^[mM][tT]-\"\n\n# QC metrics calculation options\n# var_qc_metrics: [\"ercc,highly_variable\"]\ntop_n_vars: [50, 100, 200, 500]\noutput_obs_num_nonzero_vars: \"num_nonzero_vars\"\noutput_obs_total_counts_vars: \"total_counts\"\noutput_var_num_nonzero_obs: \"num_nonzero_obs\"\noutput_var_total_counts_obs: \"total_counts\"\noutput_var_obs_mean: \"obs_mean\"\noutput_var_pct_dropout: \"pct_dropout\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/qc/qc/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html#argument-groups",
    "href": "components/workflows/qc/qc.html#argument-groups",
    "title": "Qc",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"input.h5mu\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n--layer\nLayer to calculate qc metrics for.\nstring, example: \"raw_counts\"\n\n\n\n\n\nMitochondrial Gene Detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_name_mitochondrial_genes\nIn which .var slot to store a boolean array corresponding the mitochondrial genes.\nstring\n\n\n--obs_name_mitochondrial_fraction\n.Obs slot to store the fraction of reads found to be mitochondrial. Defaults to ‘fraction_’ suffixed by the value of –var_name_mitochondrial_genes\nstring\n\n\n--var_gene_names\n.var column name to be used to detect mitochondrial genes instead of .var_names (default if not set). Gene names matching with the regex value from –mitochondrial_gene_regex will be identified as a mitochondrial gene.\nstring, example: \"gene_symbol\"\n\n\n--mitochondrial_gene_regex\nRegex string that identifies mitochondrial genes from –var_gene_names. By default will detect human and mouse mitochondrial genes from a gene symbol.\nstring, default: \"^[mM][tT]-\"\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes. Defaults to the value from –var_name_mitochondrial_genes.\nList of string, example: \"ercc,highly_variable\", multiple_sep: \",\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \",\"\n\n\n--output_obs_num_nonzero_vars\nName of column in .obs describing, for each observation, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each row the number of columns that contain data.\nstring, default: \"num_nonzero_vars\"\n\n\n--output_obs_total_counts_vars\nName of the column for .obs describing, for each observation (row), the sum of the stored values in the columns.\nstring, default: \"total_counts\"\n\n\n--output_var_num_nonzero_obs\nName of column describing, for each feature, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each column the number of rows that contain data.\nstring, default: \"num_nonzero_obs\"\n\n\n--output_var_total_counts_obs\nName of the column in .var describing, for each feature (column), the sum of the stored values in the rows.\nstring, default: \"total_counts\"\n\n\n--output_var_obs_mean\nName of the column in .obs providing the mean of the values in each row.\nstring, default: \"obs_mean\"\n\n\n--output_var_pct_dropout\nName of the column in .obs providing for each feature the percentage of observations the feature does not appear on (i.e. is missing). Same as --output_var_num_nonzero_obs but percentage based.\nstring, default: \"pct_dropout\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html#authors",
    "href": "components/workflows/qc/qc.html#authors",
    "title": "Qc",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/qc/qc.html#visualisation",
    "href": "components/workflows/qc/qc.html#visualisation",
    "title": "Qc",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v12(filter)\n    v20(grep_annotation_column)\n    v27(cross)\n    v37(cross)\n    v44(mix)\n    v45(filter)\n    v53(calculate_qc_metrics)\n    v60(cross)\n    v70(cross)\n    v76(filter)\n    v106(concat)\n    v84(publish)\n    v91(cross)\n    v101(cross)\n    v113(cross)\n    v120(cross)\n    v132(cross)\n    v139(cross)\n    v143(Output)\n    v44--&gt;v45\n    v0--&gt;v2\n    v12--&gt;v20\n    v20--&gt;v27\n    v12--&gt;v27\n    v12--&gt;v37\n    v45--&gt;v53\n    v53--&gt;v60\n    v45--&gt;v60\n    v45--&gt;v70\n    v76--&gt;v84\n    v84--&gt;v91\n    v76--&gt;v91\n    v76--&gt;v101\n    v101--&gt;v106\n    v106--&gt;v113\n    v2--&gt;v113\n    v113--&gt;v120\n    v2--&gt;v120\n    v2--&gt;v132\n    v132--&gt;v139\n    v2--&gt;v139\n    v139--&gt;v143\n    v2--&gt;v12\n    v37--&gt;v44\n    v20--&gt;v37\n    v2--&gt;v44\n    v70--&gt;v76\n    v53--&gt;v70\n    v84--&gt;v101\n    v106--&gt;v132\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v12 fill:#e3dcea,stroke:#7a4baa;\n    style v20 fill:#e3dcea,stroke:#7a4baa;\n    style v27 fill:#e3dcea,stroke:#7a4baa;\n    style v37 fill:#e3dcea,stroke:#7a4baa;\n    style v44 fill:#e3dcea,stroke:#7a4baa;\n    style v45 fill:#e3dcea,stroke:#7a4baa;\n    style v53 fill:#e3dcea,stroke:#7a4baa;\n    style v60 fill:#e3dcea,stroke:#7a4baa;\n    style v70 fill:#e3dcea,stroke:#7a4baa;\n    style v76 fill:#e3dcea,stroke:#7a4baa;\n    style v106 fill:#e3dcea,stroke:#7a4baa;\n    style v84 fill:#e3dcea,stroke:#7a4baa;\n    style v91 fill:#e3dcea,stroke:#7a4baa;\n    style v101 fill:#e3dcea,stroke:#7a4baa;\n    style v113 fill:#e3dcea,stroke:#7a4baa;\n    style v120 fill:#e3dcea,stroke:#7a4baa;\n    style v132 fill:#e3dcea,stroke:#7a4baa;\n    style v139 fill:#e3dcea,stroke:#7a4baa;\n    style v143 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Qc",
      "Qc"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html",
    "href": "components/workflows/integration/harmony_leiden.html",
    "title": "Harmony leiden",
    "section": "",
    "text": "ID: harmony_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html#example-commands",
    "href": "components/workflows/integration/harmony_leiden.html#example-commands",
    "title": "Harmony leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/integration/harmony_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Neighbour calculation\nuns_neighbors: \"harmonypy_integration_neighbors\"\nobsp_neighbor_distances: \"harmonypy_integration_distances\"\nobsp_neighbor_connectivities: \"harmonypy_integration_connectivities\"\n\n# Harmony integration options\nembedding: \"X_pca\"\nobsm_integrated: \"X_pca_integrated\"\nobs_covariates: # please fill in - example: [\"batch\", \"sample\"]\ntheta: [2.0]\n\n# Clustering options\nobs_cluster: \"harmony_integration_leiden\"\nleiden_resolution: [1.0]\n\n# Umap options\nobsm_umap: \"X_leiden_harmony_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/harmony_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html#argument-groups",
    "href": "components/workflows/integration/harmony_leiden.html#argument-groups",
    "title": "Harmony leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nNeighbour calculation\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"harmonypy_integration_neighbors\"\n\n\n--obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"harmonypy_integration_distances\"\n\n\n--obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"harmonypy_integration_connectivities\"\n\n\n\n\n\nHarmony integration options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--embedding\nEmbedding to use as input\nstring, default: \"X_pca\"\n\n\n--obsm_integrated\nIn which .obsm slot to store the resulting integrated embedding.\nstring, default: \"X_pca_integrated\"\n\n\n--obs_covariates\nThe .obs field(s) that define the covariate(s) to regress out.\nList of string, required, example: \"batch\", \"sample\", multiple_sep: \";\"\n\n\n--theta\nDiversity clustering penalty parameter. Specify for each variable in group.by.vars. theta=0 does not encourage any diversity. Larger values of theta result in more diverse clusters.”\nList of double, default: 2, multiple_sep: \";\"\n\n\n\n\n\nClustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"harmony_integration_leiden\"\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_leiden_harmony_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html#authors",
    "href": "components/workflows/integration/harmony_leiden.html#authors",
    "title": "Harmony leiden",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/harmony_leiden.html#visualisation",
    "href": "components/workflows/integration/harmony_leiden.html#visualisation",
    "title": "Harmony leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(harmonypy)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v71(concat)\n    v49(find_neighbors)\n    v56(cross)\n    v66(cross)\n    v73(filter)\n    v81(leiden)\n    v88(cross)\n    v98(cross)\n    v104(filter)\n    v112(move_obsm_to_obs)\n    v119(cross)\n    v129(cross)\n    v136(mix)\n    v137(filter)\n    v167(concat)\n    v145(umap)\n    v152(cross)\n    v162(cross)\n    v174(cross)\n    v181(cross)\n    v193(cross)\n    v200(cross)\n    v204(Output)\n    v136--&gt;v137\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v66--&gt;v71\n    v73--&gt;v81\n    v81--&gt;v88\n    v73--&gt;v88\n    v73--&gt;v98\n    v104--&gt;v112\n    v112--&gt;v119\n    v104--&gt;v119\n    v104--&gt;v129\n    v137--&gt;v145\n    v145--&gt;v152\n    v137--&gt;v152\n    v137--&gt;v162\n    v162--&gt;v167\n    v167--&gt;v174\n    v2--&gt;v174\n    v174--&gt;v181\n    v2--&gt;v181\n    v2--&gt;v193\n    v193--&gt;v200\n    v2--&gt;v200\n    v200--&gt;v204\n    v35--&gt;v41\n    v18--&gt;v35\n    v49--&gt;v66\n    v71--&gt;v73\n    v98--&gt;v104\n    v81--&gt;v98\n    v129--&gt;v136\n    v112--&gt;v129\n    v71--&gt;v136\n    v145--&gt;v162\n    v167--&gt;v193\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v73 fill:#e3dcea,stroke:#7a4baa;\n    style v81 fill:#e3dcea,stroke:#7a4baa;\n    style v88 fill:#e3dcea,stroke:#7a4baa;\n    style v98 fill:#e3dcea,stroke:#7a4baa;\n    style v104 fill:#e3dcea,stroke:#7a4baa;\n    style v112 fill:#e3dcea,stroke:#7a4baa;\n    style v119 fill:#e3dcea,stroke:#7a4baa;\n    style v129 fill:#e3dcea,stroke:#7a4baa;\n    style v136 fill:#e3dcea,stroke:#7a4baa;\n    style v137 fill:#e3dcea,stroke:#7a4baa;\n    style v167 fill:#e3dcea,stroke:#7a4baa;\n    style v145 fill:#e3dcea,stroke:#7a4baa;\n    style v152 fill:#e3dcea,stroke:#7a4baa;\n    style v162 fill:#e3dcea,stroke:#7a4baa;\n    style v174 fill:#e3dcea,stroke:#7a4baa;\n    style v181 fill:#e3dcea,stroke:#7a4baa;\n    style v193 fill:#e3dcea,stroke:#7a4baa;\n    style v200 fill:#e3dcea,stroke:#7a4baa;\n    style v204 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Harmony leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html",
    "href": "components/workflows/integration/scanorama_leiden.html",
    "title": "Scanorama leiden",
    "section": "",
    "text": "ID: scanorama_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html#example-commands",
    "href": "components/workflows/integration/scanorama_leiden.html#example-commands",
    "title": "Scanorama leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/integration/scanorama_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Neighbour calculation\nuns_neighbors: \"scanorama_integration_neighbors\"\nobsp_neighbor_distances: \"scanorama_integration_distances\"\nobsp_neighbor_connectivities: \"scanorama_integration_connectivities\"\n\n# Scanorama integration options\nobs_batch: \"sample_id\"\nobsm_input: \"X_pca\"\nobsm_output: \"X_scanorama\"\nknn: 20\nbatch_size: 5000\nsigma: 15.0\napprox: true\nalpha: 0.1\n\n# Clustering options\nobs_cluster: \"scanorama_integration_leiden\"\nleiden_resolution: [1.0]\n\n# Umap options\nobsm_umap: \"X_leiden_scanorama_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/scanorama_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html#argument-groups",
    "href": "components/workflows/integration/scanorama_leiden.html#argument-groups",
    "title": "Scanorama leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nNeighbour calculation\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"scanorama_integration_neighbors\"\n\n\n--obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"scanorama_integration_distances\"\n\n\n--obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"scanorama_integration_connectivities\"\n\n\n\n\n\nScanorama integration options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_batch\nColumn name discriminating between your batches.\nstring, default: \"sample_id\"\n\n\n--obsm_input\n.obsm slot that points to embedding to run scanorama on.\nstring, default: \"X_pca\"\n\n\n--obsm_output\nThe name of the field in adata.obsm where the integrated embeddings will be stored after running this function. Defaults to X_scanorama.\nstring, default: \"X_scanorama\"\n\n\n--knn\nNumber of nearest neighbors to use for matching.\ninteger, default: 20\n\n\n--batch_size\nThe batch size used in the alignment vector computation. Useful when integrating very large (&gt;100k samples) datasets. Set to large value that runs within available memory.\ninteger, default: 5000\n\n\n--sigma\nCorrection smoothing parameter on Gaussian kernel.\ndouble, default: 15\n\n\n--approx\nUse approximate nearest neighbors with Python annoy; greatly speeds up matching runtime.\nboolean, default: TRUE\n\n\n--alpha\nAlignment score minimum cutoff\ndouble, default: 0.1\n\n\n\n\n\nClustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions specified in ‘–leiden_resolution’.\nstring, default: \"scanorama_integration_leiden\"\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_leiden_scanorama_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html#authors",
    "href": "components/workflows/integration/scanorama_leiden.html#authors",
    "title": "Scanorama leiden",
    "section": "Authors",
    "text": "Authors\n\nMauro Saporita   (author)\nPovilas Gibas   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scanorama_leiden.html#visualisation",
    "href": "components/workflows/integration/scanorama_leiden.html#visualisation",
    "title": "Scanorama leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(scanorama)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v71(concat)\n    v49(find_neighbors)\n    v56(cross)\n    v66(cross)\n    v73(filter)\n    v81(leiden)\n    v88(cross)\n    v98(cross)\n    v104(filter)\n    v112(move_obsm_to_obs)\n    v119(cross)\n    v129(cross)\n    v136(mix)\n    v137(filter)\n    v167(concat)\n    v145(umap)\n    v152(cross)\n    v162(cross)\n    v174(cross)\n    v181(cross)\n    v193(cross)\n    v200(cross)\n    v204(Output)\n    v136--&gt;v137\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v66--&gt;v71\n    v73--&gt;v81\n    v81--&gt;v88\n    v73--&gt;v88\n    v73--&gt;v98\n    v104--&gt;v112\n    v112--&gt;v119\n    v104--&gt;v119\n    v104--&gt;v129\n    v137--&gt;v145\n    v145--&gt;v152\n    v137--&gt;v152\n    v137--&gt;v162\n    v162--&gt;v167\n    v167--&gt;v174\n    v2--&gt;v174\n    v174--&gt;v181\n    v2--&gt;v181\n    v2--&gt;v193\n    v193--&gt;v200\n    v2--&gt;v200\n    v200--&gt;v204\n    v35--&gt;v41\n    v18--&gt;v35\n    v49--&gt;v66\n    v71--&gt;v73\n    v98--&gt;v104\n    v81--&gt;v98\n    v129--&gt;v136\n    v112--&gt;v129\n    v71--&gt;v136\n    v145--&gt;v162\n    v167--&gt;v193\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v73 fill:#e3dcea,stroke:#7a4baa;\n    style v81 fill:#e3dcea,stroke:#7a4baa;\n    style v88 fill:#e3dcea,stroke:#7a4baa;\n    style v98 fill:#e3dcea,stroke:#7a4baa;\n    style v104 fill:#e3dcea,stroke:#7a4baa;\n    style v112 fill:#e3dcea,stroke:#7a4baa;\n    style v119 fill:#e3dcea,stroke:#7a4baa;\n    style v129 fill:#e3dcea,stroke:#7a4baa;\n    style v136 fill:#e3dcea,stroke:#7a4baa;\n    style v137 fill:#e3dcea,stroke:#7a4baa;\n    style v167 fill:#e3dcea,stroke:#7a4baa;\n    style v145 fill:#e3dcea,stroke:#7a4baa;\n    style v152 fill:#e3dcea,stroke:#7a4baa;\n    style v162 fill:#e3dcea,stroke:#7a4baa;\n    style v174 fill:#e3dcea,stroke:#7a4baa;\n    style v181 fill:#e3dcea,stroke:#7a4baa;\n    style v193 fill:#e3dcea,stroke:#7a4baa;\n    style v200 fill:#e3dcea,stroke:#7a4baa;\n    style v204 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scanorama leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scgpt_leiden.html",
    "href": "components/workflows/integration/scgpt_leiden.html",
    "title": "Scgpt leiden",
    "section": "",
    "text": "ID: scgpt_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scgpt leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scgpt_leiden.html#example-commands",
    "href": "components/workflows/integration/scgpt_leiden.html#example-commands",
    "title": "Scgpt leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/integration/scgpt_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\nmodality: \"rna\"\n# input_layer: \"foo\"\n# var_gene_names: \"foo\"\n# obs_batch_label: \"foo\"\n\n# Model\nmodel: # please fill in - example: \"resources_test/scgpt/best_model.pt\"\nmodel_vocab: # please fill in - example: \"resources_test/scgpt/vocab.json\"\nmodel_config: # please fill in - example: \"args.json\"\n# finetuned_checkpoints_key: \"model_state_dict\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\nobsm_integrated: \"X_scgpt\"\n\n# Padding arguments\npad_token: \"&lt;pad&gt;\"\npad_value: -2\n\n# HVG subset arguments\nn_hvg: 1200\n\n# Tokenization arguments\n# max_seq_len: 123\n\n# Embedding arguments\ndsbn: true\nbatch_size: 64\n\n# Binning arguments\nn_input_bins: 51\n# seed: 123\n\n# Clustering arguments\nleiden_resolution: [1.0]\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/scgpt_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scgpt leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scgpt_leiden.html#argument-groups",
    "href": "components/workflows/integration/scgpt_leiden.html#argument-groups",
    "title": "Scgpt leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the input file.\nfile, required, example: \"input.h5mu\"\n\n\n--modality\n\nstring, default: \"rna\"\n\n\n--input_layer\nMudata layer (key from layers) to use as input data for hvg subsetting and binning; if not specified, X is used.\nstring\n\n\n--var_gene_names\nThe name of the adata var column containing gene names; when no gene_name_layer is provided, the var index will be used.\nstring\n\n\n--obs_batch_label\nThe name of the adata obs column containing the batch labels.\nstring\n\n\n\n\n\nModel\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--model\nPath to scGPT model file.\nfile, required, example: \"resources_test/scgpt/best_model.pt\"\n\n\n--model_vocab\nPath to scGPT model vocabulary file.\nfile, required, example: \"resources_test/scgpt/vocab.json\"\n\n\n--model_config\nPath to scGPT model config file.\nfile, required, example: \"args.json\"\n\n\n--finetuned_checkpoints_key\nKey in the model file containing the pretrained checkpoints. Only relevant for fine-tuned models.\nstring, example: \"model_state_dict\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nOutput file path\nfile, required, example: \"output.h5mu\"\n\n\n--obsm_integrated\nIn which .obsm slot to store the resulting integrated embedding.\nstring, default: \"X_scgpt\"\n\n\n\n\n\nPadding arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--pad_token\nToken used for padding.\nstring, default: \"&lt;pad&gt;\"\n\n\n--pad_value\nThe value of the padding token.\ninteger, default: -2\n\n\n\n\n\nHVG subset arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--n_hvg\nNumber of highly variable genes to subset for.\ninteger, default: 1200\n\n\n\n\n\nTokenization arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--max_seq_len\nThe maximum sequence length of the tokenized data. Defaults to the number of features if not provided.\ninteger\n\n\n\n\n\nEmbedding arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--dsbn\nApply domain-specific batch normalization\nboolean, default: TRUE\n\n\n--batch_size\nThe batch size to be used for embedding inference.\ninteger, default: 64\n\n\n\n\n\nBinning arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--n_input_bins\nThe number of bins to discretize the data into; When no value is provided, data won’t be binned.\ninteger, default: 51\n\n\n--seed\nSeed for random number generation used for binning. If not set, no seed is used.\ninteger\n\n\n\n\n\nClustering arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scgpt leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scgpt_leiden.html#authors",
    "href": "components/workflows/integration/scgpt_leiden.html#authors",
    "title": "Scgpt leiden",
    "section": "Authors",
    "text": "Authors\n\nDorien Roosen   (maintainer, author)\nElizabeth Mlynarski (author)\nWeiwei Schultz (contributor)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scgpt leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scgpt_leiden.html#visualisation",
    "href": "components/workflows/integration/scgpt_leiden.html#visualisation",
    "title": "Scgpt leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(highly_variable_features_scanpy)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v49(cross_check_genes)\n    v56(cross)\n    v66(cross)\n    v72(filter)\n    v80(binning)\n    v87(cross)\n    v97(cross)\n    v103(filter)\n    v111(pad_tokenize)\n    v118(cross)\n    v128(cross)\n    v134(filter)\n    v142(embedding)\n    v149(cross)\n    v159(cross)\n    v165(filter)\n    v173(find_neighbors)\n    v180(cross)\n    v190(cross)\n    v199(branch)\n    v226(concat)\n    v204(leiden)\n    v211(cross)\n    v221(cross)\n    v230(branch)\n    v257(concat)\n    v235(move_obsm_to_obs)\n    v242(cross)\n    v252(cross)\n    v258(filter)\n    v288(concat)\n    v266(umap)\n    v273(cross)\n    v283(cross)\n    v295(cross)\n    v302(cross)\n    v314(cross)\n    v321(cross)\n    v325(Output)\n    v199--&gt;v226\n    v230--&gt;v257\n    v257--&gt;v258\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v72--&gt;v80\n    v80--&gt;v87\n    v72--&gt;v87\n    v72--&gt;v97\n    v103--&gt;v111\n    v111--&gt;v118\n    v103--&gt;v118\n    v103--&gt;v128\n    v134--&gt;v142\n    v142--&gt;v149\n    v134--&gt;v149\n    v134--&gt;v159\n    v165--&gt;v173\n    v173--&gt;v180\n    v165--&gt;v180\n    v165--&gt;v190\n    v199--&gt;v204\n    v204--&gt;v211\n    v199--&gt;v211\n    v199--&gt;v221\n    v221--&gt;v226\n    v230--&gt;v235\n    v235--&gt;v242\n    v230--&gt;v242\n    v230--&gt;v252\n    v252--&gt;v257\n    v258--&gt;v266\n    v266--&gt;v273\n    v258--&gt;v273\n    v258--&gt;v283\n    v283--&gt;v288\n    v288--&gt;v295\n    v2--&gt;v295\n    v295--&gt;v302\n    v2--&gt;v302\n    v2--&gt;v314\n    v314--&gt;v321\n    v2--&gt;v321\n    v321--&gt;v325\n    v35--&gt;v41\n    v18--&gt;v35\n    v66--&gt;v72\n    v49--&gt;v66\n    v97--&gt;v103\n    v80--&gt;v97\n    v128--&gt;v134\n    v111--&gt;v128\n    v159--&gt;v165\n    v142--&gt;v159\n    v173--&gt;v190\n    v190--&gt;v199\n    v204--&gt;v221\n    v226--&gt;v230\n    v235--&gt;v252\n    v266--&gt;v283\n    v288--&gt;v314\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v72 fill:#e3dcea,stroke:#7a4baa;\n    style v80 fill:#e3dcea,stroke:#7a4baa;\n    style v87 fill:#e3dcea,stroke:#7a4baa;\n    style v97 fill:#e3dcea,stroke:#7a4baa;\n    style v103 fill:#e3dcea,stroke:#7a4baa;\n    style v111 fill:#e3dcea,stroke:#7a4baa;\n    style v118 fill:#e3dcea,stroke:#7a4baa;\n    style v128 fill:#e3dcea,stroke:#7a4baa;\n    style v134 fill:#e3dcea,stroke:#7a4baa;\n    style v142 fill:#e3dcea,stroke:#7a4baa;\n    style v149 fill:#e3dcea,stroke:#7a4baa;\n    style v159 fill:#e3dcea,stroke:#7a4baa;\n    style v165 fill:#e3dcea,stroke:#7a4baa;\n    style v173 fill:#e3dcea,stroke:#7a4baa;\n    style v180 fill:#e3dcea,stroke:#7a4baa;\n    style v190 fill:#e3dcea,stroke:#7a4baa;\n    style v199 fill:#e3dcea,stroke:#7a4baa;\n    style v226 fill:#e3dcea,stroke:#7a4baa;\n    style v204 fill:#e3dcea,stroke:#7a4baa;\n    style v211 fill:#e3dcea,stroke:#7a4baa;\n    style v221 fill:#e3dcea,stroke:#7a4baa;\n    style v230 fill:#e3dcea,stroke:#7a4baa;\n    style v257 fill:#e3dcea,stroke:#7a4baa;\n    style v235 fill:#e3dcea,stroke:#7a4baa;\n    style v242 fill:#e3dcea,stroke:#7a4baa;\n    style v252 fill:#e3dcea,stroke:#7a4baa;\n    style v258 fill:#e3dcea,stroke:#7a4baa;\n    style v288 fill:#e3dcea,stroke:#7a4baa;\n    style v266 fill:#e3dcea,stroke:#7a4baa;\n    style v273 fill:#e3dcea,stroke:#7a4baa;\n    style v283 fill:#e3dcea,stroke:#7a4baa;\n    style v295 fill:#e3dcea,stroke:#7a4baa;\n    style v302 fill:#e3dcea,stroke:#7a4baa;\n    style v314 fill:#e3dcea,stroke:#7a4baa;\n    style v321 fill:#e3dcea,stroke:#7a4baa;\n    style v325 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scgpt leiden"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_annotation.html",
    "href": "components/workflows/annotation/scgpt_annotation.html",
    "title": "scGPT Annotation",
    "section": "",
    "text": "ID: scgpt_annotation\nNamespace: workflows/annotation\n\n\n\nSource\nThe workflow takes a pre-processed h5mu file as query input, and performs - subsetting for HVG - cross-checking of genes with the model vocabulary - binning of gene counts - padding and tokenizing of genes - transformer-based cell type prediction Note that cell-type prediction using scGPT is only possible using a fine-tuned scGPT model",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_annotation.html#example-commands",
    "href": "components/workflows/annotation/scgpt_annotation.html#example-commands",
    "title": "scGPT Annotation",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/annotation/scgpt_annotation/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Query input\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\nmodality: \"rna\"\n# input_layer: \"foo\"\n# input_var_gene_names: \"foo\"\ninput_obs_batch_label: # please fill in - example: \"foo\"\n\n# Model input\nmodel: # please fill in - example: \"best_model.pt\"\nmodel_config: # please fill in - example: \"args.json\"\nmodel_vocab: # please fill in - example: \"vocab.json\"\nfinetuned_checkpoints_key: \"model_state_dict\"\nlabel_mapper_key: \"id_to_class\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n# output_compression: \"gzip\"\noutput_obs_predictions: \"scgpt_pred\"\noutput_obs_probability: \"scgpt_probability\"\n\n# Padding arguments\npad_token: \"&lt;pad&gt;\"\npad_value: -2\n\n# HVG subset arguments\nn_hvg: 1200\n\n# Tokenization arguments\n# max_seq_len: 123\n\n# Embedding arguments\ndsbn: true\nbatch_size: 64\n\n# Binning arguments\nn_input_bins: 51\n# seed: 123\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/annotation/scgpt_annotation/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_annotation.html#argument-groups",
    "href": "components/workflows/annotation/scgpt_annotation.html#argument-groups",
    "title": "scGPT Annotation",
    "section": "Argument groups",
    "text": "Argument groups\n\nQuery input\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the input file.\nfile, required, example: \"input.h5mu\"\n\n\n--modality\n\nstring, default: \"rna\"\n\n\n--input_layer\nMudata layer (key from layers) to use as input data for HVG subsetting and binning; if not specified, X is used.\nstring\n\n\n--input_var_gene_names\nThe .var field in the input (query) containing gene names; if not provided, the var index will be used.\nstring\n\n\n--input_obs_batch_label\nThe .obs field in the input (query) dataset containing the batch labels.\nstring, required\n\n\n\n\n\nModel input\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--model\nThe scGPT model file. Must be a fine-tuned model that contains keys for checkpoints (–finetuned_checkpoints_key) and cell type label mapper(–label_mapper_key).\nfile, required, example: \"best_model.pt\"\n\n\n--model_config\nThe scGPT model configuration file.\nfile, required, example: \"args.json\"\n\n\n--model_vocab\nThe scGPT model vocabulary file.\nfile, required, example: \"vocab.json\"\n\n\n--finetuned_checkpoints_key\nKey in the model file containing the pre-trained checkpoints.\nstring, default: \"model_state_dict\"\n\n\n--label_mapper_key\nKey in the model file containing the cell type class to label mapper dictionary.\nstring, default: \"id_to_class\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nOutput file path\nfile, required, example: \"output.h5mu\"\n\n\n--output_compression\nThe compression algorithm to use for the output h5mu file.\nstring, example: \"gzip\"\n\n\n--output_obs_predictions\nThe name of the adata.obs column to write predicted cell type labels to.\nstring, default: \"scgpt_pred\"\n\n\n--output_obs_probability\nThe name of the adata.obs column to write predicted cell type labels to.\nstring, default: \"scgpt_probability\"\n\n\n\n\n\nPadding arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--pad_token\nToken used for padding.\nstring, default: \"&lt;pad&gt;\"\n\n\n--pad_value\nThe value of the padding token.\ninteger, default: -2\n\n\n\n\n\nHVG subset arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--n_hvg\nNumber of highly variable genes to subset for.\ninteger, default: 1200\n\n\n\n\n\nTokenization arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--max_seq_len\nThe maximum sequence length of the tokenized data.\ninteger\n\n\n\n\n\nEmbedding arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--dsbn\nApply domain-specific batch normalization\nboolean, default: TRUE\n\n\n--batch_size\nThe batch size to be used for embedding inference.\ninteger, default: 64\n\n\n\n\n\nBinning arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--n_input_bins\nThe number of bins to discretize the data into; When no value is provided, data won’t be binned.\ninteger, default: 51\n\n\n--seed\nSeed for random number generation used for binning. If not set, no seed is used.\ninteger",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_annotation.html#authors",
    "href": "components/workflows/annotation/scgpt_annotation.html#authors",
    "title": "scGPT Annotation",
    "section": "Authors",
    "text": "Authors\n\nDorien Roosen   (author, maintainer)\nElizabeth Mlynarski (contributor)\nWeiwei Schultz (contributor)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_annotation.html#visualisation",
    "href": "components/workflows/annotation/scgpt_annotation.html#visualisation",
    "title": "scGPT Annotation",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(highly_variable_features_scanpy)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v49(cross_check_genes)\n    v56(cross)\n    v66(cross)\n    v72(filter)\n    v80(binning)\n    v87(cross)\n    v97(cross)\n    v103(filter)\n    v111(pad_tokenize)\n    v118(cross)\n    v128(cross)\n    v134(filter)\n    v164(concat)\n    v142(scgpt_celltype_annotation)\n    v149(cross)\n    v159(cross)\n    v170(cross)\n    v177(cross)\n    v189(cross)\n    v196(cross)\n    v200(Output)\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v72--&gt;v80\n    v80--&gt;v87\n    v72--&gt;v87\n    v72--&gt;v97\n    v103--&gt;v111\n    v111--&gt;v118\n    v103--&gt;v118\n    v103--&gt;v128\n    v134--&gt;v142\n    v142--&gt;v149\n    v134--&gt;v149\n    v134--&gt;v159\n    v159--&gt;v164\n    v164--&gt;v170\n    v2--&gt;v170\n    v170--&gt;v177\n    v2--&gt;v177\n    v2--&gt;v189\n    v189--&gt;v196\n    v2--&gt;v196\n    v196--&gt;v200\n    v35--&gt;v41\n    v18--&gt;v35\n    v66--&gt;v72\n    v49--&gt;v66\n    v97--&gt;v103\n    v80--&gt;v97\n    v128--&gt;v134\n    v111--&gt;v128\n    v142--&gt;v159\n    v164--&gt;v189\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v72 fill:#e3dcea,stroke:#7a4baa;\n    style v80 fill:#e3dcea,stroke:#7a4baa;\n    style v87 fill:#e3dcea,stroke:#7a4baa;\n    style v97 fill:#e3dcea,stroke:#7a4baa;\n    style v103 fill:#e3dcea,stroke:#7a4baa;\n    style v111 fill:#e3dcea,stroke:#7a4baa;\n    style v118 fill:#e3dcea,stroke:#7a4baa;\n    style v128 fill:#e3dcea,stroke:#7a4baa;\n    style v134 fill:#e3dcea,stroke:#7a4baa;\n    style v164 fill:#e3dcea,stroke:#7a4baa;\n    style v142 fill:#e3dcea,stroke:#7a4baa;\n    style v149 fill:#e3dcea,stroke:#7a4baa;\n    style v159 fill:#e3dcea,stroke:#7a4baa;\n    style v170 fill:#e3dcea,stroke:#7a4baa;\n    style v177 fill:#e3dcea,stroke:#7a4baa;\n    style v189 fill:#e3dcea,stroke:#7a4baa;\n    style v196 fill:#e3dcea,stroke:#7a4baa;\n    style v200 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html",
    "href": "components/workflows/ingestion/demux.html",
    "title": "Demux",
    "section": "",
    "text": "ID: demux\nNamespace: workflows/ingestion\n\n\n\nSource\nConvert .bcl files to .fastq files using bcl2fastq, bcl-convert or Cell Ranger mkfastq.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html#example-commands",
    "href": "components/workflows/ingestion/demux.html#example-commands",
    "title": "Demux",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/ingestion/demux/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Arguments\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"bcl_dir\"\nsample_sheet: # please fill in - example: \"bcl_dir\"\ndemultiplexer: \"bcl2fastq\"\n# ignore_missing: true\n# output_fastq: \"$id.$key.output_fastq\"\n# output_fastqc: \"$id.$key.output_fastqc\"\n# output_multiqc: \"$id.$key.output_multiqc\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/demux/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html#argument-group",
    "href": "components/workflows/ingestion/demux.html#argument-group",
    "title": "Demux",
    "section": "Argument group",
    "text": "Argument group\n\nArguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nInput run directory\nfile, required, example: \"bcl_dir\"\n\n\n--sample_sheet\nPointer to the sample sheet\nfile, required, example: \"bcl_dir\"\n\n\n--demultiplexer\nThe multiplexer to use, one of bclconvert or mkfastq\nstring, default: \"bcl2fastq\"\n\n\n--ignore_missing\nShould the demultiplexer ignore missing entities (filter, …)\nboolean\n\n\n--output_fastq\nOutput directory containig fastq files\nfile, required, example: \"fastq_dir\"\n\n\n--output_fastqc\nReports directory produced by FastQC\nfile, example: \"reports_dir\"\n\n\n--output_multiqc\nReports directory produced by MultiQC\nfile, example: \"reports_dir\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html#authors",
    "href": "components/workflows/ingestion/demux.html#authors",
    "title": "Demux",
    "section": "Authors",
    "text": "Authors\n\nToni Verbeiren   (author, maintainer)\nMarijke Van Moerbeke    (author)\nAngela Oliveira Pisco    (author)\nSamuel D’Souza   (author)\nRobrecht Cannoodt    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/demux.html#visualisation",
    "href": "components/workflows/ingestion/demux.html#visualisation",
    "title": "Demux",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v12(filter)\n    v20(cellranger_mkfastq)\n    v27(cross)\n    v37(cross)\n    v117(mix)\n    v48(filter)\n    v56(bcl_convert)\n    v63(cross)\n    v73(cross)\n    v84(filter)\n    v92(bcl2fastq)\n    v99(cross)\n    v109(cross)\n    v118(filter)\n    v126(fastqc)\n    v133(cross)\n    v143(cross)\n    v149(filter)\n    v179(concat)\n    v157(multiqc)\n    v164(cross)\n    v174(cross)\n    v186(cross)\n    v193(cross)\n    v205(cross)\n    v212(cross)\n    v216(Output)\n    v117--&gt;v118\n    v0--&gt;v2\n    v12--&gt;v20\n    v20--&gt;v27\n    v12--&gt;v27\n    v12--&gt;v37\n    v48--&gt;v56\n    v56--&gt;v63\n    v48--&gt;v63\n    v48--&gt;v73\n    v84--&gt;v92\n    v92--&gt;v99\n    v84--&gt;v99\n    v84--&gt;v109\n    v118--&gt;v126\n    v126--&gt;v133\n    v118--&gt;v133\n    v118--&gt;v143\n    v149--&gt;v157\n    v157--&gt;v164\n    v149--&gt;v164\n    v149--&gt;v174\n    v174--&gt;v179\n    v179--&gt;v186\n    v2--&gt;v186\n    v186--&gt;v193\n    v2--&gt;v193\n    v2--&gt;v205\n    v205--&gt;v212\n    v2--&gt;v212\n    v212--&gt;v216\n    v2--&gt;v12\n    v37--&gt;v117\n    v20--&gt;v37\n    v2--&gt;v48\n    v73--&gt;v117\n    v56--&gt;v73\n    v2--&gt;v84\n    v109--&gt;v117\n    v92--&gt;v109\n    v143--&gt;v149\n    v126--&gt;v143\n    v157--&gt;v174\n    v179--&gt;v205\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v12 fill:#e3dcea,stroke:#7a4baa;\n    style v20 fill:#e3dcea,stroke:#7a4baa;\n    style v27 fill:#e3dcea,stroke:#7a4baa;\n    style v37 fill:#e3dcea,stroke:#7a4baa;\n    style v117 fill:#e3dcea,stroke:#7a4baa;\n    style v48 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v63 fill:#e3dcea,stroke:#7a4baa;\n    style v73 fill:#e3dcea,stroke:#7a4baa;\n    style v84 fill:#e3dcea,stroke:#7a4baa;\n    style v92 fill:#e3dcea,stroke:#7a4baa;\n    style v99 fill:#e3dcea,stroke:#7a4baa;\n    style v109 fill:#e3dcea,stroke:#7a4baa;\n    style v118 fill:#e3dcea,stroke:#7a4baa;\n    style v126 fill:#e3dcea,stroke:#7a4baa;\n    style v133 fill:#e3dcea,stroke:#7a4baa;\n    style v143 fill:#e3dcea,stroke:#7a4baa;\n    style v149 fill:#e3dcea,stroke:#7a4baa;\n    style v179 fill:#e3dcea,stroke:#7a4baa;\n    style v157 fill:#e3dcea,stroke:#7a4baa;\n    style v164 fill:#e3dcea,stroke:#7a4baa;\n    style v174 fill:#e3dcea,stroke:#7a4baa;\n    style v186 fill:#e3dcea,stroke:#7a4baa;\n    style v193 fill:#e3dcea,stroke:#7a4baa;\n    style v205 fill:#e3dcea,stroke:#7a4baa;\n    style v212 fill:#e3dcea,stroke:#7a4baa;\n    style v216 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Demux"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html",
    "href": "components/workflows/ingestion/cellranger_multi.html",
    "title": "Cell Ranger multi",
    "section": "",
    "text": "ID: cellranger_multi\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html#example-commands",
    "href": "components/workflows/ingestion/cellranger_multi.html#example-commands",
    "title": "Cell Ranger multi",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_multi/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Input files\n# input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n\n# Feature type-specific input files\n# gex_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# abc_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# cgc_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# mux_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# vdj_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# vdj_t_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# vdj_t_gd_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# vdj_b_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n# agc_input: [\"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\"]\n\n# Library arguments\n# library_id: [\"mysample1\"]\n# library_type: [\"Gene Expression\"]\n# library_subsample: [\"0.5\"]\n# library_lanes: [\"1-4\"]\n# library_chemistry: \"foo\"\n\n# Sample parameters\n# sample_ids: [\"foo\"]\n# sample_description: [\"foo\"]\n# sample_expect_cells: [3000]\n# sample_force_cells: [3000]\n\n# Feature Barcode library specific arguments\n# feature_reference: \"feature_reference.csv\"\n# feature_r1_length: 123\n# feature_r2_length: 123\n# min_crispr_umi: 123\n\n# Gene expression arguments\ngex_reference: # please fill in - example: \"reference_genome.tar.gz\"\ngex_secondary_analysis: false\ngex_generate_bam: false\n# gex_expect_cells: 3000\n# gex_force_cells: 3000\ngex_include_introns: true\n# gex_r1_length: 123\n# gex_r2_length: 123\ngex_chemistry: \"auto\"\n\n# VDJ related parameters\n# vdj_reference: \"reference_vdj.tar.gz\"\n# vdj_inner_enrichment_primers: \"enrichment_primers.txt\"\n# vdj_r1_length: 123\n# vdj_r2_length: 123\n\n# Cell multiplexing parameters\n# cell_multiplex_oligo_ids: [\"foo\"]\n# min_assignment_confidence: 123.0\n# cmo_set: \"path/to/file\"\n# barcode_sample_assignment: \"path/to/file\"\n\n# Fixed RNA profiling paramaters\n# probe_set: \"path/to/file\"\n# filter_probes: true\n# probe_barcode_ids: [\"foo\"]\n\n# Antigen Capture (BEAM) libary arguments\n# control_id: [\"foo\"]\n# mhc_allele: [\"foo\"]\n\n# General arguments\ncheck_library_compatibility: true\n\n# Outputs\n# output_raw: \"$id.$key.output_raw\"\n# output_h5mu: \"$id.$key.output_h5mu.h5mu\"\nuns_metrics: \"metrics_cellranger\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_multi/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html#argument-groups",
    "href": "components/workflows/ingestion/cellranger_multi.html#argument-groups",
    "title": "Cell Ranger multi",
    "section": "Argument groups",
    "text": "Argument groups\n\nInput files\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--input\nThe FASTQ files to be analyzed. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n\n\n\nFeature type-specific input files\nHelper functionality to allow feature type-specific input files, without the need to specify library_type or library_id. The library_id will be inferred from the input paths.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--gex_input\nThe FASTQ files to be analyzed for Gene Expression. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--abc_input\nThe FASTQ files to be analyzed for Antibody Capture. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--cgc_input\nThe FASTQ files to be analyzed for CRISPR Guide Capture. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--mux_input\nThe FASTQ files to be analyzed for Multiplexing Capture. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--vdj_input\nThe FASTQ files to be analyzed for VDJ. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--vdj_t_input\nThe FASTQ files to be analyzed for VDJ-T. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--vdj_t_gd_input\nThe FASTQ files to be analyzed for VDJ-T-GD. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--vdj_b_input\nThe FASTQ files to be analyzed for VDJ-B. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--agc_input\nThe FASTQ files to be analyzed for Antigen Capture. FASTQ files should conform to the naming conventions of bcl2fastq and mkfastq: [Sample Name]_S[Sample Index]_L00[Lane Number]_[Read Type]_001.fastq.gz\nList of file, example: \"mysample_S1_L001_R1_001.fastq.gz\", \"mysample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n\n\n\nLibrary arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--library_id\nThe Illumina sample name to analyze. This must exactly match the ’Sample Name’part of the FASTQ files specified in the --input argument.\nList of string, example: \"mysample1\", multiple_sep: \";\"\n\n\n--library_type\nThe underlying feature type of the library.\nList of string, example: \"Gene Expression\", multiple_sep: \";\"\n\n\n--library_subsample\nThe rate at which reads from the provided FASTQ files are sampled. Must be strictly greater than 0 and less than or equal to 1.\nList of string, example: \"0.5\", multiple_sep: \";\"\n\n\n--library_lanes\nLanes associated with this sample. Defaults to using all lanes.\nList of string, example: \"1-4\", multiple_sep: \";\"\n\n\n--library_chemistry\nOnly applicable to FRP. Library-specific assay configuration. By default, the assay configuration is detected automatically. Typically, users will not need to specify a chemistry.\nstring\n\n\n\n\n\nSample parameters\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--sample_ids\nA name to identify a multiplexed sample. Must be alphanumeric with hyphens and/or underscores, and less than 64 characters. Required for Cell Multiplexing libraries.\nList of string, multiple_sep: \";\"\n\n\n--sample_description\nA description for the sample.\nList of string, multiple_sep: \";\"\n\n\n--sample_expect_cells\nExpected number of recovered cells, used as input to cell calling algorithm.\nList of integer, example: 3000, multiple_sep: \";\"\n\n\n--sample_force_cells\nForce pipeline to use this number of cells, bypassing cell detection.\nList of integer, example: 3000, multiple_sep: \";\"\n\n\n\n\n\nFeature Barcode library specific arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--feature_reference\nPath to the Feature reference CSV file, declaring Feature Barcode constructs and associated barcodes. Required only for Antibody Capture or CRISPR Guide Capture libraries. See https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/feature-bc-analysis#feature-ref for more information.”\nfile, example: \"feature_reference.csv\"\n\n\n--feature_r1_length\nLimit the length of the input Read 1 sequence of V(D)J libraries to the first N bases, where N is the user-supplied value. Note that the length includes the Barcode and UMI sequences so do not set this below 26.\ninteger\n\n\n--feature_r2_length\nLimit the length of the input Read 2 sequence of V(D)J libraries to the first N bases, where N is a user-supplied value. Trimming occurs before sequencing metrics are computed and therefore, limiting the length of Read 2 may affect Q30 scores.\ninteger\n\n\n--min_crispr_umi\nSet the minimum number of CRISPR guide RNA UMIs required for protospacer detection. If a lower or higher sensitivity is desired for detection, this value can be customized according to specific experimental needs. Applicable only to datasets that include a CRISPR Guide Capture library.\ninteger\n\n\n\n\n\nGene expression arguments\nArguments relevant to the analysis of gene expression data.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--gex_reference\nGenome refence index built by Cell Ranger mkref.\nfile, required, example: \"reference_genome.tar.gz\"\n\n\n--gex_secondary_analysis\nWhether or not to run the secondary analysis e.g. clustering.\nboolean, default: FALSE\n\n\n--gex_generate_bam\nWhether to generate a BAM file.\nboolean, default: FALSE\n\n\n--gex_expect_cells\nExpected number of recovered cells, used as input to cell calling algorithm.\ninteger, example: 3000\n\n\n--gex_force_cells\nForce pipeline to use this number of cells, bypassing cell detection.\ninteger, example: 3000\n\n\n--gex_include_introns\nWhether or not to include intronic reads in counts. This option does not apply to Fixed RNA Profiling analysis.\nboolean, default: TRUE\n\n\n--gex_r1_length\nLimit the length of the input Read 1 sequence of V(D)J libraries to the first N bases, where N is the user-supplied value. Note that the length includes the Barcode and UMI sequences so do not set this below 26.\ninteger\n\n\n--gex_r2_length\nLimit the length of the input Read 2 sequence of V(D)J libraries to the first N bases, where N is a user-supplied value. Trimming occurs before sequencing metrics are computed and therefore, limiting the length of Read 2 may affect Q30 scores.\ninteger\n\n\n--gex_chemistry\nAssay configuration. Either specify a single value which will be applied to all libraries, or a number of values that is equal to the number of libararies. The latter is only applicable to only applicable to Fixed RNA Profiling. - auto: Chemistry autodetection (default) - threeprime: Single Cell 3’ - SC3Pv1, SC3Pv2, SC3Pv3, SC3Pv4: Single Cell 3’ v1, v2, v3, or v4 - SC3Pv3HT: Single Cell 3’ v3.1 HT - SC-FB: Single Cell Antibody-only 3’ v2 or 5’ - fiveprime: Single Cell 5’ - SC5P-PE: Paired-end Single Cell 5’ - SC5P-R2: R2-only Single Cell 5’ - SC5P-R2-v3: R2-only Single Cell 5’ v3 - SCP5-PE-v3: Single Cell 5’ paired-end v3 (GEM-X) - SC5PHT : Single Cell 5’ v2 HT - SFRP: Fixed RNA Profiling (Singleplex) - MFRP: Fixed RNA Profiling (Multiplex, Probe Barcode on R2) - MFRP-R1: Fixed RNA Profiling (Multiplex, Probe Barcode on R1) - MFRP-RNA: Fixed RNA Profiling (Multiplex, RNA, Probe Barcode on R2) - MFRP-Ab: Fixed RNA Profiling (Multiplex, Antibody, Probe Barcode at R2:69) - MFRP-Ab-R2pos50: Fixed RNA Profiling (Multiplex, Antibody, Probe Barcode at R2:50) - MFRP-RNA-R1: Fixed RNA Profiling (Multiplex, RNA, Probe Barcode on R1) - MFRP-Ab-R1: Fixed RNA Profiling (Multiplex, Antibody, Probe Barcode on R1) - ARC-v1 for analyzing the Gene Expression portion of Multiome data. If Cell Ranger auto-detects ARC-v1 chemistry, an error is triggered. See https://kb.10xgenomics.com/hc/en-us/articles/115003764132-How-does-Cell-Ranger-auto-detect-chemistry- for more information.\nstring, default: \"auto\"\n\n\n\n\n\nVDJ related parameters\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--vdj_reference\nVDJ refence index built by Cell Ranger mkref.\nfile, example: \"reference_vdj.tar.gz\"\n\n\n--vdj_inner_enrichment_primers\nV(D)J Immune Profiling libraries: if inner enrichment primers other than those provided in the 10x Genomics kits are used, they need to be specified here as a text file with one primer per line.\nfile, example: \"enrichment_primers.txt\"\n\n\n--vdj_r1_length\nLimit the length of the input Read 1 sequence of V(D)J libraries to the first N bases, where N is the user-supplied value. Note that the length includes the Barcode and UMI sequences so do not set this below 26.\ninteger\n\n\n--vdj_r2_length\nLimit the length of the input Read 2 sequence of V(D)J libraries to the first N bases, where N is a user-supplied value. Trimming occurs before sequencing metrics are computed and therefore, limiting the length of Read 2 may affect Q30 scores\ninteger\n\n\n\n\n\nCell multiplexing parameters\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--cell_multiplex_oligo_ids\nThe Cell Multiplexing oligo IDs used to multiplex this sample. If multiple CMOs were used for a sample, separate IDs with a pipe (e.g., CMO301|CMO302). Required for Cell Multiplexing libraries.\nList of string, multiple_sep: \";\"\n\n\n--min_assignment_confidence\nThe minimum estimated likelihood to call a sample as tagged with a Cell Multiplexing Oligo (CMO) instead of “Unassigned”. Users may wish to tolerate a higher rate of mis-assignment in order to obtain more singlets to include in their analysis, or a lower rate of mis-assignment at the cost of obtaining fewer singlets.\ndouble\n\n\n--cmo_set\nPath to a custom CMO set CSV file, declaring CMO constructs and associated barcodes. If the default CMO reference IDs that are built into the Cell Ranger software are required, this option does not need to be used.\nfile\n\n\n--barcode_sample_assignment\nPath to a barcode-sample assignment CSV file that specifies the barcodes that belong to each sample.\nfile\n\n\n\n\n\nFixed RNA profiling paramaters\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--probe_set\nA probe set reference CSV file. It specifies the sequences used as a reference for probe alignment and the gene ID associated with each probe. It must include 4 columns (probe file format 1.0.0): gene_id,probe_seq,probe_id,included,region and an optional 5th column (probe file format 1.0.1). - gene_id: The Ensembl gene identifier targeted by the probe. - probe_seq: The nucleotide sequence of the probe, which is complementary to the transcript sequence. - probe_id: The probe identifier, whose format is described in Probe identifiers. - included: A TRUE or FALSE flag specifying whether the probe is included in the filtered counts matrix output or excluded by the probe filter. See filter-probes option of cellranger multi. All probes of a gene must be marked TRUE in the included column for that gene to be included. - region: Present only in v1.0.1 probe set reference CSV. The gene boundary targeted by the probe. Accepted values are spliced or unspliced. The file also contains a number of required metadata fields in the header in the format #key=value: - panel_name: The name of the probe set. - panel_type: Always predesigned for predesigned probe sets. - reference_genome: The reference genome build used for probe design. - reference_version: The version of the Cell Ranger reference transcriptome used for probe design. - probe_set_file_format: The version of the probe set file format specification that this file conforms to.\nfile\n\n\n--filter_probes\nIf ‘false’, include all non-deprecated probes listed in the probe set reference CSV file. If ‘true’ or not set, probes that are predicted to have off-target activity to homologous genes are excluded from analysis. Not filtering will result in UMI counts from all non-deprecated probes, including those with predicted off-target activity, to be used in the analysis. Probes whose ID is prefixed with DEPRECATED are always excluded from the analysis.\nboolean\n\n\n--probe_barcode_ids\nThe Fixed RNA Probe Barcode ID used for this sample, and for multiplex GEX + Antibody Capture libraries, the corresponding Antibody Multiplexing Barcode IDs. 10x recommends specifying both barcodes (e.g., BC001+AB001) when an Antibody Capture library is present. The barcode pair order is BC+AB and they are separated with a “+” (no spaces). Alternatively, you can specify the Probe Barcode ID alone and Cell Ranger’s barcode pairing auto-detection algorithm will automatically match to the corresponding Antibody Multiplexing Barcode.\nList of string, multiple_sep: \";\"\n\n\n\n\n\nAntigen Capture (BEAM) libary arguments\nThese arguments are recommended if an Antigen Capture (BEAM) library is present. It is needed to calculate the antigen specificity score.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--control_id\nA user-defined ID for any negative controls used in the T/BCR Antigen Capture assay. Must match id specified in the feature reference CSV. May only include ASCII characters and must not use whitespace, slash, quote, or comma characters. Each ID must be unique and must not collide with a gene identifier from the transcriptome.\nList of string, multiple_sep: \";\"\n\n\n--mhc_allele\nThe MHC allele for TCR Antigen Capture libraries. Must match mhc_allele name specified in the Feature Reference CSV.\nList of string, multiple_sep: \";\"\n\n\n\n\n\nGeneral arguments\nThese arguments are applicable to all library types.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--check_library_compatibility\nOptional. This option allows users to disable the check that evaluates 10x Barcode overlap between ibraries when multiple libraries are specified (e.g., Gene Expression + Antibody Capture). Setting this option to false will disable the check across all library combinations. We recommend running this check (default), however if the pipeline errors out, users can bypass the check to generate outputs for troubleshooting.\nboolean, default: TRUE\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output_raw\nThe raw output folder.\nfile, required, example: \"output_dir\"\n\n\n--output_h5mu\nLocations for the output files. Must contain a wildcard (*) character, which will be replaced with the sample name.\nfile, required, example: \"*.h5mu\"\n\n\n--uns_metrics\nName of the .uns slot under which to QC metrics (if any).\nstring, default: \"metrics_cellranger\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html#authors",
    "href": "components/workflows/ingestion/cellranger_multi.html#authors",
    "title": "Cell Ranger multi",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_multi.html#visualisation",
    "href": "components/workflows/ingestion/cellranger_multi.html#visualisation",
    "title": "Cell Ranger multi",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v9(filter)\n    v17(cellranger_multi_component)\n    v24(cross)\n    v34(cross)\n    v40(filter)\n    v48(from_cellranger_multi_to_h5mu)\n    v55(cross)\n    v65(cross)\n    v71(flatMap)\n    v77(cross)\n    v84(cross)\n    v96(cross)\n    v103(cross)\n    v107(Output)\n    v0--&gt;v2\n    v2--&gt;v9\n    v9--&gt;v17\n    v17--&gt;v24\n    v9--&gt;v24\n    v9--&gt;v34\n    v40--&gt;v48\n    v48--&gt;v55\n    v40--&gt;v55\n    v40--&gt;v65\n    v71--&gt;v77\n    v2--&gt;v77\n    v77--&gt;v84\n    v2--&gt;v84\n    v2--&gt;v96\n    v96--&gt;v103\n    v2--&gt;v103\n    v103--&gt;v107\n    v34--&gt;v40\n    v17--&gt;v34\n    v65--&gt;v71\n    v48--&gt;v65\n    v71--&gt;v96\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v9 fill:#e3dcea,stroke:#7a4baa;\n    style v17 fill:#e3dcea,stroke:#7a4baa;\n    style v24 fill:#e3dcea,stroke:#7a4baa;\n    style v34 fill:#e3dcea,stroke:#7a4baa;\n    style v40 fill:#e3dcea,stroke:#7a4baa;\n    style v48 fill:#e3dcea,stroke:#7a4baa;\n    style v55 fill:#e3dcea,stroke:#7a4baa;\n    style v65 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v77 fill:#e3dcea,stroke:#7a4baa;\n    style v84 fill:#e3dcea,stroke:#7a4baa;\n    style v96 fill:#e3dcea,stroke:#7a4baa;\n    style v103 fill:#e3dcea,stroke:#7a4baa;\n    style v107 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger multi"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html",
    "title": "Cell Ranger post-processing",
    "section": "",
    "text": "ID: cellranger_postprocessing\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html#example-commands",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html#example-commands",
    "title": "Cell Ranger post-processing",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_postprocessing/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\n\n# Outputs\n# output: \"$id.$key.output\"\n\n# Correction arguments\nperform_correction: false\ncellbender_epochs: 150\n\n# Filtering arguments\n# min_genes: 100\n# min_counts: 1000\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_postprocessing/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html#argument-groups",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html#argument-groups",
    "title": "Cell Ranger post-processing",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nInput h5mu file created by running Cell Ranger and converting its output to h5mu.\nfile, required, example: \"input.h5mu\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nThe converted h5mu file.\nfile\n\n\n\n\n\nCorrection arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--perform_correction\nWhether or not to run CellBender to perform count correction.\nboolean_true\n\n\n--cellbender_epochs\nNumber of epochs to run CellBender for.\ninteger, default: 150\n\n\n\n\n\nFiltering arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--min_genes\nMinimum number of counts required for a cell to pass filtering.\ninteger, example: 100\n\n\n--min_counts\nMinimum number of genes expressed required for a cell to pass filtering.\ninteger, example: 1000",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html#authors",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html#authors",
    "title": "Cell Ranger post-processing",
    "section": "Authors",
    "text": "Authors\n\nAngela Oliveira Pisco    (author)\nRobrecht Cannoodt    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_postprocessing.html#visualisation",
    "href": "components/workflows/ingestion/cellranger_postprocessing.html#visualisation",
    "title": "Cell Ranger post-processing",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v12(branch)\n    v39(concat)\n    v17(cellbender_remove_background)\n    v24(cross)\n    v34(cross)\n    v43(branch)\n    v70(concat)\n    v48(filter_with_counts)\n    v55(cross)\n    v65(cross)\n    v71(filter)\n    v101(concat)\n    v79(publish)\n    v86(cross)\n    v96(cross)\n    v108(cross)\n    v115(cross)\n    v127(cross)\n    v134(cross)\n    v138(Output)\n    v12--&gt;v39\n    v43--&gt;v70\n    v70--&gt;v71\n    v0--&gt;v2\n    v12--&gt;v17\n    v17--&gt;v24\n    v12--&gt;v24\n    v12--&gt;v34\n    v34--&gt;v39\n    v43--&gt;v48\n    v48--&gt;v55\n    v43--&gt;v55\n    v43--&gt;v65\n    v65--&gt;v70\n    v71--&gt;v79\n    v79--&gt;v86\n    v71--&gt;v86\n    v71--&gt;v96\n    v96--&gt;v101\n    v101--&gt;v108\n    v2--&gt;v108\n    v108--&gt;v115\n    v2--&gt;v115\n    v2--&gt;v127\n    v127--&gt;v134\n    v2--&gt;v134\n    v134--&gt;v138\n    v2--&gt;v12\n    v17--&gt;v34\n    v39--&gt;v43\n    v48--&gt;v65\n    v79--&gt;v96\n    v101--&gt;v127\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v12 fill:#e3dcea,stroke:#7a4baa;\n    style v39 fill:#e3dcea,stroke:#7a4baa;\n    style v17 fill:#e3dcea,stroke:#7a4baa;\n    style v24 fill:#e3dcea,stroke:#7a4baa;\n    style v34 fill:#e3dcea,stroke:#7a4baa;\n    style v43 fill:#e3dcea,stroke:#7a4baa;\n    style v70 fill:#e3dcea,stroke:#7a4baa;\n    style v48 fill:#e3dcea,stroke:#7a4baa;\n    style v55 fill:#e3dcea,stroke:#7a4baa;\n    style v65 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v101 fill:#e3dcea,stroke:#7a4baa;\n    style v79 fill:#e3dcea,stroke:#7a4baa;\n    style v86 fill:#e3dcea,stroke:#7a4baa;\n    style v96 fill:#e3dcea,stroke:#7a4baa;\n    style v108 fill:#e3dcea,stroke:#7a4baa;\n    style v115 fill:#e3dcea,stroke:#7a4baa;\n    style v127 fill:#e3dcea,stroke:#7a4baa;\n    style v134 fill:#e3dcea,stroke:#7a4baa;\n    style v138 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger post-processing"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html",
    "href": "components/workflows/ingestion/cellranger_mapping.html",
    "title": "Cell Ranger mapping",
    "section": "",
    "text": "ID: cellranger_mapping\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html#example-commands",
    "href": "components/workflows/ingestion/cellranger_mapping.html#example-commands",
    "title": "Cell Ranger mapping",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_mapping/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: [\"sample_S1_L001_R1_001.fastq.gz\", \"sample_S1_L001_R2_001.fastq.gz\"]\nreference: # please fill in - example: \"reference.tar.gz\"\n\n# Outputs\n# output_raw: \"$id.$key.output_raw\"\n# output_h5mu: \"$id.$key.output_h5mu.h5mu\"\nuns_metrics: \"metrics_summary\"\noutput_type: \"raw\"\n\n# Cell Ranger arguments\n# expect_cells: 3000\nchemistry: \"auto\"\nsecondary_analysis: false\ngenerate_bam: true\ninclude_introns: true\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/cellranger_mapping/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html#argument-groups",
    "href": "components/workflows/ingestion/cellranger_mapping.html#argument-groups",
    "title": "Cell Ranger mapping",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nThe fastq.gz files to align. Can also be a single directory containing fastq.gz files.\nList of file, required, example: \"sample_S1_L001_R1_001.fastq.gz\", \"sample_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--reference\nThe path to Cell Ranger reference tar.gz file.\nfile, required, example: \"reference.tar.gz\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output_raw\nLocation where the output folder from Cell Ranger will be stored.\nfile, required, example: \"output_dir\"\n\n\n--output_h5mu\nThe output from Cell Ranger, converted to h5mu.\nfile, required, example: \"output.h5mu\"\n\n\n--uns_metrics\nName of the .uns slot under which to QC metrics (if any).\nstring, default: \"metrics_summary\"\n\n\n--output_type\nWhich Cell Ranger output to use for converting to h5mu.\nstring, default: \"raw\"\n\n\n\n\n\nCell Ranger arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--expect_cells\nExpected number of recovered cells, used as input to cell calling algorithm.\ninteger, example: 3000\n\n\n--chemistry\nAssay configuration. - auto: autodetect mode - threeprime: Single Cell 3’ - fiveprime: Single Cell 5’ - SC3Pv1: Single Cell 3’ v1 - SC3Pv2: Single Cell 3’ v2 - SC3Pv3: Single Cell 3’ v3 - SC3Pv3LT: Single Cell 3’ v3 LT - SC3Pv3HT: Single Cell 3’ v3 HT - SC5P-PE: Single Cell 5’ paired-end - SC5P-R2: Single Cell 5’ R2-only - SC-FB: Single Cell Antibody-only 3’ v2 or 5’ See https://kb.10xgenomics.com/hc/en-us/articles/115003764132-How-does-Cell-Ranger-auto-detect-chemistry- for more information.\nstring, default: \"auto\"\n\n\n--secondary_analysis\nWhether or not to run the secondary analysis e.g. clustering.\nboolean, default: FALSE\n\n\n--generate_bam\nWhether to generate a BAM file.\nboolean, default: TRUE\n\n\n--include_introns\nInclude intronic reads in count (default=true unless –target-panel is specified in which case default=false)\nboolean, default: TRUE",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html#authors",
    "href": "components/workflows/ingestion/cellranger_mapping.html#authors",
    "title": "Cell Ranger mapping",
    "section": "Authors",
    "text": "Authors\n\nAngela Oliveira Pisco    (author)\nRobrecht Cannoodt    (author, maintainer)\nDries De Maeyer   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/cellranger_mapping.html#visualisation",
    "href": "components/workflows/ingestion/cellranger_mapping.html#visualisation",
    "title": "Cell Ranger mapping",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v9(filter)\n    v17(cellranger_count)\n    v24(cross)\n    v34(cross)\n    v40(filter)\n    v48(cellranger_count_split)\n    v55(cross)\n    v65(cross)\n    v71(filter)\n    v101(concat)\n    v79(from_10xh5_to_h5mu)\n    v86(cross)\n    v96(cross)\n    v108(cross)\n    v115(cross)\n    v127(cross)\n    v134(cross)\n    v138(Output)\n    v0--&gt;v2\n    v2--&gt;v9\n    v9--&gt;v17\n    v17--&gt;v24\n    v9--&gt;v24\n    v9--&gt;v34\n    v40--&gt;v48\n    v48--&gt;v55\n    v40--&gt;v55\n    v40--&gt;v65\n    v71--&gt;v79\n    v79--&gt;v86\n    v71--&gt;v86\n    v71--&gt;v96\n    v96--&gt;v101\n    v101--&gt;v108\n    v2--&gt;v108\n    v108--&gt;v115\n    v2--&gt;v115\n    v2--&gt;v127\n    v127--&gt;v134\n    v2--&gt;v134\n    v134--&gt;v138\n    v34--&gt;v40\n    v17--&gt;v34\n    v65--&gt;v71\n    v48--&gt;v65\n    v79--&gt;v96\n    v101--&gt;v127\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v9 fill:#e3dcea,stroke:#7a4baa;\n    style v17 fill:#e3dcea,stroke:#7a4baa;\n    style v24 fill:#e3dcea,stroke:#7a4baa;\n    style v34 fill:#e3dcea,stroke:#7a4baa;\n    style v40 fill:#e3dcea,stroke:#7a4baa;\n    style v48 fill:#e3dcea,stroke:#7a4baa;\n    style v55 fill:#e3dcea,stroke:#7a4baa;\n    style v65 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v101 fill:#e3dcea,stroke:#7a4baa;\n    style v79 fill:#e3dcea,stroke:#7a4baa;\n    style v86 fill:#e3dcea,stroke:#7a4baa;\n    style v96 fill:#e3dcea,stroke:#7a4baa;\n    style v108 fill:#e3dcea,stroke:#7a4baa;\n    style v115 fill:#e3dcea,stroke:#7a4baa;\n    style v127 fill:#e3dcea,stroke:#7a4baa;\n    style v134 fill:#e3dcea,stroke:#7a4baa;\n    style v138 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Cell Ranger mapping"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html",
    "href": "components/workflows/prot/prot_multisample.html",
    "title": "Prot multisample",
    "section": "",
    "text": "ID: prot_multisample\nNamespace: workflows/prot\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html#example-commands",
    "href": "components/workflows/prot/prot_multisample.html#example-commands",
    "title": "Prot multisample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/prot/prot_multisample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"concatenated\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# QC metrics calculation options\n# var_qc_metrics: [\"ercc,highly_variable\"]\ntop_n_vars: [50, 100, 200, 500]\noutput_obs_num_nonzero_vars: \"num_nonzero_vars\"\noutput_obs_total_counts_vars: \"total_counts\"\noutput_var_num_nonzero_obs: \"num_nonzero_obs\"\noutput_var_total_counts_obs: \"total_counts\"\noutput_var_obs_mean: \"obs_mean\"\noutput_var_pct_dropout: \"pct_dropout\"\n\n# CLR arguments\nclr_axis: 0\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/prot/prot_multisample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html#argument-groups",
    "href": "components/workflows/prot/prot_multisample.html#argument-groups",
    "title": "Prot multisample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the concatenated file\nstring, required, example: \"concatenated\"\n\n\n--input\nPath to the samples.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nInput layer to use. If not specified, .X is used.\nstring\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes. Defaults to the value from –var_name_mitochondrial_genes.\nList of string, example: \"ercc,highly_variable\", multiple_sep: \",\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \",\"\n\n\n--output_obs_num_nonzero_vars\nName of column in .obs describing, for each observation, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each row the number of columns that contain data.\nstring, default: \"num_nonzero_vars\"\n\n\n--output_obs_total_counts_vars\nName of the column for .obs describing, for each observation (row), the sum of the stored values in the columns.\nstring, default: \"total_counts\"\n\n\n--output_var_num_nonzero_obs\nName of column describing, for each feature, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each column the number of rows that contain data.\nstring, default: \"num_nonzero_obs\"\n\n\n--output_var_total_counts_obs\nName of the column in .var describing, for each feature (column), the sum of the stored values in the rows.\nstring, default: \"total_counts\"\n\n\n--output_var_obs_mean\nName of the column in .obs providing the mean of the values in each row.\nstring, default: \"obs_mean\"\n\n\n--output_var_pct_dropout\nName of the column in .obs providing for each feature the percentage of observations the feature does not appear on (i.e. is missing). Same as --output_var_num_nonzero_obs but percentage based.\nstring, default: \"pct_dropout\"\n\n\n\n\n\nCLR arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--clr_axis\nAxis across which CLR is performed.\ninteger, default: 0",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html#authors",
    "href": "components/workflows/prot/prot_multisample.html#authors",
    "title": "Prot multisample",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_multisample.html#visualisation",
    "href": "components/workflows/prot/prot_multisample.html#visualisation",
    "title": "Prot multisample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(clr)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v167(concat)\n    v51(filter)\n    v66(cross)\n    v76(cross)\n    v83(mix)\n    v84(filter)\n    v99(cross)\n    v109(cross)\n    v115(filter)\n    v145(concat)\n    v130(cross)\n    v140(cross)\n    v152(cross)\n    v162(cross)\n    v174(cross)\n    v181(cross)\n    v193(cross)\n    v200(cross)\n    v204(Output)\n    subgraph group_prot_qc [prot_qc]\n        v59(grep_annotation_column)\n        v92(calculate_qc_metrics)\n        v123(publish)\n    end\n    v83--&gt;v84\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v51--&gt;v59\n    v59--&gt;v66\n    v51--&gt;v66\n    v51--&gt;v76\n    v84--&gt;v92\n    v92--&gt;v99\n    v84--&gt;v99\n    v84--&gt;v109\n    v115--&gt;v123\n    v123--&gt;v130\n    v115--&gt;v130\n    v115--&gt;v140\n    v140--&gt;v145\n    v145--&gt;v152\n    v41--&gt;v152\n    v41--&gt;v162\n    v162--&gt;v167\n    v167--&gt;v174\n    v2--&gt;v174\n    v174--&gt;v181\n    v2--&gt;v181\n    v2--&gt;v193\n    v193--&gt;v200\n    v2--&gt;v200\n    v200--&gt;v204\n    v35--&gt;v41\n    v18--&gt;v35\n    v41--&gt;v51\n    v76--&gt;v83\n    v59--&gt;v76\n    v41--&gt;v83\n    v109--&gt;v115\n    v92--&gt;v109\n    v123--&gt;v140\n    v145--&gt;v162\n    v167--&gt;v193\n    style group_prot_qc fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v167 fill:#e3dcea,stroke:#7a4baa;\n    style v51 fill:#e3dcea,stroke:#7a4baa;\n    style v59 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v76 fill:#e3dcea,stroke:#7a4baa;\n    style v83 fill:#e3dcea,stroke:#7a4baa;\n    style v84 fill:#e3dcea,stroke:#7a4baa;\n    style v92 fill:#e3dcea,stroke:#7a4baa;\n    style v99 fill:#e3dcea,stroke:#7a4baa;\n    style v109 fill:#e3dcea,stroke:#7a4baa;\n    style v115 fill:#e3dcea,stroke:#7a4baa;\n    style v145 fill:#e3dcea,stroke:#7a4baa;\n    style v123 fill:#e3dcea,stroke:#7a4baa;\n    style v130 fill:#e3dcea,stroke:#7a4baa;\n    style v140 fill:#e3dcea,stroke:#7a4baa;\n    style v152 fill:#e3dcea,stroke:#7a4baa;\n    style v162 fill:#e3dcea,stroke:#7a4baa;\n    style v174 fill:#e3dcea,stroke:#7a4baa;\n    style v181 fill:#e3dcea,stroke:#7a4baa;\n    style v193 fill:#e3dcea,stroke:#7a4baa;\n    style v200 fill:#e3dcea,stroke:#7a4baa;\n    style v204 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot multisample"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html",
    "href": "components/workflows/multiomics/split_modalities.html",
    "title": "Split modalities",
    "section": "",
    "text": "ID: split_modalities\nNamespace: workflows/multiomics\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html#example-commands",
    "href": "components/workflows/multiomics/split_modalities.html#example-commands",
    "title": "Split modalities",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/multiomics/split_modalities/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\n\n# Outputs\n# output: \"$id.$key.output\"\n# output_types: \"$id.$key.output_types.csv\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/multiomics/split_modalities/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html#argument-groups",
    "href": "components/workflows/multiomics/split_modalities.html#argument-groups",
    "title": "Split modalities",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"input.h5mu\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nOutput directory containing multiple h5mu files.\nfile, required, example: \"/path/to/output\"\n\n\n--output_types\nA csv containing the base filename and modality type per output file.\nfile, required, example: \"types.csv\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html#authors",
    "href": "components/workflows/multiomics/split_modalities.html#authors",
    "title": "Split modalities",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/split_modalities.html#visualisation",
    "href": "components/workflows/multiomics/split_modalities.html#visualisation",
    "title": "Split modalities",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v13(filter)\n    v18(split_modalities_component)\n    v25(cross)\n    v35(cross)\n    v45(concat)\n    v52(cross)\n    v59(cross)\n    v71(cross)\n    v78(cross)\n    v82(Output)\n    v0--&gt;v2\n    v13--&gt;v18\n    v18--&gt;v25\n    v13--&gt;v25\n    v13--&gt;v35\n    v45--&gt;v52\n    v2--&gt;v52\n    v52--&gt;v59\n    v2--&gt;v59\n    v2--&gt;v71\n    v71--&gt;v78\n    v2--&gt;v78\n    v78--&gt;v82\n    v2--&gt;v13\n    v35--&gt;v45\n    v18--&gt;v35\n    v45--&gt;v71\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v13 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v45 fill:#e3dcea,stroke:#7a4baa;\n    style v52 fill:#e3dcea,stroke:#7a4baa;\n    style v59 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v78 fill:#e3dcea,stroke:#7a4baa;\n    style v82 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Split modalities"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html",
    "href": "components/workflows/multiomics/process_samples.html",
    "title": "Process samples",
    "section": "",
    "text": "ID: process_samples\nNamespace: workflows/multiomics\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html#example-commands",
    "href": "components/workflows/multiomics/process_samples.html#example-commands",
    "title": "Process samples",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/multiomics/process_samples/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\n# rna_layer: \"foo\"\n# prot_layer: \"foo\"\n# gdo_layer: \"foo\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Sample ID options\nadd_id_to_obs: true\nadd_id_obs_output: \"sample_id\"\nadd_id_make_observation_keys_unique: true\n\n# RNA filtering options\n# rna_min_counts: 200\n# rna_max_counts: 5000000\n# rna_min_genes_per_cell: 200\n# rna_max_genes_per_cell: 1500000\n# rna_min_cells_per_gene: 3\n# rna_min_fraction_mito: 0.0\n# rna_max_fraction_mito: 0.2\n\n# CITE-seq filtering options\n# prot_min_counts: 3\n# prot_max_counts: 5000000\n# prot_min_proteins_per_cell: 200\n# prot_max_proteins_per_cell: 100000000\n# prot_min_cells_per_protein: 3\n\n# GDO filtering options\n# gdo_min_counts: 3\n# gdo_max_counts: 5000000\n# gdo_min_guides_per_cell: 200\n# gdo_max_guides_per_cell: 100000000\n# gdo_min_cells_per_guide: 3\n\n# Highly variable features detection\nhighly_variable_features_var_output: \"filter_with_hvg\"\nhighly_variable_features_obs_batch_key: \"sample_id\"\n\n# Mitochondrial Gene Detection\n# var_name_mitochondrial_genes: \"foo\"\n# obs_name_mitochondrial_fraction: \"foo\"\n# var_gene_names: \"gene_symbol\"\nmitochondrial_gene_regex: \"^[mM][tT]-\"\n\n# QC metrics calculation options\n# var_qc_metrics: [\"ercc,highly_variable\"]\ntop_n_vars: [50, 100, 200, 500]\n\n# PCA options\npca_overwrite: false\n\n# CLR options\nclr_axis: 0\n\n# RNA Scaling options\nrna_enable_scaling: false\nrna_scaling_output_layer: \"scaled\"\nrna_scaling_pca_obsm_output: \"scaled_pca\"\nrna_scaling_pca_loadings_varm_output: \"scaled_pca_loadings\"\nrna_scaling_pca_variance_uns_output: \"scaled_pca_variance\"\nrna_scaling_umap_obsm_output: \"scaled_umap\"\n# rna_scaling_max_value: 123.0\nrna_scaling_zero_center: true\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/multiomics/process_samples/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html#argument-groups",
    "href": "components/workflows/multiomics/process_samples.html#argument-groups",
    "title": "Process samples",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"input.h5mu\"\n\n\n--rna_layer\nInput layer for the gene expression modality. If not specified, .X is used.\nstring\n\n\n--prot_layer\nInput layer for the antibody capture modality. If not specified, .X is used.\nstring\n\n\n--gdo_layer\nInput layer for the guide-derived oligonucleotide (GDO) data. If not specified, .X is used.\nstring\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nSample ID options\nOptions for adding the id to .obs on the MuData object. Having a sample id present in a requirement of several components for this pipeline.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--add_id_to_obs\nAdd the value passed with –id to .obs.\nboolean, default: TRUE\n\n\n--add_id_obs_output\n.Obs column to add the sample IDs to. Required and only used when –add_id_to_obs is set to ‘true’\nstring, default: \"sample_id\"\n\n\n--add_id_make_observation_keys_unique\nJoin the id to the .obs index (.obs_names). Only used when –add_id_to_obs is set to ‘true’.\nboolean, default: TRUE\n\n\n\n\n\nRNA filtering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_min_counts\nMinimum number of counts captured per cell.\ninteger, example: 200\n\n\n--rna_max_counts\nMaximum number of counts captured per cell.\ninteger, example: 5000000\n\n\n--rna_min_genes_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--rna_max_genes_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 1500000\n\n\n--rna_min_cells_per_gene\nMinimum of non-zero values per gene.\ninteger, example: 3\n\n\n--rna_min_fraction_mito\nMinimum fraction of UMIs that are mitochondrial.\ndouble, example: 0\n\n\n--rna_max_fraction_mito\nMaximum fraction of UMIs that are mitochondrial.\ndouble, example: 0.2\n\n\n\n\n\nCITE-seq filtering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--prot_min_counts\nMinimum number of counts per cell.\ninteger, example: 3\n\n\n--prot_max_counts\nMinimum number of counts per cell.\ninteger, example: 5000000\n\n\n--prot_min_proteins_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--prot_max_proteins_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 100000000\n\n\n--prot_min_cells_per_protein\nMinimum of non-zero values per protein.\ninteger, example: 3\n\n\n\n\n\nGDO filtering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--gdo_min_counts\nMinimum number of counts per cell.\ninteger, example: 3\n\n\n--gdo_max_counts\nMinimum number of counts per cell.\ninteger, example: 5000000\n\n\n--gdo_min_guides_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--gdo_max_guides_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 100000000\n\n\n--gdo_min_cells_per_guide\nMinimum of non-zero values per guide.\ninteger, example: 3\n\n\n\n\n\nHighly variable features detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--highly_variable_features_var_output\nIn which .var slot to store a boolean array corresponding to the highly variable genes.\nstring, default: \"filter_with_hvg\"\n\n\n--highly_variable_features_obs_batch_key\nIf specified, highly-variable genes are selected within each batch separately and merged. This simple process avoids the selection of batch-specific genes and acts as a lightweight batch correction method.\nstring, default: \"sample_id\"\n\n\n\n\n\nMitochondrial Gene Detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_name_mitochondrial_genes\nIn which .var slot to store a boolean array corresponding the mitochondrial genes.\nstring\n\n\n--obs_name_mitochondrial_fraction\nWhen specified, write the fraction of counts originating from mitochondrial genes (based on –mitochondrial_gene_regex) to an .obs column with the specified name. Requires –var_name_mitochondrial_genes.\nstring\n\n\n--var_gene_names\n.var column name to be used to detect mitochondrial genes instead of .var_names (default if not set). Gene names matching with the regex value from –mitochondrial_gene_regex will be identified as a mitochondrial gene.\nstring, example: \"gene_symbol\"\n\n\n--mitochondrial_gene_regex\nRegex string that identifies mitochondrial genes from –var_gene_names. By default will detect human and mouse mitochondrial genes from a gene symbol.\nstring, default: \"^[mM][tT]-\"\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes. Defaults to the combined values specified for –var_name_mitochondrial_genes and –highly_variable_features_var_output.\nList of string, example: \"ercc,highly_variable\", multiple_sep: \",\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \",\"\n\n\n\n\n\nPCA options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--pca_overwrite\nAllow overwriting slots for PCA output.\nboolean_true\n\n\n\n\n\nCLR options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--clr_axis\nAxis to perform the CLR transformation on.\ninteger, default: 0\n\n\n\n\n\nRNA Scaling options\nOptions for enabling scaling of the log-normalized data to unit variance and zero mean. The scaled data will be output a different layer and representation with reduced dimensions will be created and stored in addition to the non-scaled data.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_enable_scaling\nEnable scaling for the RNA modality.\nboolean_true\n\n\n--rna_scaling_output_layer\nOutput layer where the scaled log-normalized data will be stored.\nstring, default: \"scaled\"\n\n\n--rna_scaling_pca_obsm_output\nName of the .obsm key where the PCA representation of the log-normalized and scaled data is stored.\nstring, default: \"scaled_pca\"\n\n\n--rna_scaling_pca_loadings_varm_output\nName of the .varm key where the PCA loadings of the log-normalized and scaled data is stored.\nstring, default: \"scaled_pca_loadings\"\n\n\n--rna_scaling_pca_variance_uns_output\nName of the .uns key where the variance and variance ratio will be stored as a map. The map will contain two keys: variance and variance_ratio respectively.\nstring, default: \"scaled_pca_variance\"\n\n\n--rna_scaling_umap_obsm_output\nName of the .obsm key where the UMAP representation of the log-normalized and scaled data is stored.\nstring, default: \"scaled_umap\"\n\n\n--rna_scaling_max_value\nClip (truncate) data to this value after scaling. If not specified, do not clip.\ndouble\n\n\n--rna_scaling_zero_center\nIf set, omit zero-centering variables, which allows to handle sparse input efficiently.”\nboolean_false",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html#authors",
    "href": "components/workflows/multiomics/process_samples.html#authors",
    "title": "Process samples",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_samples.html#visualisation",
    "href": "components/workflows/multiomics/process_samples.html#visualisation",
    "title": "Process samples",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v15(filter)\n    v20(add_id)\n    v27(cross)\n    v37(cross)\n    v43(filter)\n    v54(filter)\n    v66(cross)\n    v76(cross)\n    v86(concat)\n    v93(cross)\n    v103(cross)\n    v109(flatMap)\n    v115(filter)\n    v124(filter)\n    v134(filter)\n    v149(cross)\n    v159(cross)\n    v166(mix)\n    v167(filter)\n    v182(cross)\n    v192(cross)\n    v198(filter)\n    v228(concat)\n    v213(cross)\n    v223(cross)\n    v235(cross)\n    v245(cross)\n    v254(branch)\n    v281(concat)\n    v266(cross)\n    v276(cross)\n    v282(filter)\n    v297(cross)\n    v307(cross)\n    v313(filter)\n    v328(cross)\n    v338(cross)\n    v344(filter)\n    v374(concat)\n    v359(cross)\n    v369(cross)\n    v381(cross)\n    v391(cross)\n    v593(mix)\n    v402(filter)\n    v410(filter)\n    v425(cross)\n    v435(cross)\n    v441(filter)\n    v471(concat)\n    v456(cross)\n    v466(cross)\n    v478(cross)\n    v488(cross)\n    v499(filter)\n    v507(filter)\n    v522(cross)\n    v532(cross)\n    v538(filter)\n    v568(concat)\n    v553(cross)\n    v563(cross)\n    v575(cross)\n    v585(cross)\n    v595(mix)\n    v602(filter)\n    v610(concatenate_h5mu)\n    v617(cross)\n    v627(cross)\n    v636(filter)\n    v1689(concat)\n    v645(filter)\n    v656(filter)\n    v668(cross)\n    v678(cross)\n    v688(concat)\n    v695(cross)\n    v705(cross)\n    v711(flatMap)\n    v719(filter)\n    v727(filter)\n    v742(cross)\n    v752(cross)\n    v758(filter)\n    v773(cross)\n    v783(cross)\n    v789(filter)\n    v804(cross)\n    v814(cross)\n    v823(branch)\n    v850(concat)\n    v835(cross)\n    v845(cross)\n    v851(filter)\n    v866(cross)\n    v876(cross)\n    v882(filter)\n    v1008(concat)\n    v892(filter)\n    v907(cross)\n    v917(cross)\n    v924(mix)\n    v925(filter)\n    v940(cross)\n    v950(cross)\n    v956(filter)\n    v986(concat)\n    v971(cross)\n    v981(cross)\n    v993(cross)\n    v1003(cross)\n    v1015(cross)\n    v1025(cross)\n    v1226(mix)\n    v1036(filter)\n    v1044(filter)\n    v1059(cross)\n    v1069(cross)\n    v1075(filter)\n    v1201(concat)\n    v1085(filter)\n    v1100(cross)\n    v1110(cross)\n    v1117(mix)\n    v1118(filter)\n    v1133(cross)\n    v1143(cross)\n    v1149(filter)\n    v1179(concat)\n    v1164(cross)\n    v1174(cross)\n    v1186(cross)\n    v1196(cross)\n    v1208(cross)\n    v1218(cross)\n    v1228(mix)\n    v1236(filter)\n    v1251(cross)\n    v1261(cross)\n    v1271(branch)\n    v1390(concat)\n    v1276(filter)\n    v1291(cross)\n    v1301(cross)\n    v1307(filter)\n    v1322(cross)\n    v1332(cross)\n    v1338(filter)\n    v1368(concat)\n    v1353(cross)\n    v1363(cross)\n    v1375(cross)\n    v1385(cross)\n    v1394(branch)\n    v1513(concat)\n    v1399(filter)\n    v1414(cross)\n    v1424(cross)\n    v1430(filter)\n    v1445(cross)\n    v1455(cross)\n    v1461(filter)\n    v1491(concat)\n    v1476(cross)\n    v1486(cross)\n    v1498(cross)\n    v1508(cross)\n    v1517(branch)\n    v1636(concat)\n    v1522(filter)\n    v1537(cross)\n    v1547(cross)\n    v1553(filter)\n    v1568(cross)\n    v1578(cross)\n    v1584(filter)\n    v1614(concat)\n    v1599(cross)\n    v1609(cross)\n    v1621(cross)\n    v1631(cross)\n    v1637(filter)\n    v1667(concat)\n    v1652(cross)\n    v1662(cross)\n    v1674(cross)\n    v1684(cross)\n    v1696(cross)\n    v1703(cross)\n    v1715(cross)\n    v1722(cross)\n    v1726(Output)\n    subgraph group_split_modalities_workflow [split_modalities_workflow]\n        v59(split_modalities_component)\n    end\n    subgraph group_rna_singlesample [rna_singlesample]\n        v142(grep_annotation_column)\n        v175(calculate_qc_metrics)\n        v206(publish)\n        v259(delimit_fraction)\n        v290(rna_filter_with_counts)\n        v321(rna_do_filter)\n        v352(filter_with_scrublet)\n    end\n    subgraph group_prot_singlesample [prot_singlesample]\n        v418(prot_filter_with_counts)\n        v449(prot_do_filter)\n    end\n    subgraph group_gdo_singlesample [gdo_singlesample]\n        v515(gdo_filter_with_counts)\n        v546(gdo_do_filter)\n    end\n    subgraph group_process_batches [process_batches]\n        v661(split_modalities_component)\n        v1244(merge)\n        v1284(pca)\n        v1315(find_neighbors)\n        v1346(umap)\n        v1407(pca)\n        v1438(find_neighbors)\n        v1469(umap)\n        v1530(pca)\n        v1561(find_neighbors)\n        v1592(umap)\n        v1645(publish)\n        subgraph group_rna_multisample [rna_multisample]\n            v735(normalize_total)\n            v766(log1p)\n            v797(delete_layer)\n            v828(scale)\n            v859(highly_variable_features_scanpy)\n            v900(grep_annotation_column)\n            v933(calculate_qc_metrics)\n            v964(publish)\n        end\n        subgraph group_prot_multisample [prot_multisample]\n            v1052(clr)\n            v1093(grep_annotation_column)\n            v1126(calculate_qc_metrics)\n            v1157(publish)\n        end\n    end\n    v166--&gt;v167\n    v254--&gt;v281\n    v281--&gt;v282\n    v593--&gt;v595\n    v823--&gt;v850\n    v850--&gt;v851\n    v924--&gt;v925\n    v1117--&gt;v1118\n    v1226--&gt;v1228\n    v1271--&gt;v1390\n    v1394--&gt;v1513\n    v1517--&gt;v1636\n    v1636--&gt;v1637\n    v0--&gt;v2\n    v15--&gt;v20\n    v20--&gt;v27\n    v15--&gt;v27\n    v15--&gt;v37\n    v54--&gt;v59\n    v59--&gt;v66\n    v54--&gt;v66\n    v54--&gt;v76\n    v86--&gt;v93\n    v43--&gt;v93\n    v43--&gt;v103\n    v115--&gt;v124\n    v134--&gt;v142\n    v142--&gt;v149\n    v134--&gt;v149\n    v134--&gt;v159\n    v167--&gt;v175\n    v175--&gt;v182\n    v167--&gt;v182\n    v167--&gt;v192\n    v198--&gt;v206\n    v206--&gt;v213\n    v198--&gt;v213\n    v198--&gt;v223\n    v223--&gt;v228\n    v228--&gt;v235\n    v124--&gt;v235\n    v124--&gt;v245\n    v254--&gt;v259\n    v259--&gt;v266\n    v254--&gt;v266\n    v254--&gt;v276\n    v276--&gt;v281\n    v282--&gt;v290\n    v290--&gt;v297\n    v282--&gt;v297\n    v282--&gt;v307\n    v313--&gt;v321\n    v321--&gt;v328\n    v313--&gt;v328\n    v313--&gt;v338\n    v344--&gt;v352\n    v352--&gt;v359\n    v344--&gt;v359\n    v344--&gt;v369\n    v369--&gt;v374\n    v374--&gt;v381\n    v115--&gt;v381\n    v115--&gt;v391\n    v402--&gt;v410\n    v410--&gt;v418\n    v418--&gt;v425\n    v410--&gt;v425\n    v410--&gt;v435\n    v441--&gt;v449\n    v449--&gt;v456\n    v441--&gt;v456\n    v441--&gt;v466\n    v466--&gt;v471\n    v471--&gt;v478\n    v402--&gt;v478\n    v402--&gt;v488\n    v499--&gt;v507\n    v507--&gt;v515\n    v515--&gt;v522\n    v507--&gt;v522\n    v507--&gt;v532\n    v538--&gt;v546\n    v546--&gt;v553\n    v538--&gt;v553\n    v538--&gt;v563\n    v563--&gt;v568\n    v568--&gt;v575\n    v499--&gt;v575\n    v499--&gt;v585\n    v602--&gt;v610\n    v610--&gt;v617\n    v602--&gt;v617\n    v602--&gt;v627\n    v656--&gt;v661\n    v661--&gt;v668\n    v656--&gt;v668\n    v656--&gt;v678\n    v688--&gt;v695\n    v645--&gt;v695\n    v645--&gt;v705\n    v719--&gt;v727\n    v727--&gt;v735\n    v735--&gt;v742\n    v727--&gt;v742\n    v727--&gt;v752\n    v758--&gt;v766\n    v766--&gt;v773\n    v758--&gt;v773\n    v758--&gt;v783\n    v789--&gt;v797\n    v797--&gt;v804\n    v789--&gt;v804\n    v789--&gt;v814\n    v823--&gt;v828\n    v828--&gt;v835\n    v823--&gt;v835\n    v823--&gt;v845\n    v845--&gt;v850\n    v851--&gt;v859\n    v859--&gt;v866\n    v851--&gt;v866\n    v851--&gt;v876\n    v892--&gt;v900\n    v900--&gt;v907\n    v892--&gt;v907\n    v892--&gt;v917\n    v925--&gt;v933\n    v933--&gt;v940\n    v925--&gt;v940\n    v925--&gt;v950\n    v956--&gt;v964\n    v964--&gt;v971\n    v956--&gt;v971\n    v956--&gt;v981\n    v981--&gt;v986\n    v986--&gt;v993\n    v882--&gt;v993\n    v882--&gt;v1003\n    v1003--&gt;v1008\n    v1008--&gt;v1015\n    v719--&gt;v1015\n    v719--&gt;v1025\n    v1036--&gt;v1044\n    v1044--&gt;v1052\n    v1052--&gt;v1059\n    v1044--&gt;v1059\n    v1044--&gt;v1069\n    v1085--&gt;v1093\n    v1093--&gt;v1100\n    v1085--&gt;v1100\n    v1085--&gt;v1110\n    v1118--&gt;v1126\n    v1126--&gt;v1133\n    v1118--&gt;v1133\n    v1118--&gt;v1143\n    v1149--&gt;v1157\n    v1157--&gt;v1164\n    v1149--&gt;v1164\n    v1149--&gt;v1174\n    v1174--&gt;v1179\n    v1179--&gt;v1186\n    v1075--&gt;v1186\n    v1075--&gt;v1196\n    v1196--&gt;v1201\n    v1201--&gt;v1208\n    v1036--&gt;v1208\n    v1036--&gt;v1218\n    v1236--&gt;v1244\n    v1244--&gt;v1251\n    v1236--&gt;v1251\n    v1236--&gt;v1261\n    v1271--&gt;v1276\n    v1276--&gt;v1284\n    v1284--&gt;v1291\n    v1276--&gt;v1291\n    v1276--&gt;v1301\n    v1307--&gt;v1315\n    v1315--&gt;v1322\n    v1307--&gt;v1322\n    v1307--&gt;v1332\n    v1338--&gt;v1346\n    v1346--&gt;v1353\n    v1338--&gt;v1353\n    v1338--&gt;v1363\n    v1363--&gt;v1368\n    v1368--&gt;v1375\n    v1271--&gt;v1375\n    v1271--&gt;v1385\n    v1385--&gt;v1390\n    v1394--&gt;v1399\n    v1399--&gt;v1407\n    v1407--&gt;v1414\n    v1399--&gt;v1414\n    v1399--&gt;v1424\n    v1430--&gt;v1438\n    v1438--&gt;v1445\n    v1430--&gt;v1445\n    v1430--&gt;v1455\n    v1461--&gt;v1469\n    v1469--&gt;v1476\n    v1461--&gt;v1476\n    v1461--&gt;v1486\n    v1486--&gt;v1491\n    v1491--&gt;v1498\n    v1394--&gt;v1498\n    v1394--&gt;v1508\n    v1508--&gt;v1513\n    v1517--&gt;v1522\n    v1522--&gt;v1530\n    v1530--&gt;v1537\n    v1522--&gt;v1537\n    v1522--&gt;v1547\n    v1553--&gt;v1561\n    v1561--&gt;v1568\n    v1553--&gt;v1568\n    v1553--&gt;v1578\n    v1584--&gt;v1592\n    v1592--&gt;v1599\n    v1584--&gt;v1599\n    v1584--&gt;v1609\n    v1609--&gt;v1614\n    v1614--&gt;v1621\n    v1517--&gt;v1621\n    v1517--&gt;v1631\n    v1631--&gt;v1636\n    v1637--&gt;v1645\n    v1645--&gt;v1652\n    v1637--&gt;v1652\n    v1637--&gt;v1662\n    v1662--&gt;v1667\n    v1667--&gt;v1674\n    v636--&gt;v1674\n    v636--&gt;v1684\n    v1684--&gt;v1689\n    v1689--&gt;v1696\n    v2--&gt;v1696\n    v1696--&gt;v1703\n    v2--&gt;v1703\n    v2--&gt;v1715\n    v1715--&gt;v1722\n    v2--&gt;v1722\n    v1722--&gt;v1726\n    v2--&gt;v15\n    v37--&gt;v43\n    v20--&gt;v37\n    v103--&gt;v109\n    v43--&gt;v54\n    v76--&gt;v86\n    v59--&gt;v76\n    v86--&gt;v103\n    v109--&gt;v115\n    v391--&gt;v593\n    v124--&gt;v134\n    v159--&gt;v166\n    v142--&gt;v159\n    v124--&gt;v166\n    v192--&gt;v198\n    v175--&gt;v192\n    v206--&gt;v223\n    v228--&gt;v245\n    v245--&gt;v254\n    v259--&gt;v276\n    v307--&gt;v313\n    v290--&gt;v307\n    v338--&gt;v344\n    v321--&gt;v338\n    v352--&gt;v369\n    v374--&gt;v391\n    v109--&gt;v402\n    v488--&gt;v593\n    v435--&gt;v441\n    v418--&gt;v435\n    v449--&gt;v466\n    v471--&gt;v488\n    v109--&gt;v499\n    v585--&gt;v593\n    v532--&gt;v538\n    v515--&gt;v532\n    v546--&gt;v563\n    v568--&gt;v585\n    v109--&gt;v595\n    v595--&gt;v602\n    v610--&gt;v627\n    v627--&gt;v636\n    v636--&gt;v645\n    v705--&gt;v711\n    v645--&gt;v656\n    v678--&gt;v688\n    v661--&gt;v678\n    v688--&gt;v705\n    v711--&gt;v719\n    v1025--&gt;v1226\n    v752--&gt;v758\n    v735--&gt;v752\n    v783--&gt;v789\n    v766--&gt;v783\n    v797--&gt;v814\n    v814--&gt;v823\n    v828--&gt;v845\n    v876--&gt;v882\n    v859--&gt;v876\n    v882--&gt;v892\n    v917--&gt;v924\n    v900--&gt;v917\n    v882--&gt;v924\n    v950--&gt;v956\n    v933--&gt;v950\n    v964--&gt;v981\n    v986--&gt;v1003\n    v1008--&gt;v1025\n    v711--&gt;v1036\n    v1218--&gt;v1226\n    v1069--&gt;v1075\n    v1052--&gt;v1069\n    v1075--&gt;v1085\n    v1110--&gt;v1117\n    v1093--&gt;v1110\n    v1075--&gt;v1117\n    v1143--&gt;v1149\n    v1126--&gt;v1143\n    v1157--&gt;v1174\n    v1179--&gt;v1196\n    v1201--&gt;v1218\n    v711--&gt;v1228\n    v1228--&gt;v1236\n    v1244--&gt;v1261\n    v1261--&gt;v1271\n    v1301--&gt;v1307\n    v1284--&gt;v1301\n    v1332--&gt;v1338\n    v1315--&gt;v1332\n    v1346--&gt;v1363\n    v1368--&gt;v1385\n    v1390--&gt;v1394\n    v1424--&gt;v1430\n    v1407--&gt;v1424\n    v1455--&gt;v1461\n    v1438--&gt;v1455\n    v1469--&gt;v1486\n    v1491--&gt;v1508\n    v1513--&gt;v1517\n    v1547--&gt;v1553\n    v1530--&gt;v1547\n    v1578--&gt;v1584\n    v1561--&gt;v1578\n    v1592--&gt;v1609\n    v1614--&gt;v1631\n    v1645--&gt;v1662\n    v1667--&gt;v1684\n    v1689--&gt;v1715\n    style group_split_modalities_workflow fill:#F0F0F0,stroke:#969696;\n    style group_rna_singlesample fill:#F0F0F0,stroke:#969696;\n    style group_prot_singlesample fill:#F0F0F0,stroke:#969696;\n    style group_gdo_singlesample fill:#F0F0F0,stroke:#969696;\n    style group_process_batches fill:#F0F0F0,stroke:#969696;\n    style group_rna_multisample fill:#D9D9D9,stroke:#737373;\n    style group_prot_multisample fill:#D9D9D9,stroke:#737373;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v15 fill:#e3dcea,stroke:#7a4baa;\n    style v20 fill:#e3dcea,stroke:#7a4baa;\n    style v27 fill:#e3dcea,stroke:#7a4baa;\n    style v37 fill:#e3dcea,stroke:#7a4baa;\n    style v43 fill:#e3dcea,stroke:#7a4baa;\n    style v54 fill:#e3dcea,stroke:#7a4baa;\n    style v59 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v76 fill:#e3dcea,stroke:#7a4baa;\n    style v86 fill:#e3dcea,stroke:#7a4baa;\n    style v93 fill:#e3dcea,stroke:#7a4baa;\n    style v103 fill:#e3dcea,stroke:#7a4baa;\n    style v109 fill:#e3dcea,stroke:#7a4baa;\n    style v115 fill:#e3dcea,stroke:#7a4baa;\n    style v124 fill:#e3dcea,stroke:#7a4baa;\n    style v134 fill:#e3dcea,stroke:#7a4baa;\n    style v142 fill:#e3dcea,stroke:#7a4baa;\n    style v149 fill:#e3dcea,stroke:#7a4baa;\n    style v159 fill:#e3dcea,stroke:#7a4baa;\n    style v166 fill:#e3dcea,stroke:#7a4baa;\n    style v167 fill:#e3dcea,stroke:#7a4baa;\n    style v175 fill:#e3dcea,stroke:#7a4baa;\n    style v182 fill:#e3dcea,stroke:#7a4baa;\n    style v192 fill:#e3dcea,stroke:#7a4baa;\n    style v198 fill:#e3dcea,stroke:#7a4baa;\n    style v228 fill:#e3dcea,stroke:#7a4baa;\n    style v206 fill:#e3dcea,stroke:#7a4baa;\n    style v213 fill:#e3dcea,stroke:#7a4baa;\n    style v223 fill:#e3dcea,stroke:#7a4baa;\n    style v235 fill:#e3dcea,stroke:#7a4baa;\n    style v245 fill:#e3dcea,stroke:#7a4baa;\n    style v254 fill:#e3dcea,stroke:#7a4baa;\n    style v281 fill:#e3dcea,stroke:#7a4baa;\n    style v259 fill:#e3dcea,stroke:#7a4baa;\n    style v266 fill:#e3dcea,stroke:#7a4baa;\n    style v276 fill:#e3dcea,stroke:#7a4baa;\n    style v282 fill:#e3dcea,stroke:#7a4baa;\n    style v290 fill:#e3dcea,stroke:#7a4baa;\n    style v297 fill:#e3dcea,stroke:#7a4baa;\n    style v307 fill:#e3dcea,stroke:#7a4baa;\n    style v313 fill:#e3dcea,stroke:#7a4baa;\n    style v321 fill:#e3dcea,stroke:#7a4baa;\n    style v328 fill:#e3dcea,stroke:#7a4baa;\n    style v338 fill:#e3dcea,stroke:#7a4baa;\n    style v344 fill:#e3dcea,stroke:#7a4baa;\n    style v374 fill:#e3dcea,stroke:#7a4baa;\n    style v352 fill:#e3dcea,stroke:#7a4baa;\n    style v359 fill:#e3dcea,stroke:#7a4baa;\n    style v369 fill:#e3dcea,stroke:#7a4baa;\n    style v381 fill:#e3dcea,stroke:#7a4baa;\n    style v391 fill:#e3dcea,stroke:#7a4baa;\n    style v593 fill:#e3dcea,stroke:#7a4baa;\n    style v402 fill:#e3dcea,stroke:#7a4baa;\n    style v410 fill:#e3dcea,stroke:#7a4baa;\n    style v418 fill:#e3dcea,stroke:#7a4baa;\n    style v425 fill:#e3dcea,stroke:#7a4baa;\n    style v435 fill:#e3dcea,stroke:#7a4baa;\n    style v441 fill:#e3dcea,stroke:#7a4baa;\n    style v471 fill:#e3dcea,stroke:#7a4baa;\n    style v449 fill:#e3dcea,stroke:#7a4baa;\n    style v456 fill:#e3dcea,stroke:#7a4baa;\n    style v466 fill:#e3dcea,stroke:#7a4baa;\n    style v478 fill:#e3dcea,stroke:#7a4baa;\n    style v488 fill:#e3dcea,stroke:#7a4baa;\n    style v499 fill:#e3dcea,stroke:#7a4baa;\n    style v507 fill:#e3dcea,stroke:#7a4baa;\n    style v515 fill:#e3dcea,stroke:#7a4baa;\n    style v522 fill:#e3dcea,stroke:#7a4baa;\n    style v532 fill:#e3dcea,stroke:#7a4baa;\n    style v538 fill:#e3dcea,stroke:#7a4baa;\n    style v568 fill:#e3dcea,stroke:#7a4baa;\n    style v546 fill:#e3dcea,stroke:#7a4baa;\n    style v553 fill:#e3dcea,stroke:#7a4baa;\n    style v563 fill:#e3dcea,stroke:#7a4baa;\n    style v575 fill:#e3dcea,stroke:#7a4baa;\n    style v585 fill:#e3dcea,stroke:#7a4baa;\n    style v595 fill:#e3dcea,stroke:#7a4baa;\n    style v602 fill:#e3dcea,stroke:#7a4baa;\n    style v610 fill:#e3dcea,stroke:#7a4baa;\n    style v617 fill:#e3dcea,stroke:#7a4baa;\n    style v627 fill:#e3dcea,stroke:#7a4baa;\n    style v636 fill:#e3dcea,stroke:#7a4baa;\n    style v1689 fill:#e3dcea,stroke:#7a4baa;\n    style v645 fill:#e3dcea,stroke:#7a4baa;\n    style v656 fill:#e3dcea,stroke:#7a4baa;\n    style v661 fill:#e3dcea,stroke:#7a4baa;\n    style v668 fill:#e3dcea,stroke:#7a4baa;\n    style v678 fill:#e3dcea,stroke:#7a4baa;\n    style v688 fill:#e3dcea,stroke:#7a4baa;\n    style v695 fill:#e3dcea,stroke:#7a4baa;\n    style v705 fill:#e3dcea,stroke:#7a4baa;\n    style v711 fill:#e3dcea,stroke:#7a4baa;\n    style v719 fill:#e3dcea,stroke:#7a4baa;\n    style v727 fill:#e3dcea,stroke:#7a4baa;\n    style v735 fill:#e3dcea,stroke:#7a4baa;\n    style v742 fill:#e3dcea,stroke:#7a4baa;\n    style v752 fill:#e3dcea,stroke:#7a4baa;\n    style v758 fill:#e3dcea,stroke:#7a4baa;\n    style v766 fill:#e3dcea,stroke:#7a4baa;\n    style v773 fill:#e3dcea,stroke:#7a4baa;\n    style v783 fill:#e3dcea,stroke:#7a4baa;\n    style v789 fill:#e3dcea,stroke:#7a4baa;\n    style v797 fill:#e3dcea,stroke:#7a4baa;\n    style v804 fill:#e3dcea,stroke:#7a4baa;\n    style v814 fill:#e3dcea,stroke:#7a4baa;\n    style v823 fill:#e3dcea,stroke:#7a4baa;\n    style v850 fill:#e3dcea,stroke:#7a4baa;\n    style v828 fill:#e3dcea,stroke:#7a4baa;\n    style v835 fill:#e3dcea,stroke:#7a4baa;\n    style v845 fill:#e3dcea,stroke:#7a4baa;\n    style v851 fill:#e3dcea,stroke:#7a4baa;\n    style v859 fill:#e3dcea,stroke:#7a4baa;\n    style v866 fill:#e3dcea,stroke:#7a4baa;\n    style v876 fill:#e3dcea,stroke:#7a4baa;\n    style v882 fill:#e3dcea,stroke:#7a4baa;\n    style v1008 fill:#e3dcea,stroke:#7a4baa;\n    style v892 fill:#e3dcea,stroke:#7a4baa;\n    style v900 fill:#e3dcea,stroke:#7a4baa;\n    style v907 fill:#e3dcea,stroke:#7a4baa;\n    style v917 fill:#e3dcea,stroke:#7a4baa;\n    style v924 fill:#e3dcea,stroke:#7a4baa;\n    style v925 fill:#e3dcea,stroke:#7a4baa;\n    style v933 fill:#e3dcea,stroke:#7a4baa;\n    style v940 fill:#e3dcea,stroke:#7a4baa;\n    style v950 fill:#e3dcea,stroke:#7a4baa;\n    style v956 fill:#e3dcea,stroke:#7a4baa;\n    style v986 fill:#e3dcea,stroke:#7a4baa;\n    style v964 fill:#e3dcea,stroke:#7a4baa;\n    style v971 fill:#e3dcea,stroke:#7a4baa;\n    style v981 fill:#e3dcea,stroke:#7a4baa;\n    style v993 fill:#e3dcea,stroke:#7a4baa;\n    style v1003 fill:#e3dcea,stroke:#7a4baa;\n    style v1015 fill:#e3dcea,stroke:#7a4baa;\n    style v1025 fill:#e3dcea,stroke:#7a4baa;\n    style v1226 fill:#e3dcea,stroke:#7a4baa;\n    style v1036 fill:#e3dcea,stroke:#7a4baa;\n    style v1044 fill:#e3dcea,stroke:#7a4baa;\n    style v1052 fill:#e3dcea,stroke:#7a4baa;\n    style v1059 fill:#e3dcea,stroke:#7a4baa;\n    style v1069 fill:#e3dcea,stroke:#7a4baa;\n    style v1075 fill:#e3dcea,stroke:#7a4baa;\n    style v1201 fill:#e3dcea,stroke:#7a4baa;\n    style v1085 fill:#e3dcea,stroke:#7a4baa;\n    style v1093 fill:#e3dcea,stroke:#7a4baa;\n    style v1100 fill:#e3dcea,stroke:#7a4baa;\n    style v1110 fill:#e3dcea,stroke:#7a4baa;\n    style v1117 fill:#e3dcea,stroke:#7a4baa;\n    style v1118 fill:#e3dcea,stroke:#7a4baa;\n    style v1126 fill:#e3dcea,stroke:#7a4baa;\n    style v1133 fill:#e3dcea,stroke:#7a4baa;\n    style v1143 fill:#e3dcea,stroke:#7a4baa;\n    style v1149 fill:#e3dcea,stroke:#7a4baa;\n    style v1179 fill:#e3dcea,stroke:#7a4baa;\n    style v1157 fill:#e3dcea,stroke:#7a4baa;\n    style v1164 fill:#e3dcea,stroke:#7a4baa;\n    style v1174 fill:#e3dcea,stroke:#7a4baa;\n    style v1186 fill:#e3dcea,stroke:#7a4baa;\n    style v1196 fill:#e3dcea,stroke:#7a4baa;\n    style v1208 fill:#e3dcea,stroke:#7a4baa;\n    style v1218 fill:#e3dcea,stroke:#7a4baa;\n    style v1228 fill:#e3dcea,stroke:#7a4baa;\n    style v1236 fill:#e3dcea,stroke:#7a4baa;\n    style v1244 fill:#e3dcea,stroke:#7a4baa;\n    style v1251 fill:#e3dcea,stroke:#7a4baa;\n    style v1261 fill:#e3dcea,stroke:#7a4baa;\n    style v1271 fill:#e3dcea,stroke:#7a4baa;\n    style v1390 fill:#e3dcea,stroke:#7a4baa;\n    style v1276 fill:#e3dcea,stroke:#7a4baa;\n    style v1284 fill:#e3dcea,stroke:#7a4baa;\n    style v1291 fill:#e3dcea,stroke:#7a4baa;\n    style v1301 fill:#e3dcea,stroke:#7a4baa;\n    style v1307 fill:#e3dcea,stroke:#7a4baa;\n    style v1315 fill:#e3dcea,stroke:#7a4baa;\n    style v1322 fill:#e3dcea,stroke:#7a4baa;\n    style v1332 fill:#e3dcea,stroke:#7a4baa;\n    style v1338 fill:#e3dcea,stroke:#7a4baa;\n    style v1368 fill:#e3dcea,stroke:#7a4baa;\n    style v1346 fill:#e3dcea,stroke:#7a4baa;\n    style v1353 fill:#e3dcea,stroke:#7a4baa;\n    style v1363 fill:#e3dcea,stroke:#7a4baa;\n    style v1375 fill:#e3dcea,stroke:#7a4baa;\n    style v1385 fill:#e3dcea,stroke:#7a4baa;\n    style v1394 fill:#e3dcea,stroke:#7a4baa;\n    style v1513 fill:#e3dcea,stroke:#7a4baa;\n    style v1399 fill:#e3dcea,stroke:#7a4baa;\n    style v1407 fill:#e3dcea,stroke:#7a4baa;\n    style v1414 fill:#e3dcea,stroke:#7a4baa;\n    style v1424 fill:#e3dcea,stroke:#7a4baa;\n    style v1430 fill:#e3dcea,stroke:#7a4baa;\n    style v1438 fill:#e3dcea,stroke:#7a4baa;\n    style v1445 fill:#e3dcea,stroke:#7a4baa;\n    style v1455 fill:#e3dcea,stroke:#7a4baa;\n    style v1461 fill:#e3dcea,stroke:#7a4baa;\n    style v1491 fill:#e3dcea,stroke:#7a4baa;\n    style v1469 fill:#e3dcea,stroke:#7a4baa;\n    style v1476 fill:#e3dcea,stroke:#7a4baa;\n    style v1486 fill:#e3dcea,stroke:#7a4baa;\n    style v1498 fill:#e3dcea,stroke:#7a4baa;\n    style v1508 fill:#e3dcea,stroke:#7a4baa;\n    style v1517 fill:#e3dcea,stroke:#7a4baa;\n    style v1636 fill:#e3dcea,stroke:#7a4baa;\n    style v1522 fill:#e3dcea,stroke:#7a4baa;\n    style v1530 fill:#e3dcea,stroke:#7a4baa;\n    style v1537 fill:#e3dcea,stroke:#7a4baa;\n    style v1547 fill:#e3dcea,stroke:#7a4baa;\n    style v1553 fill:#e3dcea,stroke:#7a4baa;\n    style v1561 fill:#e3dcea,stroke:#7a4baa;\n    style v1568 fill:#e3dcea,stroke:#7a4baa;\n    style v1578 fill:#e3dcea,stroke:#7a4baa;\n    style v1584 fill:#e3dcea,stroke:#7a4baa;\n    style v1614 fill:#e3dcea,stroke:#7a4baa;\n    style v1592 fill:#e3dcea,stroke:#7a4baa;\n    style v1599 fill:#e3dcea,stroke:#7a4baa;\n    style v1609 fill:#e3dcea,stroke:#7a4baa;\n    style v1621 fill:#e3dcea,stroke:#7a4baa;\n    style v1631 fill:#e3dcea,stroke:#7a4baa;\n    style v1637 fill:#e3dcea,stroke:#7a4baa;\n    style v1667 fill:#e3dcea,stroke:#7a4baa;\n    style v1645 fill:#e3dcea,stroke:#7a4baa;\n    style v1652 fill:#e3dcea,stroke:#7a4baa;\n    style v1662 fill:#e3dcea,stroke:#7a4baa;\n    style v1674 fill:#e3dcea,stroke:#7a4baa;\n    style v1684 fill:#e3dcea,stroke:#7a4baa;\n    style v1696 fill:#e3dcea,stroke:#7a4baa;\n    style v1703 fill:#e3dcea,stroke:#7a4baa;\n    style v1715 fill:#e3dcea,stroke:#7a4baa;\n    style v1722 fill:#e3dcea,stroke:#7a4baa;\n    style v1726 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process samples"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html",
    "href": "components/workflows/rna/rna_multisample.html",
    "title": "Rna multisample",
    "section": "",
    "text": "ID: rna_multisample\nNamespace: workflows/rna\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html#example-commands",
    "href": "components/workflows/rna/rna_multisample.html#example-commands",
    "title": "Rna multisample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/rna/rna_multisample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"concatenated\"\ninput: # please fill in - example: \"dataset.h5mu\"\nmodality: \"rna\"\n# layer: \"foo\"\n\n# Output\n# output: \"$id.$key.output.h5mu\"\n\n# Filtering highly variable features\nhighly_variable_features_var_output: \"filter_with_hvg\"\nhighly_variable_features_obs_batch_key: \"sample_id\"\nhighly_variable_features_flavor: \"seurat\"\n# highly_variable_features_n_top_features: 123\n\n# QC metrics calculation options\nvar_qc_metrics: [\"filter_with_hvg\"]\ntop_n_vars: [50, 100, 200, 500]\noutput_obs_num_nonzero_vars: \"num_nonzero_vars\"\noutput_obs_total_counts_vars: \"total_counts\"\noutput_var_num_nonzero_obs: \"num_nonzero_obs\"\noutput_var_total_counts_obs: \"total_counts\"\noutput_var_obs_mean: \"obs_mean\"\noutput_var_pct_dropout: \"pct_dropout\"\n\n# RNA Scaling options\nenable_scaling: false\nscaling_output_layer: \"scaled\"\n# scaling_max_value: 123.0\nscaling_zero_center: true\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/rna/rna_multisample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html#argument-groups",
    "href": "components/workflows/rna/rna_multisample.html#argument-groups",
    "title": "Rna multisample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the concatenated file\nstring, required, example: \"concatenated\"\n\n\n--input\nPath to the samples.\nfile, required, example: \"dataset.h5mu\"\n\n\n--modality\nModality to process.\nstring, default: \"rna\"\n\n\n--layer\nInput layer to use. If not specified, .X is used.\nstring\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nFiltering highly variable features\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--highly_variable_features_var_output\nIn which .var slot to store a boolean array corresponding to the highly variable features.\nstring, default: \"filter_with_hvg\"\n\n\n--highly_variable_features_obs_batch_key\nIf specified, highly-variable features are selected within each batch separately and merged. This simple process avoids the selection of batch-specific features and acts as a lightweight batch correction method. For all flavors, featues are first sorted by how many batches they are highly variable. For dispersion-based flavors ties are broken by normalized dispersion. If flavor = ‘seurat_v3’, ties are broken by the median (across batches) rank based on within-batch normalized variance.\nstring, default: \"sample_id\"\n\n\n--highly_variable_features_flavor\nChoose the flavor for identifying highly variable features. For the dispersion based methods in their default workflows, Seurat passes the cutoffs whereas Cell Ranger passes n_top_features.\nstring, default: \"seurat\"\n\n\n--highly_variable_features_n_top_features\nNumber of highly-variable features to keep. Mandatory if filter_with_hvg_flavor is set to ‘seurat_v3’.\ninteger\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes.\nList of string, default: \"filter_with_hvg\", example: \"ercc,highly_variable\", multiple_sep: \",\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \",\"\n\n\n--output_obs_num_nonzero_vars\nName of column in .obs describing, for each observation, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each row the number of columns that contain data.\nstring, default: \"num_nonzero_vars\"\n\n\n--output_obs_total_counts_vars\nName of the column for .obs describing, for each observation (row), the sum of the stored values in the columns.\nstring, default: \"total_counts\"\n\n\n--output_var_num_nonzero_obs\nName of column describing, for each feature, the number of stored values (including explicit zeroes). In other words, the name of the column that counts for each column the number of rows that contain data.\nstring, default: \"num_nonzero_obs\"\n\n\n--output_var_total_counts_obs\nName of the column in .var describing, for each feature (column), the sum of the stored values in the rows.\nstring, default: \"total_counts\"\n\n\n--output_var_obs_mean\nName of the column in .obs providing the mean of the values in each row.\nstring, default: \"obs_mean\"\n\n\n--output_var_pct_dropout\nName of the column in .obs providing for each feature the percentage of observations the feature does not appear on (i.e. is missing). Same as --num_nonzero_obs but percentage based.\nstring, default: \"pct_dropout\"\n\n\n\n\n\nRNA Scaling options\nOptions for enabling scaling of the log-normalized data to unit variance and zero mean. The scaled data will be output a different layer and representation with reduced dimensions will be created and stored in addition to the non-scaled data.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--enable_scaling\nEnable scaling for the RNA modality.\nboolean_true\n\n\n--scaling_output_layer\nOutput layer where the scaled log-normalized data will be stored.\nstring, default: \"scaled\"\n\n\n--scaling_max_value\nClip (truncate) data to this value after scaling. If not specified, do not clip.\ndouble\n\n\n--scaling_zero_center\nIf set, omit zero-centering variables, which allows to handle sparse input efficiently.”\nboolean_false",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html#authors",
    "href": "components/workflows/rna/rna_multisample.html#authors",
    "title": "Rna multisample",
    "section": "Authors",
    "text": "Authors\n\nDries De Maeyer   (author)\nRobrecht Cannoodt    (author, maintainer)\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_multisample.html#visualisation",
    "href": "components/workflows/rna/rna_multisample.html#visualisation",
    "title": "Rna multisample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(normalize_total)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v49(log1p)\n    v56(cross)\n    v66(cross)\n    v72(filter)\n    v80(delete_layer)\n    v87(cross)\n    v97(cross)\n    v106(branch)\n    v133(concat)\n    v111(scale)\n    v118(cross)\n    v128(cross)\n    v134(filter)\n    v142(highly_variable_features_scanpy)\n    v149(cross)\n    v159(cross)\n    v165(filter)\n    v291(concat)\n    v175(filter)\n    v190(cross)\n    v200(cross)\n    v207(mix)\n    v208(filter)\n    v223(cross)\n    v233(cross)\n    v239(filter)\n    v269(concat)\n    v254(cross)\n    v264(cross)\n    v276(cross)\n    v286(cross)\n    v298(cross)\n    v305(cross)\n    v317(cross)\n    v324(cross)\n    v328(Output)\n    subgraph group_rna_qc [rna_qc]\n        v183(grep_annotation_column)\n        v216(calculate_qc_metrics)\n        v247(publish)\n    end\n    v106--&gt;v133\n    v133--&gt;v134\n    v207--&gt;v208\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v72--&gt;v80\n    v80--&gt;v87\n    v72--&gt;v87\n    v72--&gt;v97\n    v106--&gt;v111\n    v111--&gt;v118\n    v106--&gt;v118\n    v106--&gt;v128\n    v128--&gt;v133\n    v134--&gt;v142\n    v142--&gt;v149\n    v134--&gt;v149\n    v134--&gt;v159\n    v175--&gt;v183\n    v183--&gt;v190\n    v175--&gt;v190\n    v175--&gt;v200\n    v208--&gt;v216\n    v216--&gt;v223\n    v208--&gt;v223\n    v208--&gt;v233\n    v239--&gt;v247\n    v247--&gt;v254\n    v239--&gt;v254\n    v239--&gt;v264\n    v264--&gt;v269\n    v269--&gt;v276\n    v165--&gt;v276\n    v165--&gt;v286\n    v286--&gt;v291\n    v291--&gt;v298\n    v2--&gt;v298\n    v298--&gt;v305\n    v2--&gt;v305\n    v2--&gt;v317\n    v317--&gt;v324\n    v2--&gt;v324\n    v324--&gt;v328\n    v35--&gt;v41\n    v18--&gt;v35\n    v66--&gt;v72\n    v49--&gt;v66\n    v80--&gt;v97\n    v97--&gt;v106\n    v111--&gt;v128\n    v159--&gt;v165\n    v142--&gt;v159\n    v165--&gt;v175\n    v200--&gt;v207\n    v183--&gt;v200\n    v165--&gt;v207\n    v233--&gt;v239\n    v216--&gt;v233\n    v247--&gt;v264\n    v269--&gt;v286\n    v291--&gt;v317\n    style group_rna_qc fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v72 fill:#e3dcea,stroke:#7a4baa;\n    style v80 fill:#e3dcea,stroke:#7a4baa;\n    style v87 fill:#e3dcea,stroke:#7a4baa;\n    style v97 fill:#e3dcea,stroke:#7a4baa;\n    style v106 fill:#e3dcea,stroke:#7a4baa;\n    style v133 fill:#e3dcea,stroke:#7a4baa;\n    style v111 fill:#e3dcea,stroke:#7a4baa;\n    style v118 fill:#e3dcea,stroke:#7a4baa;\n    style v128 fill:#e3dcea,stroke:#7a4baa;\n    style v134 fill:#e3dcea,stroke:#7a4baa;\n    style v142 fill:#e3dcea,stroke:#7a4baa;\n    style v149 fill:#e3dcea,stroke:#7a4baa;\n    style v159 fill:#e3dcea,stroke:#7a4baa;\n    style v165 fill:#e3dcea,stroke:#7a4baa;\n    style v291 fill:#e3dcea,stroke:#7a4baa;\n    style v175 fill:#e3dcea,stroke:#7a4baa;\n    style v183 fill:#e3dcea,stroke:#7a4baa;\n    style v190 fill:#e3dcea,stroke:#7a4baa;\n    style v200 fill:#e3dcea,stroke:#7a4baa;\n    style v207 fill:#e3dcea,stroke:#7a4baa;\n    style v208 fill:#e3dcea,stroke:#7a4baa;\n    style v216 fill:#e3dcea,stroke:#7a4baa;\n    style v223 fill:#e3dcea,stroke:#7a4baa;\n    style v233 fill:#e3dcea,stroke:#7a4baa;\n    style v239 fill:#e3dcea,stroke:#7a4baa;\n    style v269 fill:#e3dcea,stroke:#7a4baa;\n    style v247 fill:#e3dcea,stroke:#7a4baa;\n    style v254 fill:#e3dcea,stroke:#7a4baa;\n    style v264 fill:#e3dcea,stroke:#7a4baa;\n    style v276 fill:#e3dcea,stroke:#7a4baa;\n    style v286 fill:#e3dcea,stroke:#7a4baa;\n    style v298 fill:#e3dcea,stroke:#7a4baa;\n    style v305 fill:#e3dcea,stroke:#7a4baa;\n    style v317 fill:#e3dcea,stroke:#7a4baa;\n    style v324 fill:#e3dcea,stroke:#7a4baa;\n    style v328 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna multisample"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenPipelines",
    "section": "",
    "text": "Reusable components built with Viash for flexible pipeline construction.\n\n\n\n\nLeverage Nextflow for execution on various platforms (local, cloud, HPC).\n\n\n\n\nContainerized components and unit testing ensure reliable analyses.\n\n\n\n\n\n\nA framework for sharing and integrating components to foster teamwork.\n\n\n\n\nAdapts to evolving single-cell analysis by incorporating current best practices and novel methods.\n\n\n\n\nSupports diverse single-cell data types (RNA, protein, VDJ, ATAC) for multi-omics analyses."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "OpenPipelines",
    "section": "",
    "text": "Reusable components built with Viash for flexible pipeline construction.\n\n\n\n\nLeverage Nextflow for execution on various platforms (local, cloud, HPC).\n\n\n\n\nContainerized components and unit testing ensure reliable analyses.\n\n\n\n\n\n\nA framework for sharing and integrating components to foster teamwork.\n\n\n\n\nAdapts to evolving single-cell analysis by incorporating current best practices and novel methods.\n\n\n\n\nSupports diverse single-cell data types (RNA, protein, VDJ, ATAC) for multi-omics analyses."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "OpenPipelines",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\nflowchart LR\n  %% Ingestion\n  subgraph ingestion[Step 1: Ingestion]\n    direction LR\n    10x_ingestion[10x Ingestion]:::subwf\n    bd_ingestion[BD Rhapsody\\nIngestion]:::subwf\n    own_h5mu[Own H5MU]:::subwf\n  end\n  ingestion:::info\n\n  %% Process samples\n  subgraph process_samples[Step 2: Process Samples]\n    direction LR\n    gex[GEX]:::subwf\n    atac[ATAC]:::subwf\n    adt[ADT]:::subwf\n    vdj[VDJ]:::subwf\n    other[Other]:::subwf\n  end\n  process_samples:::info\n\n  %% Integration and downstream\n  subgraph integration[Step 3: Integration]\n    direction LR\n    harmony[Harmony]:::subwf\n    scvi[scVI]:::subwf\n    scanvi[scanVI]:::subwf\n    etc[...]:::subwf\n  end\n  integration[Integration]:::info\n  \n  subgraph downstream[Step 4: Downstream]\n    direction LR\n    celltype_annotation[Cell Type\\nAnnotation]:::subwf\n    markergenes[Marker Genes\\nAnalysis]:::subwf\n    differential[Differential\\nExpression]:::subwf\n    gene_signature_analysis[Gene Signature\\nAnalysis]:::subwf\n    ccc[Cell-Cell\\nCommunication]:::subwf\n  end\n\n  ingestion --&gt; process_samples --&gt; integration --&gt; downstream\n\n  classDef wf fill:#f0f0f0,stroke:#525252\n  classDef subwf fill:#d9d9d9,stroke:#525252\n  classDef info fill:#f0f0f0,stroke:#525252,stroke-dasharray: 4 4"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "OpenPipelines",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\n\n\n  \n  Fundamentals\n  \n    \n  \n    \n        \n  \n  Philosophy\n  \n    \n    \n        \n  \n  Concepts\n  \n    \n    \n        \n  \n  Architecture\n  \n    \n    \n        \n  \n  Roadmap\n  \n    \n    \n  \n    \n\n\nNo matching items\n\n\n\n\n\n\n  \n  User guide\n  \n    \n  \n    \n        \n  \n  Getting started\n  \n    \n    \n        \n  \n  Running pipelines\n  \n    \n    \n        \n  \n  Parameter lists\n  \n    \n    \n        \n  \n  Ingestion\n  \n    \n    \n        \n  \n  Processing\n  \n    \n    \n        \n  \n  Downstream\n  \n    \n    \n        \n  \n  Bug reports\n  \n    \n    \n  \n    \n\n\nNo matching items\n\n\n\n\n\n\n  \n  Contributing\n  \n    \n  \n    \n        \n  \n  Getting started\n  \n    \n    \n        \n  \n  Project structure\n  \n    \n    \n        \n  \n  Creating components\n  \n    \n    \n        \n  \n  Creating pipelines\n  \n    \n    \n        \n  \n  Running tests\n  \n    \n    \n        \n  \n  Publishing your changes\n  \n    \n    \n  \n    \n\n\nNo matching items\n\n\n\n\n\n\n  \n  More information\n  \n    \n  \n    \n        \n  \n  Cheat sheets\n  \n    \n    \n        \n  \n  Code of conduct\n  \n    \n    \n        \n  \n  FAQ\n  \n    \n    \n  \n    \n\n\nNo matching items"
  },
  {
    "objectID": "fundamentals/concepts.html",
    "href": "fundamentals/concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Goals\nOpenPipelines strives to provide easy ways to interact with the pipeline and/or codebase for three types of users:\n\nPipeline executor: runs the pipeline from a GUI side\nPipeline editor: adapts pipelines with existing components for specific projects\nComponent developer: develops novel components and or pipelines\n\nThis means that openpipelines must be:\n\nUsable by non-experts\nEasy to deploy\nProvide reproducable results\nScalable\nEasy to maintain and adapt\n\n\n\nRequirements\nTo meet these demands, the following concepts have been implemented at the core of Openpipeline:\n\n🌍 A language independent framework\n💾 A versitile storage solution\n🔳 Modularity\n🔀 A best-practice pipeline layout\n⌛ Versioning\n✅ Automatic testing\n💬 Community input\n📺 A graphical interace\n\n\n\nA common file format: AnnData and MuData 💾\nOne of the core principals of OpenPipelines is to use MuData as a common data format throughout the whole pipeline. This means that the input and output for most components and workflows will be a MuData file and converters from and to other common data formats are provided to improve compatibility with up-and downstream applications. Choosing a common data format greatly diminishes the development complexity because it facilitates interfacing between different tools in a pipeline without needing to convert multiple times.\nMuData is a format to store annotated multimodal data. It is derived from the AnnData format. If you are unfamiliar with AnnData or MuData, it is recommended to read up on AnnData first as it is the unimodal counterpart of MuData. MuData can be roughly described as collection of several AnnData objects (stored as a associative array in the .mod attribute). MuData provides a hierarchical way to store the data:\nMuData\n├─ .mod\n│  ├─ modality_1 (AnnData Object)\n│     ├─ .X\n│     ├─ .layers\n│         ├─ layer_1 \n│         ├─ ...\n│     ├─ .var\n│     ├─ .obs\n│     ├─ .obsm\n│     ├─ .varm\n│     ├─ .uns\n│  ├─ modality_2 (AnnData Object)\n├─ .var\n├─ .obs\n├─ .obms\n├─ .varm\n├─ .uns\n\n.mod: an associative array of AnnData objects. Used in OpenPipelines to store the different modalities (CITE-seq, RNA abundance, …)\n.X and .layers: matrices storing the measurements with the columns being the variables measured and the rows being the observations (cells in most cases).\n.var: metadata for the variables (i.e. annotation for the columns of .X or any matrix in .layers). The number of rows in the .var datafame (or the length of each entry in the dictionairy) is equal to the number of columns in the measurement matrices.\n.obs: metadata for the observations (i.e. annotation for the rows of .X or any matrix in .layers). The number of rows in the .obs datafame (or the length of each entry in the dictionairy) is equal to the number of rows in the measurement matrices.\nvarm: the multi-dimensional variable annotation. A key-dataframe mapping where the number of rows in each dataframe is equal to the number of columns in the measurement matrices.\nobsm: the multi-dimensional observation annotation. A key-dataframe mapping where the number of rows in each dataframe is equal to the number of rows in the measurement matrices.\n.uns: A mapping where no restrictions are enforced on the dimensions of the data.\n\n\n\nModularity and a language independent framework 🔳\nTODO\n\n\nA graphical interface 📺\nTODO",
    "crumbs": [
      "Fundamentals",
      "Concepts"
    ]
  },
  {
    "objectID": "fundamentals/index.html",
    "href": "fundamentals/index.html",
    "title": "Fundamentals",
    "section": "",
    "text": "Philosophy: Our approach and mission\n  \n  \n  \n    Concepts: The core concepts behind this project\n  \n  \n  \n    Architecture: Structure of the project\n  \n  \n  \n    Roadmap: Development roadmap\n  \n  \n\n\nNo matching items",
    "crumbs": [
      "Fundamentals"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html",
    "href": "fundamentals/architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "OpenPipeline is a pipeline for the processing of multimodal single-cell data that scales to a great many of samples. Covering the architecture requires us to explain many angles, including: what the expected inputs and outputs are for each workflow are, how do the workflows relate to each other, and what the state of the data is at each step of the pipeline. Here is an overview of the general steps involved in processing sequencing data into a single integrated object. We will discuss each of the steps further below.\nflowchart TD  \n  ingest[\"Ingestion\"] --&gt; split --&gt; unimodalsinglesample[\"Unimodal Single Sample Processing\"] --&gt; concat --&gt; unimodalmultisample[\"Unimodal Multi Sample Processing\"] --&gt; merging --&gt; integation_setup[\"Integration Setup\"] --&gt; integration[\"Integration\"]  --&gt; downstreamprocessing[\"Downstream Processing\"]\n\n\n\n\nFigure 1: Overview of the steps included in OpenPipeline for the analysis of single cell multiomics data.\nflowchart TB\n  subgraph ingestion\n    direction TB\n    subgraph cellranger_multi\n      direction TB\n      mapping_10x --&gt; convert_to_h5mu_10x\n      mapping_10x[mapping]\n      convert_to_h5mu_10x[convert_to_h5mu]\n    end\n    subgraph bdrhap_v1\n      direction TB\n      mapping_bd1 --&gt; convert_to_h5mu_bd1\n      mapping_bd1[mapping]\n      convert_to_h5mu_bd1[convert_to_h5mu]\n    end\n    subgraph bdrhap_v2\n      direction TB\n      mapping_bd2 --&gt; convert_to_h5mu_bd2\n      mapping_bd2[mapping]\n      convert_to_h5mu_bd2[convert_to_h5mu]\n    end\n    cellranger_multi:::subwf\n    bdrhap_v1:::subwf\n    bdrhap_v2:::subwf\n  end\n  ingestion:::wf\n  subgraph process_samples\n    split_modalities --&gt; rna_singlesample & prot_singlesample & gdo_singlesample & atac_singlesample & other_modalities --&gt; concat --&gt; process_batches\n    split_modalities\n    rna_singlesample\n    prot_singlesample\n    gdo_singlesample\n    atac_singlesample\n    other_modalities\n    concat\n    subgraph process_batches\n      direction LR\n      split_modalities2 --&gt; rna_multisample & prot_multisample & atac_multisample & other_modalities2 --&gt; merge\n      split_modalities2[split_modalities]\n      other_modalities2[other_modalities]\n      rna_multisample\n      prot_multisample\n      merge\n    end\n    process_batches:::subwf\n    atac_multisample\n  end\n  process_samples:::wf\n  raw_counts --- ingestion --&gt; raw_h5mu\n  raw_h5mu --- process_samples --&gt; processed_h5mu\n  subgraph integration\n    direction LR\n    integration_method --&gt; find_neighbors --&gt; leiden --&gt; umap\n    integration_method -.- intmeth\n    integration_method\n    find_neighbors\n    leiden\n    umap\n    subgraph intmeth [integration_method]\n      bbknn\n      harmony\n      scanorama\n      scvi\n      totalvi\n      scgpt_integration\n    end\n    intmeth:::info\n  end\n  integration:::wf\n  processed_h5mu --- integration ---&gt; integrated_h5mu\n  subgraph celltype_annotation\n    direction TB\n    integration_method2[integration_method]\n    celltypist\n    scanvi\n    scgpt_annotation\n    onclass\n    svm\n    randomforest\n    pynndescent_knn\n    consensus_voting\n    integration_method2 --&gt; pynndescent_knn --&gt; consensus_voting\n    celltypist & scanvi & scgpt_annotation & onclass & svm & randomforest --&gt; consensus_voting\n  end\n  reference_atlas --&gt; celltype_annotation\n  celltype_annotation:::wf\n  integrated_h5mu --- celltype_annotation --&gt; annotated_h5mu\n\n  classDef wf fill:#f0f0f0,stroke:#525252\n  classDef subwf fill:#d9d9d9,stroke:#525252\n  classDef info fill:#f0f0f0,stroke:#525252,stroke-dasharray: 4 4",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#ingestion-workflows",
    "href": "fundamentals/architecture.html#ingestion-workflows",
    "title": "Architecture",
    "section": "Ingestion workflows",
    "text": "Ingestion workflows\nAll of the following workflows from the ingestion namespace have been discussed in more detail in the ingestion section:\n\ningestion/bd_rhapsody\ningestion/cellranger_mapping\ningestion/cellranger_multi\ningestion/demux\ningestion/make_reference",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#multiomics-workflows",
    "href": "fundamentals/architecture.html#multiomics-workflows",
    "title": "Architecture",
    "section": "Multiomics workflows",
    "text": "Multiomics workflows\nThere exists no singlesample workflow. However, the prot_singlesample and rna_singlesample pipelines do exist and they map identically to the functionality described in the single-sample antibody capture processing and single-sample gene expression processing sections respectively. If you would like to process your samples as described in the unimodal single sample processing section, you can execute both workflows in tandem for the two modalities.\nContrary to the workflows for single sample processing, there exists a multiomics/multisample workflow. However this workflow is not just the multiomics/prot_multisample and multiomics/rna_multisample workflows that have been combined. Instead, it combines the multiomics/prot_multisample, multiomics/rna_multisample and multiomics/integration/initialize_integration workflows. The purpose of this pipeline is to provide an extra ‘entrypoint’ into the full pipeline that skips the singlesample processing, allowing reprocessing samples that have already been processed before. A popular usecase is to manually select one or more celltypes which need to be processed again or the integration of observations from multiple experiments into a single dataset. Keep in mind that concatenation is not included in the multisample pipeline, so when multiple input files are specified they are processed in parallel. If you would like to integrate multiple experiments, you need to first concatenate them in a seperate step:",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#the-full-pipeline",
    "href": "fundamentals/architecture.html#the-full-pipeline",
    "title": "Architecture",
    "section": "The “full” pipeline",
    "text": "The “full” pipeline\nThe name of this pipeline is a bit of a misnomer, because it does not include all the steps from ingestion to integration. As will be discussed in the ingestion section, which ingestion strategy you need is dependant on your technology provider and the chosen platform. For integration, there exist many methods and combination of methods, and you may wish to choose which integration methods are applicable for your usecase. As a consequence, these two stages in the analysis of single-cell need to be executed seperatly and not as part of a single unified pipeline. All other steps outlined below on the other hand are included into the “full” pipeline, which can therefore be summarized in the following figure:\n\n\n\n\n\n\nflowchart TD  \n  split --&gt; unimodalsinglesample[\"Unimodal Single Sample Processing\"] --&gt; concat --&gt; unimodalmultisample[\"Unimodal Multi Sample Processing\"] --&gt; merging --&gt; integation_setup\n\n\n\n\nFigure 2: Overview of the steps included in the full pipelines from OpenPipeline.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#integration-workflows",
    "href": "fundamentals/architecture.html#integration-workflows",
    "title": "Architecture",
    "section": "Integration workflows",
    "text": "Integration workflows\nFor each of the integration methods (and their optional combination with other tools), a seperate pipeline is defined. More information for each of the pipelines is available in the integration methods section.\n\nmultiomics/integration/bbknn_leiden\nmultiomics/integration/harmony_leiden\nmultiomics/integration/scanorama_leiden\nmultiomics/integration/scvi_leiden\nmultiomics/integration/totalvi_leiden\nmultiomics/integration/initialize_integration",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-splitting",
    "href": "fundamentals/architecture.html#sec-splitting",
    "title": "Architecture",
    "section": "Splitting modalities",
    "text": "Splitting modalities\nWe refer to splitting modalities when multimodal MuData file is split into several unimodal MuData files. The number of output files is equal to the number of modalities present in the input file. Splitting the modalities works on MuData files containing data for multiple samples or for single-sample files.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-merging",
    "href": "fundamentals/architecture.html#sec-merging",
    "title": "Architecture",
    "section": "Merging of modalities",
    "text": "Merging of modalities\nMerging refers to combining multiple files with data for one modality into a single output file that contains all input modalities. It is the inverse operation of splitting the modalities.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#concatenation-of-samples",
    "href": "fundamentals/architecture.html#concatenation-of-samples",
    "title": "Architecture",
    "section": "Concatenation of samples",
    "text": "Concatenation of samples\nJoining of observations for different samples, stored in their respective MuData file, into a single MuData file for all samples together is called sample concatenation. In practice, this operation is performed for each modality separately. An extra column (with default name sample_id) is added to the annotation of the observations (.obs) to indicate where each observation originated from.\n\nSpecial care must be taken when considering annotations for observations and features while concatenating the samples. Indeed, the data from different samples can contain conflicting information. Openpipeline’s concat component provides an argument other_axis_mode that allows a user to specify what happens when conflicting information is found. The move option for this argument is the default behavior. In this mode, each annotation column (from .obs and .var) is compared across samples. When no conflicts are found or the column is unique for a sample, the column is added output object. When a conflict does occur, all of the columns are gathered from the samples and stored into a dataframe. This dataframe is then stored into .obsm for annotations for the observations and .varm for feature annotations. This way, a user can have a look at the conflicts and decide what to do with them.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#creating-a-transcriptomics-reference",
    "href": "fundamentals/architecture.html#creating-a-transcriptomics-reference",
    "title": "Architecture",
    "section": "Creating a transcriptomics reference",
    "text": "Creating a transcriptomics reference\nMapping reads from the FASTQ files to features requires a reference that needs to be provided to the mapping component. Depending on the usecase, you might even need to provide references specific for the modalities that you are trying to analyze. For gene expression data, the reference is a reference genome, together with its appropriate gene annotation. A genome reference is often indexed in order to improve the mapping speed. Additionally, some mapping frameworks provided by the single-cell technology providers require extra preprocessing of the reference before they can be used with their worklow. OpenPipelines provides a make_reference that allows you to create references in many formats which can be used to map your reads to.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-single-sample-gex",
    "href": "fundamentals/architecture.html#sec-single-sample-gex",
    "title": "Architecture",
    "section": "Single-sample Gene Expression Processing",
    "text": "Single-sample Gene Expression Processing\nSingle-sample gene expression processing involves two steps: removing cells based on count statistics and flagging observations originating from doublets.\nThe removal of cells based on basic count statistics is split up into two parts: first, cells are flagged for removal by filter_with_counts. It flags observations based on several thresholds:\n\nThe number of genes that have a least a single count. Both a maximum and minimum number of genes for a cell to be removed can be specified.\nThe percentage of read counts that originated from a mitochodrial genes. Cells can be filtered based on both a maximum or minimum fraction of mitochondrial genes.\nThe minimum or maximum total number of counts captured per cell. Cells with 0 total counts are always removed.\n\nFlagging cells for removal involved adding a boolean column to the .obs dataframe. After the cells have been flagged for removal, the cells are actually filtered using do_filter, which reads the values in .obs and removed the cells labeled True. This applies the general phylosophy of “separation of concerns”: one component is responsible for labeling the cells, another for removing them. This keeps the codebase for a single component small and its functionality testable.\nThe next and final step in the single-sample gene expression processing is doublet detection using filter_with_scrublet. Like filter_with_counts, it will not remove cells but add a column to .obs (which have the name filter_with_scrublet by default). The single-sample GEX workflow will not remove not be removed during the processing (hence no do_filter). However, you can choose to remove them yourself before doing your analyses by applying a filter with the column in .obs yourself.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-single-sample-adt",
    "href": "fundamentals/architecture.html#sec-single-sample-adt",
    "title": "Architecture",
    "section": "Single-sample Antibody Capture Processing",
    "text": "Single-sample Antibody Capture Processing\nThe process of filtering antibody capture data is similar to the filtering in the single-sample gene-expression processing, but without doublet detection. In some particular cases you can use your ADT data to perform doublet detection using for example cell-type maskers. More information can be found in the single-cell best practices book.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#multisample-gene-expression-processing",
    "href": "fundamentals/architecture.html#multisample-gene-expression-processing",
    "title": "Architecture",
    "section": "Multisample Gene Expression Processing",
    "text": "Multisample Gene Expression Processing\nProcessing multisample gene expression involved the following steps:\n\nNormalization: Normalization aims to adjust the raw counts in the dataset for variable sampling effects by scaling the observable variance to a specified range. There are different ways to transform the data, but the normalization method is to make sure each observation (cell) has a total count equal to the median of total counts over all genes for observations (cells) before normalization.\nLog transformation: Calculates \\(X = ln(X + 1)\\), which converts multiplicative relative changes to additive differences. This allows for interpreting the gene expression in terms of relative, rather than absolute, abundances of genes.\nHighly variable gene detection: Detects genes that have a large change in expression between samples. By default, OpenPipeline uses the method from Seurat (Satija et al.). As with other “filtering” components, the filter_with_hvg component does not remove features, but rather annotates genes of interest by adding a boolean column to .var.\nQC metric calculations",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#multisample-antibody-capture-processing",
    "href": "fundamentals/architecture.html#multisample-antibody-capture-processing",
    "title": "Architecture",
    "section": "Multisample Antibody Capture Processing",
    "text": "Multisample Antibody Capture Processing\nProcessing the ADT modality for multiple samples",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-dimensionality-reduction",
    "href": "fundamentals/architecture.html#sec-dimensionality-reduction",
    "title": "Architecture",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nscRNA-seq is a high-throughput sequencing technology that produces datasets with high dimensions in the number of cells and genes. It is true that the data should provide more information, but it also contains more noise and redudant information, making it harder to distill the usefull information. The number of genes and cells can already reduced by gene filtering, but further reduction is a necessity for downstream analysis. Dimensionality reduction projects high-dimensional data into a lower dimensional space (like taking a photo (2D) of some 3D structure). The lower dimensional representation still captures the underlying information of the data, while having fewer dimensions.\nSeveral dimensionality reduction methods have been developed and applied to single-cell data analysis. Two of which are being applied in OpenPipeline:\n\nPrincipal Component Analysis (PCA): PCA reduces the dimension of a dataset by creating a new set of variables (principal components, PCs) from a linear combination of the original features in such a way that they are as uncorrelated as possible. The PCs can be ranked in the order by which they explain the largest variability in the original dataset. By keeping the top n PCs, the PCs with the lowest variance are discarded to effectively reduce the dimensionality of the data without losing information.\nUniform manifold approximation and projection (UMAP): a non-linear dimensionality technique. It constructs a high dimensional graph representation of the dataset and optimizes the low-dimensional graph representation to be structurally as similar as possible to the original graph. In a review by Xiang et al., 2021 it showed the highest stability and separates best the original cell populations.\nt-SNE is another popular non-linear, graph based dimensionality technique which is very similar to UMAP, but it has not yet been implemented in OpenPipeline.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-initializing-integration",
    "href": "fundamentals/architecture.html#sec-initializing-integration",
    "title": "Architecture",
    "section": "Initializing integration",
    "text": "Initializing integration\nAs will be descibed in more details later on, many integration methods exist and therefore there is no single integration which is executed by default. However, there are common tasks which are run before integration either because they provide required input for many downstream integration methods or because they popular steps that would otherwise be done manually. These operations are executed by default when using the “full pipeline” as part of the initialize_integration subworkflow.\nPCA is used to reduce the dimensionality of the dataset as described previously. Find Neighbors and Leiden Clustering are useful for the identification of cell types or states in the data. Here we apply a popular method to accomplish this is to first calculate a neighborhood graph on a low dimensinonal representation of the data and then cluster the data based on similarity between data points. Finally, UMAP allows us to visualise the clusters by reducing the dimensionality of the data while still providing an accurate representation of the underlying cell population structure.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "fundamentals/architecture.html#sec-integration-methods",
    "href": "fundamentals/architecture.html#sec-integration-methods",
    "title": "Architecture",
    "section": "Integration Methods",
    "text": "Integration Methods\nIntegration is the alignment of cell types across samples. There exist three different types of integration methods, based on the degree of integration across modalities:\n\nUnimodal integration across batches. For example: scVI, scanorama, harmony\nMultimodal integration across batches and modalities. Can be used to integrate joint-profiling data where multiple modalities are measured. For example: totalVI\nMosaic integration: data integration across batches and modalities where not all cells are profiled in all modalities and it may be the case that no cells contain profiles in all integrated modalities. Mosaic integration methods have not been made available in OpenPipeline yet. An example of a tool that performs mosaic integration is StabMap.\n\nIn either of the three cases, concatenated samples are required, and merged modalities preferred. A plethora of integration methods exist, which in turn interact with other functionality (like clustering and dimensionality reduction methods) to generate a large number of possible usecases which one pipeline cannot cover in an easy manner. Therefore, there is no single integration step that is part of a global pipeline which is executed by default. Instead, a user can choose from the integration workflows provided, and ‘stack’ integration methods by adding the outputs to different output slots of the MuData object. The following sections will descibe the integration workflows that are available in OpenPipeline.\n\nUnimodal integration\nFor unimodal integration, scVI, scanorama and harmony have been added to the scvi_leiden, scanorama_leiden, and harmony_leiden workflows respectively. After executing the integration methods themselves, Find Neighbors and Leiden Clustering are run the results of the integration as wel as UMAP in order to be able to visualise the results. The functioning of these components has already been described here.\n\n\n\nMultimodal Integration\nA single multimodal integration method is currently avaiable in OpenPipeline: totalVI. It allows using information from both the gene-expression data and the antibody-capture data together to integrate the cell types. As with the other integration workflows, after running totalVI, Find Neighbors, Leiden Clustering and UMAP are run on the result. However in this case the three components are executed on both of the integrated modalities.",
    "crumbs": [
      "Fundamentals",
      "Architecture"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html",
    "href": "contributing/pull_requests.html",
    "title": "Publishing your changes",
    "section": "",
    "text": "After ensuring that the implemented changes pass all relevant tests and meets the contribution guidelines, you can create a pull request following the steps below.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html#step-1-merge-upstream-repository",
    "href": "contributing/pull_requests.html#step-1-merge-upstream-repository",
    "title": "Publishing your changes",
    "section": "Step 1: Merge upstream repository",
    "text": "Step 1: Merge upstream repository\nBefore you contribute your changes need to merge the upstream main branch into your fork. This ensures that your changes are based on the latest version of the code.\nTo do this, enter the following commands adapted from Syncing a Fork in your terminal or command prompt:\n# add the upstream repository to your local repository\ngit remote add upstream https://github.com/openpipelines-bio/openpipeline.git\n# download the changes from the openpipelines repo\ngit fetch upstream\n# change your current branch to the branch of the pull request\ngit checkout &lt;feature_branch&gt;\n# merge the changes from upstream into your branch\ngit merge upstream/main\n# push the updates, your pull request will also be updated\ngit push",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html#step-2-edit-changelog",
    "href": "contributing/pull_requests.html#step-2-edit-changelog",
    "title": "Publishing your changes",
    "section": "Step 2: Edit changelog",
    "text": "Step 2: Edit changelog\nAdd an entry to the CHANGELOG.md file describing the proposed changes.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html#step-3-create-pull-request",
    "href": "contributing/pull_requests.html#step-3-create-pull-request",
    "title": "Publishing your changes",
    "section": "Step 3: Create pull request",
    "text": "Step 3: Create pull request\nThe following steps were adapted from Creating a pull request from a fork\n\nGo to https://github.com/openpipelines-bio/openpipeline/pulls.\nClick on the New pull request button.\nOn the compare page click on the link compare across forks below the title. \nOn the right side in the head section select your fork repo and the correct branch you want to merge.\nClick on Create pull request.\nConstruct your PR by giving it a title and description.\nMake sure you select the box below the description Allow edits from maintainers.\nIf the PR is ready for review click the button Create Pull Request. Otherwise you can click the arrow next to the button and select Create Draft Pull Request and click the button when it changes.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/pull_requests.html#next-steps",
    "href": "contributing/pull_requests.html#next-steps",
    "title": "Publishing your changes",
    "section": "Next steps",
    "text": "Next steps\n\nGithub Actions\nWhenever a Pull Request (including draft) is created a github workflow will perform checks. These checks need to be succesful as a minimum requirement before a merge can be done. When there are errors in the checks, try to fix them while waiting on a review. If it is not possible to fix the error, add a comment to the PR to let the reviewers know.\n\n\nReview\nYour PR will be reviewed by maintainers of OpenPipelines. During the review, you can be asked for changes to the code.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Publishing your changes"
    ]
  },
  {
    "objectID": "contributing/running_tests.html",
    "href": "contributing/running_tests.html",
    "title": "Running tests",
    "section": "",
    "text": "The input data that is needed to run the tests will need to be downloaded from the openpipelines Amazon AWS s3 bucket first. To do so, the download/sync_test_resource component can be used, which will download the data to the correct location (resources_test) by default.\nviash run src/download/sync_test_resources/config.vsh.yaml",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Running tests"
    ]
  },
  {
    "objectID": "contributing/running_tests.html#fetch-the-test-data",
    "href": "contributing/running_tests.html#fetch-the-test-data",
    "title": "Running tests",
    "section": "",
    "text": "The input data that is needed to run the tests will need to be downloaded from the openpipelines Amazon AWS s3 bucket first. To do so, the download/sync_test_resource component can be used, which will download the data to the correct location (resources_test) by default.\nviash run src/download/sync_test_resources/config.vsh.yaml",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Running tests"
    ]
  },
  {
    "objectID": "contributing/running_tests.html#run-component-tests",
    "href": "contributing/running_tests.html#run-component-tests",
    "title": "Running tests",
    "section": "Run component tests",
    "text": "Run component tests\nTo build and run tests for individual component that you are working on, use viash test with the config.vsh.yaml of the component you would like to test. For example:\nviash test src/convert/from_10xh5_to_h5mu/config.vsh.yaml\nKeep in mind that when no platform is passed to viash test, it will use the first platform that is specified in the config, which is docker for most of the components in openpipelines. Use -p native for example if you do not want to use docker.\nIt is also possible to execute the tests for all components in each namespace using:\nviash ns test --parallel -q convert",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Running tests"
    ]
  },
  {
    "objectID": "contributing/running_tests.html#run-integration-tests",
    "href": "contributing/running_tests.html#run-integration-tests",
    "title": "Running tests",
    "section": "Run integration tests",
    "text": "Run integration tests\nIndividual integration tests can be run by using the integration_test.sh scripts for a pipeline, located next to the main.nf in the src/workflows folder.\nsrc/workflows/ingestion/cellranger_demux/integration_test.sh",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Running tests"
    ]
  },
  {
    "objectID": "contributing/creating_components.html",
    "href": "contributing/creating_components.html",
    "title": "Creating components",
    "section": "",
    "text": "One of the core principals of OpenPipelines is to use MuData as a common data format troughout the whole pipeline. See the concepts page for more information on openpipelines uses MuData to store single-cell data.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#a-common-file-format",
    "href": "contributing/creating_components.html#a-common-file-format",
    "title": "Creating components",
    "section": "",
    "text": "One of the core principals of OpenPipelines is to use MuData as a common data format troughout the whole pipeline. See the concepts page for more information on openpipelines uses MuData to store single-cell data.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#component-location",
    "href": "contributing/creating_components.html#component-location",
    "title": "Creating components",
    "section": "Component location",
    "text": "Component location\nAs discussed in the project structure, components in the repository are stored within src. Additionally, components are grouped into namespaces, according to a common functionality. An example of such a namespace is the dimensionality reduction namespace (dimred), of which the components pca and umap are members. This means that within src, the namespace folders can be found that stores the components that belong to these namespaces.\nIn order to create a new component in OpenPipelines, you will need to create a new folder that will contain the different elements of the component:\nmkdir src/my_namespace/my_component\n\n\n\n\n\n\nTip\n\n\n\nTake a look at the components that are already in src/! There might be a component that already does something similar to what you need.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#the-elements-of-a-component",
    "href": "contributing/creating_components.html#the-elements-of-a-component",
    "title": "Creating components",
    "section": "The elements of a component",
    "text": "The elements of a component\nA component consists of one or more scripts that provide the functionality of the component together with metadata of the component in a configuration file. The Viash config contains metadata of your dataset, which script is used to run it, and the required dependencies. An in-depth guide on how to create components is available on the viash website, but a few specifics and guidelines will be discussed here.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#the-config",
    "href": "contributing/creating_components.html#the-config",
    "title": "Creating components",
    "section": "The config",
    "text": "The config\nfunctionality:\n  name: \"my_component\"\n  namespace: \"my_namespace\"\n  description: \"My new custom component\"\n  authors:\n    - __merge__: ../../authors/my_name.yaml\n      roles: [ author ]\n  arguments:\n    - name: \"--output\"\n      type: file\n      example: \"output_file.h5mu\"\n      description: \"Location were the output file should be written to.\"\n      direction: \"output\"\n  resources:\n    - type: python_script\n      path: script.py\nplatforms:\n  - type: docker\n    image: python:3.11\n    setup:\n      - type: python\n        packages: mudata~=0.2.3\n  - type: nextflow\n    directives:\n      label: [highcpu, midmem]\n\nBasic information\nEach component should have the name, a namespace, a description and author information defined in the config. Because a single author can contribute to multiple components, the author information is often duplicated across components, which was causing issues with the author information being out of date and not easy to maintain. Therefore, it was decided to move author information to ./src/authors. Each author has a yaml file containing the author information, and the viash __merge__ property is used to merge this information into the viash configs.\nBasic information checklist:\n\nGive the component a name\nAdd the component to an appropriate namespace\nAdd a description\nAdd author information\n\n\n\nArguments and argument groups\nIf you component requires arguments, they should be defined in arguments or argument_groups. Try tro group individual arguments into argument_groups when the number of arguments become too larg (10 or more as a rule of thumb).\nArgument checklist:\n\nAdd a description and name\nEach argument should have the appropriate type.\nInput and output files should be of type file instead of string and use the appropriate direction:\nIf possible: add an example\nIf the argument can accept multiple values, add multiple: true\nIf the possible input for an argument is limited to certain set of values, use choices:\n\n\n\n(Test)resources\nResources define files that are required for a component to perform its function. These can be scripts, but also additional files like settings for tools you might require. Defining resources is both a necessity because viash needs to know what code to execute, but defining resources also has the added benefit that these resources are automatically made available, regardless of the build environment. For example: resources are automatically mounted within a running docker container.\nThere is a difference between defining resources and test_resources. While resources are required for a component to function, test_resources only need to be included when testing the component (with for example viash test) in addition to the regular resources. Having a look at the example above, resources are defined using the resources: property. It takes a list of multiple files or folders.\nIn openpipelines, it was decided to not use a service like git lfs to include large resources into the repository. Instead, if large resources are required, there are two possibilities: * Large resources required for testing are to uploaded into an s3 bucket that is synced automatically before running tests (both locally and on github). Please ping a maintainer when you open a PR and ask them to upload the files for you. * Other large resources that are not needed for testing can be considered as input. This means that an argument of type: file needs to be created. The downside of this method is that viash is not able to natively support remote files f\nResources checklist: - Script resources are located next to the config and added to the config with the correct type (python_script, r_script, …) - Small resources (&lt;50MB) that are not scripts can also be checked in into the repo, next to the\n\n\nThe script file\nTODO\n\n\nAuthor information\nTODO",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#adding-dependencies",
    "href": "contributing/creating_components.html#adding-dependencies",
    "title": "Creating components",
    "section": "Adding dependencies",
    "text": "Adding dependencies\nTODO",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#building-components-from-their-source",
    "href": "contributing/creating_components.html#building-components-from-their-source",
    "title": "Creating components",
    "section": "Building components from their source",
    "text": "Building components from their source\nWhen running or testing individual components, it is not necessary to execute an extra command to run the build step, viash test and viash run will build the component on the fly. However, before integrating components into a pipeline, you will need to build the components. More specifically, openpipelines uses Nextflow to combine components into pipelines, so we need to have at least the components build for nextflow platform as target. The easiest method to build the components is to use:\nviash ns build --parallel --setup cachedbuild\nAfter using viash ns build, the target folder will be populated with three subfolders, corresponding to the build platforms that viash supports: native, docker and nextflow.\nBuilding an individual component can still be useful, for example when debugging a component for which the build fails or if you want to create a standalone executable for a component to execute it without the need to use viash. To build an individual component, viash build can be used. Note that the default build directory of this viash base command is output, which is not the location where build components will be imported from when integrating them in pipelines. Using the --output argument, you can set it to any directory you want, for example:\nviash build src/filter/do_filter/config.vsh.yaml -o target/native/filter/do_filter/ -p native",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/creating_components.html#containerization",
    "href": "contributing/creating_components.html#containerization",
    "title": "Creating components",
    "section": "Containerization",
    "text": "Containerization\nOne of the key benefits of using Viash is that containers can be created that gather dependencies per component, which avoids building one container that has to encorporate all dependencies for a pipeline together. The containers for a single component can be reduced in size, defining the minimal requirements to run the component. That being said, building containers from scratch can be labour intensive and error prone, with base containers from reputable publishers often benefiting from improved reliability and security. Hence, a balance has to be made between reducing the container’s size and adding many dependencies to a small base container.\nThe preferred containerization setup in OpenPipelines uses the following guidelines:\n\nChoose a base container from a reputable source and use its latest version\nDo not use base containers that have not been updated in a while\nUse package managers to install dependencies as much as possible\nAvoid building depdencies from source.\n\nExamples of base containers that are currently being used are:\n\npython:3.11 for python environments\nubuntu:focal for general linux environments and bash scripts\neddelbuettel/r2u:22.04 for R\nnvcr.io/nvidia/pytorch:22.09-py3 for using GPU accelerated calculations using pytorch in python",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Creating components"
    ]
  },
  {
    "objectID": "contributing/getting_started.html",
    "href": "contributing/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "The OpenPipelines code is hosted on GitHub. To start working on OpenPipelines, you should create your own copy of the repository by forking it. Visit the OpenPipelines repository here and use the ‘Fork’ button on the top right hand side of the page. After you are done forking, you can clone the repository to a local directory on your computer using git clone. You can choose between using an SSH key to log in to GitHub or username and password (HTTPS) to connect to github.\n\nHTTPSSSH\n\n\ngit clone https://github.com/&lt;YOUR USERNAME&gt;/openpipeline.git\ncd openpipeline\ngit remote add upstream https://github.com/openpipeline-bio/openpipeline.git\n\n\ngit clone git@github.com:&lt;YOUR USERNAME&gt;/openpipeline.git\ncd openpipeline\ngit remote add upstream https://github.com/openpipeline-bio/openpipeline.git",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Getting started"
    ]
  },
  {
    "objectID": "contributing/getting_started.html#forking-the-code-and-cloning-the-repository",
    "href": "contributing/getting_started.html#forking-the-code-and-cloning-the-repository",
    "title": "Getting started",
    "section": "",
    "text": "The OpenPipelines code is hosted on GitHub. To start working on OpenPipelines, you should create your own copy of the repository by forking it. Visit the OpenPipelines repository here and use the ‘Fork’ button on the top right hand side of the page. After you are done forking, you can clone the repository to a local directory on your computer using git clone. You can choose between using an SSH key to log in to GitHub or username and password (HTTPS) to connect to github.\n\nHTTPSSSH\n\n\ngit clone https://github.com/&lt;YOUR USERNAME&gt;/openpipeline.git\ncd openpipeline\ngit remote add upstream https://github.com/openpipeline-bio/openpipeline.git\n\n\ngit clone git@github.com:&lt;YOUR USERNAME&gt;/openpipeline.git\ncd openpipeline\ngit remote add upstream https://github.com/openpipeline-bio/openpipeline.git",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Getting started"
    ]
  },
  {
    "objectID": "contributing/getting_started.html#sec-install-viash-nextflow",
    "href": "contributing/getting_started.html#sec-install-viash-nextflow",
    "title": "Getting started",
    "section": "Install viash and nextflow",
    "text": "Install viash and nextflow\nTo start contributing to OpenPipelines, you will need at Java 11 (or higher) and Docker installed on your system.\nOpenPipelines is being developed in Viash and Nextflow. If you are unfamiliar with either one of these platforms, you can check out their respective documentation pages.\nYou can check if is installed correctly by running the following commands.\nnextflow run hello -with-docker\nviash --version",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Getting started"
    ]
  },
  {
    "objectID": "contributing/getting_started.html#fetch-test-resources",
    "href": "contributing/getting_started.html#fetch-test-resources",
    "title": "Getting started",
    "section": "Fetch test resources",
    "text": "Fetch test resources\nOpenPipelines uses a number of test resources to test the pipelines. If everything is installed correctly, you should be able to fetch these resources by running the following command.\nviash run src/download/sync_test_resources/config.vsh.yaml",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Getting started"
    ]
  },
  {
    "objectID": "contributing/index.html",
    "href": "contributing/index.html",
    "title": "Contributing",
    "section": "",
    "text": "Getting started: Install dependencies and fetch test resources\n  \n  \n  \n    Project structure: The structure of OpenPipelines\n  \n  \n  \n    Creating components: A guide on how to create new components\n  \n  \n  \n    Creating pipelines: A guide on how to create new workflows\n  \n  \n  \n    Running tests: How to run component and integration tests.\n  \n  \n  \n    Publishing your changes: How to create a pull request\n  \n  \n\n\nNo matching items",
    "crumbs": [
      "Fundamentals",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing/project_structure.html",
    "href": "contributing/project_structure.html",
    "title": "Project structure",
    "section": "",
    "text": "The root of the repository contains two main folders:\n\nsrc, which contains the source code for components and workflows.\n(optionally) the target folder\n\nEach subfolder from src contains a Viash namespace, a logical grouping of pipeline components that perform a similar function. Within each namespace, subfolders designate individual pipeline components. For example ./src/convert/from_bdrhap_to_h5ad contains the implementation for a component from_bdrhap_to_h5ad which is grouped together with other components such as from_10xmtx_to_h5mu into a namespace convert. In a similar manner as grouping components into namespaces, pipelines are grouped together into folders. However, these are not component namespaces and as such do not interact with viash ns commands.\nAs will become apparent later on, Viash not only provides commands to perform operations on individual components, but also on groups of components in a namespace and all components in a project. As a rule of thumb, the basic Viash commands (like viash test) are designated for running commands on individual components, while ns commands are (viash ns test) are for namespaces. When cloning a fresh repository, there will be no target folder present. This is because the target folder will only be created after components have been build.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Project structure"
    ]
  },
  {
    "objectID": "contributing/project_structure.html#sec-project-structure",
    "href": "contributing/project_structure.html#sec-project-structure",
    "title": "Project structure",
    "section": "",
    "text": "The root of the repository contains two main folders:\n\nsrc, which contains the source code for components and workflows.\n(optionally) the target folder\n\nEach subfolder from src contains a Viash namespace, a logical grouping of pipeline components that perform a similar function. Within each namespace, subfolders designate individual pipeline components. For example ./src/convert/from_bdrhap_to_h5ad contains the implementation for a component from_bdrhap_to_h5ad which is grouped together with other components such as from_10xmtx_to_h5mu into a namespace convert. In a similar manner as grouping components into namespaces, pipelines are grouped together into folders. However, these are not component namespaces and as such do not interact with viash ns commands.\nAs will become apparent later on, Viash not only provides commands to perform operations on individual components, but also on groups of components in a namespace and all components in a project. As a rule of thumb, the basic Viash commands (like viash test) are designated for running commands on individual components, while ns commands are (viash ns test) are for namespaces. When cloning a fresh repository, there will be no target folder present. This is because the target folder will only be created after components have been build.",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Project structure"
    ]
  },
  {
    "objectID": "contributing/project_structure.html#sec-versioning",
    "href": "contributing/project_structure.html#sec-versioning",
    "title": "Project structure",
    "section": "Versioning and branching strategy",
    "text": "Versioning and branching strategy\nOpenPipeline tries to use of semantic versioning to govern changes between versions. An release of openpipelines uses a version number in the format MAJOR.MINOR.PATCH. Currenly, openpipelines is still at major version 0.x.y, meaning that public-facing breaking changes are possible on MINOR releases. These breaking changes will be documented in a dedicated section of the CHANGELOG that is published with each release. A PATCH release (i.e. a release where the MAJOR and MINOR version number stay the same), is used to resolve bugs with the pipeline but should not introduce breaking changes. Keep in mind that patches might introduce behavioral changes that may look breaking but are actually rectifying changes that were inadvertently introduced previously (and were in fact also ‘breaking changes’). In this case, a bug can also be released without changing the MINOR version, in a PATH release.\nBetween releases, development progress is tracked on Git branches. A git branch represents a snapshot of a codebase in time, to which changes can be added (i.e. committed). Eventually, all new feature or bugfixes must be reconsiled into a single branch so that a new release can be created. This process is called merging and the process of requesting the merging of two branches is called a pull request. Openpipelines follows the convention that the target branch for all pull requests is the main branch. Thus, the main branch contains the latest changes for the code and it can be considered the development branch.\nOnce a pull request has been approved and merged, Github Actions CI will automatically build all components (creating the target directory) and push the result to the main_build branch. In essence, the main_build branch is a copy of the main branch, but also containing the build components. Once it is time to create a openpipelines release, the Github CI release workflow is manually triggered, the components on the main branch will be build and tested. Then, the result will be pushed to the release branch and the integration tests will be run. If all tests succeeded, a new github tag and release can be created manually from the release branch.\n\n\n\n\n\n%%{init: { 'logLevel': 'debug', 'theme': 'default'} } }%%\ngitGraph\n  commit id: \"initial commit\"\n  branch main_build\n  commit id: \"CI build\"\n  checkout main\n  commit\n  checkout main_build\n  merge main\n  checkout main\n  branch feature_a\n  branch feature_b\n  checkout feature_a\n  commit\n  commit\n  checkout main\n  commit id: \"#release 0.1\" type: HIGHLIGHT\n  checkout main_build\n  merge main\n  checkout main\n  branch release\n  commit tag: \"0.1\"\n  checkout main\n  commit\n  checkout feature_b\n  commit\n  commit\n  checkout feature_a\n  commit\n  checkout main\n  merge feature_a\n  checkout main_build\n  merge main\n  checkout main\n  checkout feature_b\n  commit\n  checkout main\n  merge feature_b\n  checkout main_build\n  merge main\n  checkout release\n  merge main tag: \"0.2\"",
    "crumbs": [
      "Fundamentals",
      "Contributing",
      "Project structure"
    ]
  },
  {
    "objectID": "fundamentals/philosophy.html",
    "href": "fundamentals/philosophy.html",
    "title": "Philosophy",
    "section": "",
    "text": "Mission\nOpenPipelines are best-practice living workflows for single-cell uni- and multi-omics data. Building a best-practice pipeline requires knowledge and time that not one single person can provide, but rather requires input from a community. Additionally, a best-pratice pipeline needs constant maintenance to keep up to date with the latest standards, ideally sourcing input from a ‘living’ benchmark. Continuous improvement necessitates a robust system for sourcing and applying community input both from a technical and organisational standpoint.\n\n\n\n\n\ngraph TB\n  ben[\"🌱📈 Living benchmarks\"]\n  pra[\"🌱📖 Living best practices\"]\n  pip[\"🌱⚙️ Living reproducible pipelines\"]\n  ben --&gt; pra --&gt; pip",
    "crumbs": [
      "Fundamentals",
      "Philosophy"
    ]
  },
  {
    "objectID": "fundamentals/roadmap.html",
    "href": "fundamentals/roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "flowchart LR\n  classDef done fill:#a3f6cf,stroke:#000000;\n  classDef wip fill:#f4cb93,stroke:#000000;\n  classDef unprocessed fill:#afadff,stroke:#000000;\n\n  Raw1[Sample 1] --&gt; Split1[/Split\\nmodalities/]:::done --&gt; ProcGEX1 & ProcRNAV1 & ProcADT1 & ProcATAC1 & ProcVDJ1\n  ProcGEX1[/Process GEX\\nprofile/]:::done --&gt; ConcatGEX[/Concatenate\\nprofiles/]:::done --&gt; ProcGEX[/Process GEX\\nprofiles/]:::done\n  ProcRNAV1[/Process RNAV\\nprofile/]:::wip --&gt; ConcatRNAV[/Concatenate\\nprofiles/]:::done --&gt; ProcRNAV[/Process RNAV\\nprofiles/]:::wip\n  ProcADT1[/Process ADT\\nprofile/]:::done --&gt; ConcatADT[/Concatenate\\nprofiles/]:::done --&gt; ProcADT[/Process ADT\\nprofiles/]:::done\n  ProcATAC1[/Process ATAC\\nprofile/]:::unprocessed --&gt; ConcatATAC[/Concatenate\\nprofiles/]:::done --&gt; ProcATAC[/Process ATAC\\nprofiles/]:::unprocessed\n  ProcVDJ1[/Process VDJ\\nprofile/]:::unprocessed --&gt; ConcatVDJ[/Concatenate\\nprofiles/]:::done --&gt; ProcVDJ[/Process VDJ\\nprofiles/]:::unprocessed\n  ProcGEX & ProcRNAV & ProcADT & ProcATAC & ProcVDJ --&gt; Merge[/Merge\\nmodalities/]:::done --&gt; SetupIntegration[/Setup\\nintegration/]:::done --&gt; Integration[/Integration/]:::done\n\n\n\n\nFigure 1: Status of implemented components. Green: implemented, orange: work in progress, purple: modality included in output but unprocessed,\nGEX: Gene-expression. RNAV: RNA Velocity. ADT: Antibody-Derived Tags. ATAC: Assay for Transposase-Accessible Chromatin.\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  subgraph ingestion\n    direction TB\n    subgraph cellranger_multi\n      direction TB\n      mapping_10x --&gt; convert_to_h5mu_10x\n      mapping_10x[mapping]:::done\n      convert_to_h5mu_10x[convert_to_h5mu]:::done\n    end\n    subgraph bdrhap_v1\n      direction TB\n      mapping_bd1 --&gt; convert_to_h5mu_bd1\n      mapping_bd1[mapping]:::done\n      convert_to_h5mu_bd1[convert_to_h5mu]:::done\n    end\n    subgraph bdrhap_v2\n      direction TB\n      mapping_bd2 --&gt; convert_to_h5mu_bd2\n      mapping_bd2[mapping]:::wip\n      convert_to_h5mu_bd2[convert_to_h5mu]:::wip\n    end\n    cellranger_multi:::subwf\n    bdrhap_v1:::subwf\n    bdrhap_v2:::subwf\n  end\n  ingestion:::wf\n  subgraph process_samples\n    split_modalities --&gt; rna_singlesample & prot_singlesample & gdo_singlesample & atac_singlesample & other_modalities --&gt; concat --&gt; process_batches\n    split_modalities:::done\n    rna_singlesample:::done\n    prot_singlesample:::done\n    gdo_singlesample:::done\n    atac_singlesample:::todo\n    other_modalities:::done\n    concat:::done\n    subgraph process_batches\n      direction LR\n      split_modalities2 --&gt; rna_multisample & prot_multisample & atac_multisample & other_modalities2 --&gt; merge\n      split_modalities2[split_modalities]:::done\n      other_modalities2[other_modalities]:::done\n      rna_multisample:::done\n      prot_multisample:::done\n      merge:::done\n    end\n    process_batches:::subwf\n    atac_multisample:::todo\n  end\n  process_samples:::wf\n  raw_counts --- ingestion --&gt; raw_h5mu\n  raw_h5mu --- process_samples --&gt; processed_h5mu\n  subgraph integration\n    direction LR\n    integration_method --&gt; find_neighbors --&gt; leiden --&gt; umap\n    integration_method -.- intmeth\n    integration_method:::done\n    find_neighbors:::done\n    leiden:::done\n    umap:::done\n    subgraph intmeth [integration_method]\n      bbknn:::done\n      harmony:::done\n      scanorama:::done\n      scvi:::done\n      totalvi:::done\n      scgpt_integration:::wip\n    end\n    intmeth:::info\n  end\n  integration:::wf\n  processed_h5mu --- integration ---&gt; integrated_h5mu\n  subgraph celltype_annotation\n    direction TB\n    integration_method2[integration_method]:::done\n    celltypist:::wip\n    scanvi:::wip\n    scgpt_annotation:::wip\n    onclass:::todo\n    svm:::todo\n    randomforest:::todo\n    pynndescent_knn:::wip\n    consensus_voting:::todo\n    integration_method2 --&gt; pynndescent_knn --&gt; consensus_voting\n    celltypist & scanvi & scgpt_annotation & onclass & svm & randomforest --&gt; consensus_voting\n  end\n  reference_atlas --&gt; celltype_annotation\n  celltype_annotation:::wf\n  integrated_h5mu --- celltype_annotation --&gt; annotated_h5mu\n\n  classDef done fill:#ccebc5,stroke:#4daf4a\n  classDef wip fill:#fed9a6,stroke:#ff7f00\n  classDef todo fill:#fbb4ae,stroke:#e41a1c\n  classDef wf fill:#f0f0f0,stroke:#525252\n  classDef subwf fill:#d9d9d9,stroke:#525252\n  classDef info fill:#f0f0f0,stroke:#525252,stroke-dasharray: 4 4\n  \n  subgraph Legend\n    done[Done]:::done\n    wip[Work in progress]:::wip\n    todo[To do]:::todo\n  end\n  Legend:::info\n\n\n\n\nFigure 2",
    "crumbs": [
      "Fundamentals",
      "Roadmap"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "OpenPipelines.bio next release",
    "section": "",
    "text": "Add descriptions to all pages and add listings to index pages.\nUpdate documentation on creating components for developers.\nUpdate getting started page for developers\nUpdate project structure.\nUpdate information on running tests.\nUpdate “More information” pages\nWrite getting started page for user guide\nDocument how to run workflows\nDocument parameter lists"
  },
  {
    "objectID": "CHANGELOG.html#major-changes",
    "href": "CHANGELOG.html#major-changes",
    "title": "OpenPipelines.bio next release",
    "section": "",
    "text": "Add descriptions to all pages and add listings to index pages.\nUpdate documentation on creating components for developers.\nUpdate getting started page for developers\nUpdate project structure.\nUpdate information on running tests.\nUpdate “More information” pages\nWrite getting started page for user guide\nDocument how to run workflows\nDocument parameter lists"
  },
  {
    "objectID": "CHANGELOG.html#major-changes-1",
    "href": "CHANGELOG.html#major-changes-1",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.12.1 release."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-2",
    "href": "CHANGELOG.html#major-changes-2",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.12.0 release."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-3",
    "href": "CHANGELOG.html#major-changes-3",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.11.0 release."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-4",
    "href": "CHANGELOG.html#major-changes-4",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.10.0 release.\nAdd documentation for OpenPipelines architecture."
  },
  {
    "objectID": "CHANGELOG.html#minor-changes",
    "href": "CHANGELOG.html#minor-changes",
    "title": "OpenPipelines.bio next release",
    "section": "MINOR CHANGES",
    "text": "MINOR CHANGES\n\nAlso generate documentation for the multiple_sep values of component arguments."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-5",
    "href": "CHANGELOG.html#major-changes-5",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.9.0 release."
  },
  {
    "objectID": "CHANGELOG.html#minor-changes-1",
    "href": "CHANGELOG.html#minor-changes-1",
    "title": "OpenPipelines.bio next release",
    "section": "MINOR CHANGES",
    "text": "MINOR CHANGES\n\nUpdate to Viash actions 0.4.0."
  },
  {
    "objectID": "CHANGELOG.html#major-changes-6",
    "href": "CHANGELOG.html#major-changes-6",
    "title": "OpenPipelines.bio next release",
    "section": "MAJOR CHANGES",
    "text": "MAJOR CHANGES\n\nUpdate component documentation to v0.8.0 release.\nUse git submodule to access openpipeline repo.\nPropose new website structure.\nUpdate author page."
  },
  {
    "objectID": "components/index.html",
    "href": "components/index.html",
    "title": "Reference",
    "section": "",
    "text": "Order By\n       Default\n         \n          Name\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nNamespace\n\n\nDescription\n\n\n\n\n\n\nBD Rhapsody\n\n\nWorkflows/ingestion\n\n\nBD Rhapsody Sequence Analysis CWL pipeline v2.2.1\n\n\n\n\nBbknn leiden\n\n\nWorkflows/integration\n\n\nRun bbknn followed by leiden clustering and run umap on the result.\n\n\n\n\nCell Ranger mapping\n\n\nWorkflows/ingestion\n\n\nA pipeline for running Cell Ranger mapping.\n\n\n\n\nCell Ranger multi\n\n\nWorkflows/ingestion\n\n\nA pipeline for running Cell Ranger multi.\n\n\n\n\nCell Ranger post-processing\n\n\nWorkflows/ingestion\n\n\nPost-processing Cell Ranger datasets.\n\n\n\n\nConvert to MuData\n\n\nWorkflows/ingestion\n\n\nA pipeline to convert different file formats to .h5mu.\n\n\n\n\nDemux\n\n\nWorkflows/ingestion\n\n\nA generic pipeline for running bcl2fastq, bcl-convert or Cell Ranger mkfastq.\n\n\n\n\nDimensionality reduction\n\n\nWorkflows/multiomics\n\n\nRun calculations that output information required for most integration methods: PCA, nearest neighbour and UMAP.\n\n\n\n\nGDO Singlesample\n\n\nWorkflows/gdo\n\n\nProcessing unimodal single-sample guide-derived oligonucleotide (GDO) data.\n\n\n\n\nHarmony leiden\n\n\nWorkflows/integration\n\n\nRun harmony integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nMake reference\n\n\nWorkflows/ingestion\n\n\nBuild a transcriptomics reference into one of many formats\n\n\n\n\nProcess batches\n\n\nWorkflows/multiomics\n\n\nThis workflow serves as an entrypoint into the ‘full_pipeline’ in order to re-run the multisample processing and the integration setup.\n\n\n\n\nProcess samples\n\n\nWorkflows/multiomics\n\n\nA pipeline to analyse multiple multiomics samples.\n\n\n\n\nProt multisample\n\n\nWorkflows/prot\n\n\nProcessing unimodal multi-sample ADT data.\n\n\n\n\nProt singlesample\n\n\nWorkflows/prot\n\n\nProcessing unimodal single-sample CITE-seq data.\n\n\n\n\nQc\n\n\nWorkflows/qc\n\n\nA pipeline to add basic qc statistics to a MuData\n\n\n\n\nRna multisample\n\n\nWorkflows/rna\n\n\nProcessing unimodal multi-sample RNA transcriptomics data.\n\n\n\n\nRna singlesample\n\n\nWorkflows/rna\n\n\nProcessing unimodal single-sample RNA transcriptomics data.\n\n\n\n\nScanorama leiden\n\n\nWorkflows/integration\n\n\nRun scanorama integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nScgpt leiden\n\n\nWorkflows/integration\n\n\nRun scGPT integration (cell embedding generation) followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nScvi leiden\n\n\nWorkflows/integration\n\n\nRun scvi integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nSplit modalities\n\n\nWorkflows/multiomics\n\n\nA pipeline to split a multimodal mudata files into several unimodal mudata files.\n\n\n\n\nTotalvi leiden\n\n\nWorkflows/integration\n\n\nRun totalVI integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nscGPT Annotation\n\n\nWorkflows/annotation\n\n\nCell type annotation workflow using scGPT.\n\n\n\n\nscGPT Annotation\n\n\nWorkflows/annotation\n\n\nCell type annotation workflow that performs scGPT integration of reference and query dataset followed by KNN label transfer.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "components/index.html#workflows",
    "href": "components/index.html#workflows",
    "title": "Reference",
    "section": "",
    "text": "Order By\n       Default\n         \n          Name\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nNamespace\n\n\nDescription\n\n\n\n\n\n\nBD Rhapsody\n\n\nWorkflows/ingestion\n\n\nBD Rhapsody Sequence Analysis CWL pipeline v2.2.1\n\n\n\n\nBbknn leiden\n\n\nWorkflows/integration\n\n\nRun bbknn followed by leiden clustering and run umap on the result.\n\n\n\n\nCell Ranger mapping\n\n\nWorkflows/ingestion\n\n\nA pipeline for running Cell Ranger mapping.\n\n\n\n\nCell Ranger multi\n\n\nWorkflows/ingestion\n\n\nA pipeline for running Cell Ranger multi.\n\n\n\n\nCell Ranger post-processing\n\n\nWorkflows/ingestion\n\n\nPost-processing Cell Ranger datasets.\n\n\n\n\nConvert to MuData\n\n\nWorkflows/ingestion\n\n\nA pipeline to convert different file formats to .h5mu.\n\n\n\n\nDemux\n\n\nWorkflows/ingestion\n\n\nA generic pipeline for running bcl2fastq, bcl-convert or Cell Ranger mkfastq.\n\n\n\n\nDimensionality reduction\n\n\nWorkflows/multiomics\n\n\nRun calculations that output information required for most integration methods: PCA, nearest neighbour and UMAP.\n\n\n\n\nGDO Singlesample\n\n\nWorkflows/gdo\n\n\nProcessing unimodal single-sample guide-derived oligonucleotide (GDO) data.\n\n\n\n\nHarmony leiden\n\n\nWorkflows/integration\n\n\nRun harmony integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nMake reference\n\n\nWorkflows/ingestion\n\n\nBuild a transcriptomics reference into one of many formats\n\n\n\n\nProcess batches\n\n\nWorkflows/multiomics\n\n\nThis workflow serves as an entrypoint into the ‘full_pipeline’ in order to re-run the multisample processing and the integration setup.\n\n\n\n\nProcess samples\n\n\nWorkflows/multiomics\n\n\nA pipeline to analyse multiple multiomics samples.\n\n\n\n\nProt multisample\n\n\nWorkflows/prot\n\n\nProcessing unimodal multi-sample ADT data.\n\n\n\n\nProt singlesample\n\n\nWorkflows/prot\n\n\nProcessing unimodal single-sample CITE-seq data.\n\n\n\n\nQc\n\n\nWorkflows/qc\n\n\nA pipeline to add basic qc statistics to a MuData\n\n\n\n\nRna multisample\n\n\nWorkflows/rna\n\n\nProcessing unimodal multi-sample RNA transcriptomics data.\n\n\n\n\nRna singlesample\n\n\nWorkflows/rna\n\n\nProcessing unimodal single-sample RNA transcriptomics data.\n\n\n\n\nScanorama leiden\n\n\nWorkflows/integration\n\n\nRun scanorama integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nScgpt leiden\n\n\nWorkflows/integration\n\n\nRun scGPT integration (cell embedding generation) followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nScvi leiden\n\n\nWorkflows/integration\n\n\nRun scvi integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nSplit modalities\n\n\nWorkflows/multiomics\n\n\nA pipeline to split a multimodal mudata files into several unimodal mudata files.\n\n\n\n\nTotalvi leiden\n\n\nWorkflows/integration\n\n\nRun totalVI integration followed by neighbour calculations, leiden clustering and run umap on the result.\n\n\n\n\nscGPT Annotation\n\n\nWorkflows/annotation\n\n\nCell type annotation workflow using scGPT.\n\n\n\n\nscGPT Annotation\n\n\nWorkflows/annotation\n\n\nCell type annotation workflow that performs scGPT integration of reference and query dataset followed by KNN label transfer.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "components/index.html#modules",
    "href": "components/index.html#modules",
    "title": "Reference",
    "section": "Modules",
    "text": "Modules\n\n\n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nNamespace\n\n\nDescription\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html",
    "href": "components/workflows/rna/rna_singlesample.html",
    "title": "Rna singlesample",
    "section": "",
    "text": "ID: rna_singlesample\nNamespace: workflows/rna\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html#example-commands",
    "href": "components/workflows/rna/rna_singlesample.html#example-commands",
    "title": "Rna singlesample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/rna/rna_singlesample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Input\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\n\n# Output\n# output: \"$id.$key.output.h5mu\"\n\n# Filtering options\n# min_counts: 200\n# max_counts: 5000000\n# min_genes_per_cell: 200\n# max_genes_per_cell: 1500000\n# min_cells_per_gene: 3\n# min_fraction_mito: 0.0\n# max_fraction_mito: 0.2\n\n# Mitochondrial gene detection\n# var_name_mitochondrial_genes: \"foo\"\n# obs_name_mitochondrial_fraction: \"foo\"\n# var_gene_names: \"gene_symbol\"\nmitochondrial_gene_regex: \"^[mM][tT]-\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/rna/rna_singlesample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html#argument-groups",
    "href": "components/workflows/rna/rna_singlesample.html#argument-groups",
    "title": "Rna singlesample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nInput layer to start from. By default, .X will be used.\nstring\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nFiltering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--min_counts\nMinimum number of counts captured per cell.\ninteger, example: 200\n\n\n--max_counts\nMaximum number of counts captured per cell.\ninteger, example: 5000000\n\n\n--min_genes_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--max_genes_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 1500000\n\n\n--min_cells_per_gene\nMinimum of non-zero values per gene.\ninteger, example: 3\n\n\n--min_fraction_mito\nMinimum fraction of UMIs that are mitochondrial. Requires –obs_name_mitochondrial_fraction.\ndouble, example: 0\n\n\n--max_fraction_mito\nMaximum fraction of UMIs that are mitochondrial. Requires –obs_name_mitochondrial_fraction.\ndouble, example: 0.2\n\n\n\n\n\nMitochondrial gene detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_name_mitochondrial_genes\nIn which .var slot to store a boolean array corresponding the mitochondrial genes.\nstring\n\n\n--obs_name_mitochondrial_fraction\nWhen specified, write the fraction of counts originating from mitochondrial genes (based on –mitochondrial_gene_regex) to an .obs column with the specified name. Requires –var_name_mitochondrial_genes.\nstring\n\n\n--var_gene_names\n.var column name to be used to detect mitochondrial genes instead of .var_names (default if not set). Gene names matching with the regex value from –mitochondrial_gene_regex will be identified as a mitochondrial gene.\nstring, example: \"gene_symbol\"\n\n\n--mitochondrial_gene_regex\nRegex string that identifies mitochondrial genes from –var_gene_names. By default will detect human and mouse mitochondrial genes from a gene symbol.\nstring, default: \"^[mM][tT]-\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html#authors",
    "href": "components/workflows/rna/rna_singlesample.html#authors",
    "title": "Rna singlesample",
    "section": "Authors",
    "text": "Authors\n\nDries De Maeyer   (author)\nRobrecht Cannoodt    (author, maintainer)\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/rna/rna_singlesample.html#visualisation",
    "href": "components/workflows/rna/rna_singlesample.html#visualisation",
    "title": "Rna singlesample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v11(filter)\n    v21(filter)\n    v36(cross)\n    v46(cross)\n    v53(mix)\n    v54(filter)\n    v69(cross)\n    v79(cross)\n    v85(filter)\n    v115(concat)\n    v100(cross)\n    v110(cross)\n    v122(cross)\n    v132(cross)\n    v141(branch)\n    v168(concat)\n    v146(delimit_fraction)\n    v153(cross)\n    v163(cross)\n    v169(filter)\n    v177(rna_filter_with_counts)\n    v184(cross)\n    v194(cross)\n    v200(filter)\n    v208(rna_do_filter)\n    v215(cross)\n    v225(cross)\n    v231(filter)\n    v261(concat)\n    v239(filter_with_scrublet)\n    v246(cross)\n    v256(cross)\n    v268(cross)\n    v275(cross)\n    v287(cross)\n    v294(cross)\n    v298(Output)\n    subgraph group_qc [qc]\n        v29(grep_annotation_column)\n        v62(calculate_qc_metrics)\n        v93(publish)\n    end\n    v53--&gt;v54\n    v141--&gt;v168\n    v168--&gt;v169\n    v0--&gt;v2\n    v2--&gt;v11\n    v21--&gt;v29\n    v29--&gt;v36\n    v21--&gt;v36\n    v21--&gt;v46\n    v54--&gt;v62\n    v62--&gt;v69\n    v54--&gt;v69\n    v54--&gt;v79\n    v85--&gt;v93\n    v93--&gt;v100\n    v85--&gt;v100\n    v85--&gt;v110\n    v110--&gt;v115\n    v115--&gt;v122\n    v11--&gt;v122\n    v11--&gt;v132\n    v141--&gt;v146\n    v146--&gt;v153\n    v141--&gt;v153\n    v141--&gt;v163\n    v163--&gt;v168\n    v169--&gt;v177\n    v177--&gt;v184\n    v169--&gt;v184\n    v169--&gt;v194\n    v200--&gt;v208\n    v208--&gt;v215\n    v200--&gt;v215\n    v200--&gt;v225\n    v231--&gt;v239\n    v239--&gt;v246\n    v231--&gt;v246\n    v231--&gt;v256\n    v256--&gt;v261\n    v261--&gt;v268\n    v2--&gt;v268\n    v268--&gt;v275\n    v2--&gt;v275\n    v2--&gt;v287\n    v287--&gt;v294\n    v2--&gt;v294\n    v294--&gt;v298\n    v11--&gt;v21\n    v46--&gt;v53\n    v29--&gt;v46\n    v11--&gt;v53\n    v79--&gt;v85\n    v62--&gt;v79\n    v93--&gt;v110\n    v115--&gt;v132\n    v132--&gt;v141\n    v146--&gt;v163\n    v194--&gt;v200\n    v177--&gt;v194\n    v225--&gt;v231\n    v208--&gt;v225\n    v239--&gt;v256\n    v261--&gt;v287\n    style group_qc fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v11 fill:#e3dcea,stroke:#7a4baa;\n    style v21 fill:#e3dcea,stroke:#7a4baa;\n    style v29 fill:#e3dcea,stroke:#7a4baa;\n    style v36 fill:#e3dcea,stroke:#7a4baa;\n    style v46 fill:#e3dcea,stroke:#7a4baa;\n    style v53 fill:#e3dcea,stroke:#7a4baa;\n    style v54 fill:#e3dcea,stroke:#7a4baa;\n    style v62 fill:#e3dcea,stroke:#7a4baa;\n    style v69 fill:#e3dcea,stroke:#7a4baa;\n    style v79 fill:#e3dcea,stroke:#7a4baa;\n    style v85 fill:#e3dcea,stroke:#7a4baa;\n    style v115 fill:#e3dcea,stroke:#7a4baa;\n    style v93 fill:#e3dcea,stroke:#7a4baa;\n    style v100 fill:#e3dcea,stroke:#7a4baa;\n    style v110 fill:#e3dcea,stroke:#7a4baa;\n    style v122 fill:#e3dcea,stroke:#7a4baa;\n    style v132 fill:#e3dcea,stroke:#7a4baa;\n    style v141 fill:#e3dcea,stroke:#7a4baa;\n    style v168 fill:#e3dcea,stroke:#7a4baa;\n    style v146 fill:#e3dcea,stroke:#7a4baa;\n    style v153 fill:#e3dcea,stroke:#7a4baa;\n    style v163 fill:#e3dcea,stroke:#7a4baa;\n    style v169 fill:#e3dcea,stroke:#7a4baa;\n    style v177 fill:#e3dcea,stroke:#7a4baa;\n    style v184 fill:#e3dcea,stroke:#7a4baa;\n    style v194 fill:#e3dcea,stroke:#7a4baa;\n    style v200 fill:#e3dcea,stroke:#7a4baa;\n    style v208 fill:#e3dcea,stroke:#7a4baa;\n    style v215 fill:#e3dcea,stroke:#7a4baa;\n    style v225 fill:#e3dcea,stroke:#7a4baa;\n    style v231 fill:#e3dcea,stroke:#7a4baa;\n    style v261 fill:#e3dcea,stroke:#7a4baa;\n    style v239 fill:#e3dcea,stroke:#7a4baa;\n    style v246 fill:#e3dcea,stroke:#7a4baa;\n    style v256 fill:#e3dcea,stroke:#7a4baa;\n    style v268 fill:#e3dcea,stroke:#7a4baa;\n    style v275 fill:#e3dcea,stroke:#7a4baa;\n    style v287 fill:#e3dcea,stroke:#7a4baa;\n    style v294 fill:#e3dcea,stroke:#7a4baa;\n    style v298 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Rna",
      "Rna singlesample"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html",
    "href": "components/workflows/multiomics/dimensionality_reduction.html",
    "title": "Dimensionality reduction",
    "section": "",
    "text": "ID: dimensionality_reduction\nNamespace: workflows/multiomics\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html#example-commands",
    "href": "components/workflows/multiomics/dimensionality_reduction.html#example-commands",
    "title": "Dimensionality reduction",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/multiomics/dimensionality_reduction/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# PCA options\nobsm_pca: \"X_pca\"\n# var_pca_feature_selection: \"foo\"\n# pca_loadings_varm_output: \"foo\"\n# pca_variance_uns_output: \"foo\"\npca_overwrite: false\n\n# Neighbour calculation\nuns_neighbors: \"neighbors\"\nobsp_neighbor_distances: \"distances\"\nobsp_neighbor_connectivities: \"connectivities\"\n\n# Umap options\nobsm_umap: \"X_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/multiomics/dimensionality_reduction/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html#argument-groups",
    "href": "components/workflows/multiomics/dimensionality_reduction.html#argument-groups",
    "title": "Dimensionality reduction",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nPCA options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_pca\nIn which .obsm slot to store the resulting PCA embedding.\nstring, default: \"X_pca\"\n\n\n--var_pca_feature_selection\nColumn name in .var matrix that will be used to select which genes to run the PCA on.\nstring\n\n\n--pca_loadings_varm_output\nName of the .varm key where the PCA loadings are stored.\nstring\n\n\n--pca_variance_uns_output\nName of the .uns key where the variance and variance ratio will be stored as a map. The map will contain two keys: variance and variance_ratio respectively.\nstring\n\n\n--pca_overwrite\nAllow overwriting slots for PCA output.\nboolean_true\n\n\n\n\n\nNeighbour calculation\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"neighbors\"\n\n\n--obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"distances\"\n\n\n--obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"connectivities\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html#authors",
    "href": "components/workflows/multiomics/dimensionality_reduction.html#authors",
    "title": "Dimensionality reduction",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/dimensionality_reduction.html#visualisation",
    "href": "components/workflows/multiomics/dimensionality_reduction.html#visualisation",
    "title": "Dimensionality reduction",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(pca)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v49(find_neighbors)\n    v56(cross)\n    v66(cross)\n    v72(filter)\n    v102(concat)\n    v80(umap)\n    v87(cross)\n    v97(cross)\n    v109(cross)\n    v116(cross)\n    v128(cross)\n    v135(cross)\n    v139(Output)\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v72--&gt;v80\n    v80--&gt;v87\n    v72--&gt;v87\n    v72--&gt;v97\n    v97--&gt;v102\n    v102--&gt;v109\n    v2--&gt;v109\n    v109--&gt;v116\n    v2--&gt;v116\n    v2--&gt;v128\n    v128--&gt;v135\n    v2--&gt;v135\n    v135--&gt;v139\n    v35--&gt;v41\n    v18--&gt;v35\n    v66--&gt;v72\n    v49--&gt;v66\n    v80--&gt;v97\n    v102--&gt;v128\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v72 fill:#e3dcea,stroke:#7a4baa;\n    style v102 fill:#e3dcea,stroke:#7a4baa;\n    style v80 fill:#e3dcea,stroke:#7a4baa;\n    style v87 fill:#e3dcea,stroke:#7a4baa;\n    style v97 fill:#e3dcea,stroke:#7a4baa;\n    style v109 fill:#e3dcea,stroke:#7a4baa;\n    style v116 fill:#e3dcea,stroke:#7a4baa;\n    style v128 fill:#e3dcea,stroke:#7a4baa;\n    style v135 fill:#e3dcea,stroke:#7a4baa;\n    style v139 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Dimensionality reduction"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html",
    "href": "components/workflows/multiomics/process_batches.html",
    "title": "Process batches",
    "section": "",
    "text": "ID: process_batches\nNamespace: workflows/multiomics\n\n\n\nSource\nAn input .h5mu file will first be split in order to run the multisample processing per modality. Next, the modalities are merged again and the integration setup pipeline is executed. Please note that this workflow assumes that samples from multiple pipelines are already concatenated.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html#example-commands",
    "href": "components/workflows/multiomics/process_batches.html#example-commands",
    "title": "Process batches",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/multiomics/process_batches/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: [\"input.h5mu\"]\n# rna_layer: \"foo\"\n# prot_layer: \"foo\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Highly variable features detection\nhighly_variable_features_var_output: \"filter_with_hvg\"\nhighly_variable_features_obs_batch_key: \"sample_id\"\n\n# QC metrics calculation options\nvar_qc_metrics: [\"filter_with_hvg\"]\ntop_n_vars: [50, 100, 200, 500]\n\n# PCA options\npca_overwrite: false\n\n# CLR options\nclr_axis: 0\n\n# RNA Scaling options\nrna_enable_scaling: false\nrna_scaling_output_layer: \"scaled\"\nrna_scaling_pca_obsm_output: \"scaled_pca\"\nrna_scaling_pca_loadings_varm_output: \"scaled_pca_loadings\"\nrna_scaling_pca_variance_uns_output: \"scaled_pca_variance\"\nrna_scaling_umap_obsm_output: \"scaled_umap\"\n# rna_scaling_max_value: 123.0\nrna_scaling_zero_center: true\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/multiomics/process_batches/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html#argument-groups",
    "href": "components/workflows/multiomics/process_batches.html#argument-groups",
    "title": "Process batches",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nList of file, required, example: \"input.h5mu\", multiple_sep: \";\"\n\n\n--rna_layer\nInput layer for the gene expression modality. If not specified, .X is used.\nstring\n\n\n--prot_layer\nInput layer for the antibody capture modality. If not specified, .X is used.\nstring\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nHighly variable features detection\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--highly_variable_features_var_output\nIn which .var slot to store a boolean array corresponding to the highly variable genes.\nstring, default: \"filter_with_hvg\"\n\n\n--highly_variable_features_obs_batch_key\nIf specified, highly-variable genes are selected within each batch separately and merged. This simple process avoids the selection of batch-specific genes and acts as a lightweight batch correction method.\nstring, default: \"sample_id\"\n\n\n\n\n\nQC metrics calculation options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--var_qc_metrics\nKeys to select a boolean (containing only True or False) column from .var. For each cell, calculate the proportion of total values for genes which are labeled ‘True’, compared to the total sum of the values for all genes.\nList of string, default: \"filter_with_hvg\", example: \"ercc,highly_variable\", multiple_sep: \",\"\n\n\n--top_n_vars\nNumber of top vars to be used to calculate cumulative proportions. If not specified, proportions are not calculated. --top_n_vars 20,50 finds cumulative proportion to the 20th and 50th most expressed vars.\nList of integer, default: 50, 100, 200, 500, multiple_sep: \",\"\n\n\n\n\n\nPCA options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--pca_overwrite\nAllow overwriting slots for PCA output.\nboolean_true\n\n\n\n\n\nCLR options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--clr_axis\nAxis to perform the CLR transformation on.\ninteger, default: 0\n\n\n\n\n\nRNA Scaling options\nOptions for enabling scaling of the log-normalized data to unit variance and zero mean. The scaled data will be output a different layer and representation with reduced dimensions will be created and stored in addition to the non-scaled data.\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_enable_scaling\nEnable scaling for the RNA modality.\nboolean_true\n\n\n--rna_scaling_output_layer\nOutput layer where the scaled log-normalized data will be stored.\nstring, default: \"scaled\"\n\n\n--rna_scaling_pca_obsm_output\nName of the .obsm key where the PCA representation of the log-normalized and scaled data is stored.\nstring, default: \"scaled_pca\"\n\n\n--rna_scaling_pca_loadings_varm_output\nName of the .varm key where the PCA loadings of the log-normalized and scaled data is stored.\nstring, default: \"scaled_pca_loadings\"\n\n\n--rna_scaling_pca_variance_uns_output\nName of the .uns key where the variance and variance ratio will be stored as a map. The map will contain two keys: variance and variance_ratio respectively.\nstring, default: \"scaled_pca_variance\"\n\n\n--rna_scaling_umap_obsm_output\nName of the .obsm key where the UMAP representation of the log-normalized and scaled data is stored.\nstring, default: \"scaled_umap\"\n\n\n--rna_scaling_max_value\nClip (truncate) data to this value after scaling. If not specified, do not clip.\ndouble\n\n\n--rna_scaling_zero_center\nIf set, omit zero-centering variables, which allows to handle sparse input efficiently.”\nboolean_false",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html#authors",
    "href": "components/workflows/multiomics/process_batches.html#authors",
    "title": "Process batches",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/multiomics/process_batches.html#visualisation",
    "href": "components/workflows/multiomics/process_batches.html#visualisation",
    "title": "Process batches",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v11(filter)\n    v22(filter)\n    v34(cross)\n    v44(cross)\n    v54(concat)\n    v61(cross)\n    v71(cross)\n    v77(flatMap)\n    v85(filter)\n    v93(filter)\n    v108(cross)\n    v118(cross)\n    v124(filter)\n    v139(cross)\n    v149(cross)\n    v155(filter)\n    v170(cross)\n    v180(cross)\n    v189(branch)\n    v216(concat)\n    v201(cross)\n    v211(cross)\n    v217(filter)\n    v232(cross)\n    v242(cross)\n    v248(filter)\n    v374(concat)\n    v258(filter)\n    v273(cross)\n    v283(cross)\n    v290(mix)\n    v291(filter)\n    v306(cross)\n    v316(cross)\n    v322(filter)\n    v352(concat)\n    v337(cross)\n    v347(cross)\n    v359(cross)\n    v369(cross)\n    v381(cross)\n    v391(cross)\n    v592(mix)\n    v402(filter)\n    v410(filter)\n    v425(cross)\n    v435(cross)\n    v441(filter)\n    v567(concat)\n    v451(filter)\n    v466(cross)\n    v476(cross)\n    v483(mix)\n    v484(filter)\n    v499(cross)\n    v509(cross)\n    v515(filter)\n    v545(concat)\n    v530(cross)\n    v540(cross)\n    v552(cross)\n    v562(cross)\n    v574(cross)\n    v584(cross)\n    v594(mix)\n    v602(filter)\n    v610(merge)\n    v617(cross)\n    v627(cross)\n    v637(branch)\n    v756(concat)\n    v642(filter)\n    v657(cross)\n    v667(cross)\n    v673(filter)\n    v688(cross)\n    v698(cross)\n    v704(filter)\n    v734(concat)\n    v719(cross)\n    v729(cross)\n    v741(cross)\n    v751(cross)\n    v760(branch)\n    v879(concat)\n    v765(filter)\n    v780(cross)\n    v790(cross)\n    v796(filter)\n    v811(cross)\n    v821(cross)\n    v827(filter)\n    v857(concat)\n    v842(cross)\n    v852(cross)\n    v864(cross)\n    v874(cross)\n    v883(branch)\n    v1002(concat)\n    v888(filter)\n    v903(cross)\n    v913(cross)\n    v919(filter)\n    v934(cross)\n    v944(cross)\n    v950(filter)\n    v980(concat)\n    v965(cross)\n    v975(cross)\n    v987(cross)\n    v997(cross)\n    v1003(filter)\n    v1033(concat)\n    v1011(publish)\n    v1018(cross)\n    v1028(cross)\n    v1040(cross)\n    v1047(cross)\n    v1059(cross)\n    v1066(cross)\n    v1070(Output)\n    subgraph group_split_modalities_workflow [split_modalities_workflow]\n        v27(split_modalities_component)\n    end\n    subgraph group_rna_multisample [rna_multisample]\n        v101(normalize_total)\n        v132(log1p)\n        v163(delete_layer)\n        v194(scale)\n        v225(highly_variable_features_scanpy)\n        v266(grep_annotation_column)\n        v299(calculate_qc_metrics)\n        v330(publish)\n    end\n    subgraph group_prot_multisample [prot_multisample]\n        v418(clr)\n        v459(grep_annotation_column)\n        v492(calculate_qc_metrics)\n        v523(publish)\n    end\n    subgraph group_dimensionality_reduction_rna [dimensionality_reduction_rna]\n        v650(pca)\n        v681(find_neighbors)\n        v712(umap)\n    end\n    subgraph group_dimensionality_reduction_scaling_rna [dimensionality_reduction_scaling_rna]\n        v773(pca)\n        v804(find_neighbors)\n        v835(umap)\n    end\n    subgraph group_dimensionality_reduction_prot [dimensionality_reduction_prot]\n        v896(pca)\n        v927(find_neighbors)\n        v958(umap)\n    end\n    v189--&gt;v216\n    v216--&gt;v217\n    v290--&gt;v291\n    v483--&gt;v484\n    v592--&gt;v594\n    v637--&gt;v756\n    v760--&gt;v879\n    v883--&gt;v1002\n    v1002--&gt;v1003\n    v0--&gt;v2\n    v22--&gt;v27\n    v27--&gt;v34\n    v22--&gt;v34\n    v22--&gt;v44\n    v54--&gt;v61\n    v11--&gt;v61\n    v11--&gt;v71\n    v85--&gt;v93\n    v93--&gt;v101\n    v101--&gt;v108\n    v93--&gt;v108\n    v93--&gt;v118\n    v124--&gt;v132\n    v132--&gt;v139\n    v124--&gt;v139\n    v124--&gt;v149\n    v155--&gt;v163\n    v163--&gt;v170\n    v155--&gt;v170\n    v155--&gt;v180\n    v189--&gt;v194\n    v194--&gt;v201\n    v189--&gt;v201\n    v189--&gt;v211\n    v211--&gt;v216\n    v217--&gt;v225\n    v225--&gt;v232\n    v217--&gt;v232\n    v217--&gt;v242\n    v258--&gt;v266\n    v266--&gt;v273\n    v258--&gt;v273\n    v258--&gt;v283\n    v291--&gt;v299\n    v299--&gt;v306\n    v291--&gt;v306\n    v291--&gt;v316\n    v322--&gt;v330\n    v330--&gt;v337\n    v322--&gt;v337\n    v322--&gt;v347\n    v347--&gt;v352\n    v352--&gt;v359\n    v248--&gt;v359\n    v248--&gt;v369\n    v369--&gt;v374\n    v374--&gt;v381\n    v85--&gt;v381\n    v85--&gt;v391\n    v402--&gt;v410\n    v410--&gt;v418\n    v418--&gt;v425\n    v410--&gt;v425\n    v410--&gt;v435\n    v451--&gt;v459\n    v459--&gt;v466\n    v451--&gt;v466\n    v451--&gt;v476\n    v484--&gt;v492\n    v492--&gt;v499\n    v484--&gt;v499\n    v484--&gt;v509\n    v515--&gt;v523\n    v523--&gt;v530\n    v515--&gt;v530\n    v515--&gt;v540\n    v540--&gt;v545\n    v545--&gt;v552\n    v441--&gt;v552\n    v441--&gt;v562\n    v562--&gt;v567\n    v567--&gt;v574\n    v402--&gt;v574\n    v402--&gt;v584\n    v602--&gt;v610\n    v610--&gt;v617\n    v602--&gt;v617\n    v602--&gt;v627\n    v637--&gt;v642\n    v642--&gt;v650\n    v650--&gt;v657\n    v642--&gt;v657\n    v642--&gt;v667\n    v673--&gt;v681\n    v681--&gt;v688\n    v673--&gt;v688\n    v673--&gt;v698\n    v704--&gt;v712\n    v712--&gt;v719\n    v704--&gt;v719\n    v704--&gt;v729\n    v729--&gt;v734\n    v734--&gt;v741\n    v637--&gt;v741\n    v637--&gt;v751\n    v751--&gt;v756\n    v760--&gt;v765\n    v765--&gt;v773\n    v773--&gt;v780\n    v765--&gt;v780\n    v765--&gt;v790\n    v796--&gt;v804\n    v804--&gt;v811\n    v796--&gt;v811\n    v796--&gt;v821\n    v827--&gt;v835\n    v835--&gt;v842\n    v827--&gt;v842\n    v827--&gt;v852\n    v852--&gt;v857\n    v857--&gt;v864\n    v760--&gt;v864\n    v760--&gt;v874\n    v874--&gt;v879\n    v883--&gt;v888\n    v888--&gt;v896\n    v896--&gt;v903\n    v888--&gt;v903\n    v888--&gt;v913\n    v919--&gt;v927\n    v927--&gt;v934\n    v919--&gt;v934\n    v919--&gt;v944\n    v950--&gt;v958\n    v958--&gt;v965\n    v950--&gt;v965\n    v950--&gt;v975\n    v975--&gt;v980\n    v980--&gt;v987\n    v883--&gt;v987\n    v883--&gt;v997\n    v997--&gt;v1002\n    v1003--&gt;v1011\n    v1011--&gt;v1018\n    v1003--&gt;v1018\n    v1003--&gt;v1028\n    v1028--&gt;v1033\n    v1033--&gt;v1040\n    v2--&gt;v1040\n    v1040--&gt;v1047\n    v2--&gt;v1047\n    v2--&gt;v1059\n    v1059--&gt;v1066\n    v2--&gt;v1066\n    v1066--&gt;v1070\n    v2--&gt;v11\n    v71--&gt;v77\n    v11--&gt;v22\n    v44--&gt;v54\n    v27--&gt;v44\n    v54--&gt;v71\n    v77--&gt;v85\n    v391--&gt;v592\n    v118--&gt;v124\n    v101--&gt;v118\n    v149--&gt;v155\n    v132--&gt;v149\n    v163--&gt;v180\n    v180--&gt;v189\n    v194--&gt;v211\n    v242--&gt;v248\n    v225--&gt;v242\n    v248--&gt;v258\n    v283--&gt;v290\n    v266--&gt;v283\n    v248--&gt;v290\n    v316--&gt;v322\n    v299--&gt;v316\n    v330--&gt;v347\n    v352--&gt;v369\n    v374--&gt;v391\n    v77--&gt;v402\n    v584--&gt;v592\n    v435--&gt;v441\n    v418--&gt;v435\n    v441--&gt;v451\n    v476--&gt;v483\n    v459--&gt;v476\n    v441--&gt;v483\n    v509--&gt;v515\n    v492--&gt;v509\n    v523--&gt;v540\n    v545--&gt;v562\n    v567--&gt;v584\n    v77--&gt;v594\n    v594--&gt;v602\n    v610--&gt;v627\n    v627--&gt;v637\n    v667--&gt;v673\n    v650--&gt;v667\n    v698--&gt;v704\n    v681--&gt;v698\n    v712--&gt;v729\n    v734--&gt;v751\n    v756--&gt;v760\n    v790--&gt;v796\n    v773--&gt;v790\n    v821--&gt;v827\n    v804--&gt;v821\n    v835--&gt;v852\n    v857--&gt;v874\n    v879--&gt;v883\n    v913--&gt;v919\n    v896--&gt;v913\n    v944--&gt;v950\n    v927--&gt;v944\n    v958--&gt;v975\n    v980--&gt;v997\n    v1011--&gt;v1028\n    v1033--&gt;v1059\n    style group_split_modalities_workflow fill:#F0F0F0,stroke:#969696;\n    style group_rna_multisample fill:#F0F0F0,stroke:#969696;\n    style group_prot_multisample fill:#F0F0F0,stroke:#969696;\n    style group_dimensionality_reduction_rna fill:#F0F0F0,stroke:#969696;\n    style group_dimensionality_reduction_scaling_rna fill:#F0F0F0,stroke:#969696;\n    style group_dimensionality_reduction_prot fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v11 fill:#e3dcea,stroke:#7a4baa;\n    style v22 fill:#e3dcea,stroke:#7a4baa;\n    style v27 fill:#e3dcea,stroke:#7a4baa;\n    style v34 fill:#e3dcea,stroke:#7a4baa;\n    style v44 fill:#e3dcea,stroke:#7a4baa;\n    style v54 fill:#e3dcea,stroke:#7a4baa;\n    style v61 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v77 fill:#e3dcea,stroke:#7a4baa;\n    style v85 fill:#e3dcea,stroke:#7a4baa;\n    style v93 fill:#e3dcea,stroke:#7a4baa;\n    style v101 fill:#e3dcea,stroke:#7a4baa;\n    style v108 fill:#e3dcea,stroke:#7a4baa;\n    style v118 fill:#e3dcea,stroke:#7a4baa;\n    style v124 fill:#e3dcea,stroke:#7a4baa;\n    style v132 fill:#e3dcea,stroke:#7a4baa;\n    style v139 fill:#e3dcea,stroke:#7a4baa;\n    style v149 fill:#e3dcea,stroke:#7a4baa;\n    style v155 fill:#e3dcea,stroke:#7a4baa;\n    style v163 fill:#e3dcea,stroke:#7a4baa;\n    style v170 fill:#e3dcea,stroke:#7a4baa;\n    style v180 fill:#e3dcea,stroke:#7a4baa;\n    style v189 fill:#e3dcea,stroke:#7a4baa;\n    style v216 fill:#e3dcea,stroke:#7a4baa;\n    style v194 fill:#e3dcea,stroke:#7a4baa;\n    style v201 fill:#e3dcea,stroke:#7a4baa;\n    style v211 fill:#e3dcea,stroke:#7a4baa;\n    style v217 fill:#e3dcea,stroke:#7a4baa;\n    style v225 fill:#e3dcea,stroke:#7a4baa;\n    style v232 fill:#e3dcea,stroke:#7a4baa;\n    style v242 fill:#e3dcea,stroke:#7a4baa;\n    style v248 fill:#e3dcea,stroke:#7a4baa;\n    style v374 fill:#e3dcea,stroke:#7a4baa;\n    style v258 fill:#e3dcea,stroke:#7a4baa;\n    style v266 fill:#e3dcea,stroke:#7a4baa;\n    style v273 fill:#e3dcea,stroke:#7a4baa;\n    style v283 fill:#e3dcea,stroke:#7a4baa;\n    style v290 fill:#e3dcea,stroke:#7a4baa;\n    style v291 fill:#e3dcea,stroke:#7a4baa;\n    style v299 fill:#e3dcea,stroke:#7a4baa;\n    style v306 fill:#e3dcea,stroke:#7a4baa;\n    style v316 fill:#e3dcea,stroke:#7a4baa;\n    style v322 fill:#e3dcea,stroke:#7a4baa;\n    style v352 fill:#e3dcea,stroke:#7a4baa;\n    style v330 fill:#e3dcea,stroke:#7a4baa;\n    style v337 fill:#e3dcea,stroke:#7a4baa;\n    style v347 fill:#e3dcea,stroke:#7a4baa;\n    style v359 fill:#e3dcea,stroke:#7a4baa;\n    style v369 fill:#e3dcea,stroke:#7a4baa;\n    style v381 fill:#e3dcea,stroke:#7a4baa;\n    style v391 fill:#e3dcea,stroke:#7a4baa;\n    style v592 fill:#e3dcea,stroke:#7a4baa;\n    style v402 fill:#e3dcea,stroke:#7a4baa;\n    style v410 fill:#e3dcea,stroke:#7a4baa;\n    style v418 fill:#e3dcea,stroke:#7a4baa;\n    style v425 fill:#e3dcea,stroke:#7a4baa;\n    style v435 fill:#e3dcea,stroke:#7a4baa;\n    style v441 fill:#e3dcea,stroke:#7a4baa;\n    style v567 fill:#e3dcea,stroke:#7a4baa;\n    style v451 fill:#e3dcea,stroke:#7a4baa;\n    style v459 fill:#e3dcea,stroke:#7a4baa;\n    style v466 fill:#e3dcea,stroke:#7a4baa;\n    style v476 fill:#e3dcea,stroke:#7a4baa;\n    style v483 fill:#e3dcea,stroke:#7a4baa;\n    style v484 fill:#e3dcea,stroke:#7a4baa;\n    style v492 fill:#e3dcea,stroke:#7a4baa;\n    style v499 fill:#e3dcea,stroke:#7a4baa;\n    style v509 fill:#e3dcea,stroke:#7a4baa;\n    style v515 fill:#e3dcea,stroke:#7a4baa;\n    style v545 fill:#e3dcea,stroke:#7a4baa;\n    style v523 fill:#e3dcea,stroke:#7a4baa;\n    style v530 fill:#e3dcea,stroke:#7a4baa;\n    style v540 fill:#e3dcea,stroke:#7a4baa;\n    style v552 fill:#e3dcea,stroke:#7a4baa;\n    style v562 fill:#e3dcea,stroke:#7a4baa;\n    style v574 fill:#e3dcea,stroke:#7a4baa;\n    style v584 fill:#e3dcea,stroke:#7a4baa;\n    style v594 fill:#e3dcea,stroke:#7a4baa;\n    style v602 fill:#e3dcea,stroke:#7a4baa;\n    style v610 fill:#e3dcea,stroke:#7a4baa;\n    style v617 fill:#e3dcea,stroke:#7a4baa;\n    style v627 fill:#e3dcea,stroke:#7a4baa;\n    style v637 fill:#e3dcea,stroke:#7a4baa;\n    style v756 fill:#e3dcea,stroke:#7a4baa;\n    style v642 fill:#e3dcea,stroke:#7a4baa;\n    style v650 fill:#e3dcea,stroke:#7a4baa;\n    style v657 fill:#e3dcea,stroke:#7a4baa;\n    style v667 fill:#e3dcea,stroke:#7a4baa;\n    style v673 fill:#e3dcea,stroke:#7a4baa;\n    style v681 fill:#e3dcea,stroke:#7a4baa;\n    style v688 fill:#e3dcea,stroke:#7a4baa;\n    style v698 fill:#e3dcea,stroke:#7a4baa;\n    style v704 fill:#e3dcea,stroke:#7a4baa;\n    style v734 fill:#e3dcea,stroke:#7a4baa;\n    style v712 fill:#e3dcea,stroke:#7a4baa;\n    style v719 fill:#e3dcea,stroke:#7a4baa;\n    style v729 fill:#e3dcea,stroke:#7a4baa;\n    style v741 fill:#e3dcea,stroke:#7a4baa;\n    style v751 fill:#e3dcea,stroke:#7a4baa;\n    style v760 fill:#e3dcea,stroke:#7a4baa;\n    style v879 fill:#e3dcea,stroke:#7a4baa;\n    style v765 fill:#e3dcea,stroke:#7a4baa;\n    style v773 fill:#e3dcea,stroke:#7a4baa;\n    style v780 fill:#e3dcea,stroke:#7a4baa;\n    style v790 fill:#e3dcea,stroke:#7a4baa;\n    style v796 fill:#e3dcea,stroke:#7a4baa;\n    style v804 fill:#e3dcea,stroke:#7a4baa;\n    style v811 fill:#e3dcea,stroke:#7a4baa;\n    style v821 fill:#e3dcea,stroke:#7a4baa;\n    style v827 fill:#e3dcea,stroke:#7a4baa;\n    style v857 fill:#e3dcea,stroke:#7a4baa;\n    style v835 fill:#e3dcea,stroke:#7a4baa;\n    style v842 fill:#e3dcea,stroke:#7a4baa;\n    style v852 fill:#e3dcea,stroke:#7a4baa;\n    style v864 fill:#e3dcea,stroke:#7a4baa;\n    style v874 fill:#e3dcea,stroke:#7a4baa;\n    style v883 fill:#e3dcea,stroke:#7a4baa;\n    style v1002 fill:#e3dcea,stroke:#7a4baa;\n    style v888 fill:#e3dcea,stroke:#7a4baa;\n    style v896 fill:#e3dcea,stroke:#7a4baa;\n    style v903 fill:#e3dcea,stroke:#7a4baa;\n    style v913 fill:#e3dcea,stroke:#7a4baa;\n    style v919 fill:#e3dcea,stroke:#7a4baa;\n    style v927 fill:#e3dcea,stroke:#7a4baa;\n    style v934 fill:#e3dcea,stroke:#7a4baa;\n    style v944 fill:#e3dcea,stroke:#7a4baa;\n    style v950 fill:#e3dcea,stroke:#7a4baa;\n    style v980 fill:#e3dcea,stroke:#7a4baa;\n    style v958 fill:#e3dcea,stroke:#7a4baa;\n    style v965 fill:#e3dcea,stroke:#7a4baa;\n    style v975 fill:#e3dcea,stroke:#7a4baa;\n    style v987 fill:#e3dcea,stroke:#7a4baa;\n    style v997 fill:#e3dcea,stroke:#7a4baa;\n    style v1003 fill:#e3dcea,stroke:#7a4baa;\n    style v1033 fill:#e3dcea,stroke:#7a4baa;\n    style v1011 fill:#e3dcea,stroke:#7a4baa;\n    style v1018 fill:#e3dcea,stroke:#7a4baa;\n    style v1028 fill:#e3dcea,stroke:#7a4baa;\n    style v1040 fill:#e3dcea,stroke:#7a4baa;\n    style v1047 fill:#e3dcea,stroke:#7a4baa;\n    style v1059 fill:#e3dcea,stroke:#7a4baa;\n    style v1066 fill:#e3dcea,stroke:#7a4baa;\n    style v1070 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Multiomics",
      "Process batches"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html",
    "href": "components/workflows/prot/prot_singlesample.html",
    "title": "Prot singlesample",
    "section": "",
    "text": "ID: prot_singlesample\nNamespace: workflows/prot\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html#example-commands",
    "href": "components/workflows/prot/prot_singlesample.html#example-commands",
    "title": "Prot singlesample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/prot/prot_singlesample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Input\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\n\n# Output\n# output: \"$id.$key.output.h5mu\"\n\n# Filtering options\n# min_counts: 200\n# max_counts: 5000000\n# min_proteins_per_cell: 200\n# max_proteins_per_cell: 1500000\n# min_cells_per_protein: 3\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/prot/prot_singlesample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html#argument-groups",
    "href": "components/workflows/prot/prot_singlesample.html#argument-groups",
    "title": "Prot singlesample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nInput layer to start from. By default, .X will be used.\nstring\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nFiltering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--min_counts\nMinimum number of counts captured per cell.\ninteger, example: 200\n\n\n--max_counts\nMaximum number of counts captured per cell.\ninteger, example: 5000000\n\n\n--min_proteins_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--max_proteins_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 1500000\n\n\n--min_cells_per_protein\nMinimum of non-zero values per gene.\ninteger, example: 3",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html#authors",
    "href": "components/workflows/prot/prot_singlesample.html#authors",
    "title": "Prot singlesample",
    "section": "Authors",
    "text": "Authors\n\nDries De Maeyer   (author)\nRobrecht Cannoodt    (author, maintainer)\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/prot/prot_singlesample.html#visualisation",
    "href": "components/workflows/prot/prot_singlesample.html#visualisation",
    "title": "Prot singlesample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(prot_filter_with_counts)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v71(concat)\n    v49(prot_do_filter)\n    v56(cross)\n    v66(cross)\n    v78(cross)\n    v85(cross)\n    v97(cross)\n    v104(cross)\n    v108(Output)\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v66--&gt;v71\n    v71--&gt;v78\n    v2--&gt;v78\n    v78--&gt;v85\n    v2--&gt;v85\n    v2--&gt;v97\n    v97--&gt;v104\n    v2--&gt;v104\n    v104--&gt;v108\n    v35--&gt;v41\n    v18--&gt;v35\n    v49--&gt;v66\n    v71--&gt;v97\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v78 fill:#e3dcea,stroke:#7a4baa;\n    style v85 fill:#e3dcea,stroke:#7a4baa;\n    style v97 fill:#e3dcea,stroke:#7a4baa;\n    style v104 fill:#e3dcea,stroke:#7a4baa;\n    style v108 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Prot",
      "Prot singlesample"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html",
    "href": "components/workflows/ingestion/bd_rhapsody.html",
    "title": "BD Rhapsody",
    "section": "",
    "text": "ID: bd_rhapsody\nNamespace: workflows/ingestion\n\n\n\nSource\nThis pipeline performs analysis of single-cell multiomic sequence read (FASTQ) data. The supported sequencing libraries are those generated by the BD Rhapsody assay kits, including: Whole Transcriptome mRNA, Targeted mRNA, AbSeq Antibody-Oligonucleotides, Single-Cell Multiplexing, TCR/BCR, and ATAC-Seq\nThe CWL pipeline file is obtained by cloning ‘https://bitbucket.org/CRSwDev/cwl’ and removing all objects with class ‘DockerRequirement’ from the YAML.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html#example-commands",
    "href": "components/workflows/ingestion/bd_rhapsody.html#example-commands",
    "title": "BD Rhapsody",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/ingestion/bd_rhapsody/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\n# reads: [\"WTALibrary_S1_L001_R1_001.fastq.gz\", \"WTALibrary_S1_L001_R2_001.fastq.gz\"]\n# reads_atac: [\"ATACLibrary_S2_L001_R1_001.fastq.gz\", \"ATACLibrary_S2_L001_R2_001.fastq.gz\", \"ATACLibrary_S2_L001_I2_001.fastq.gz\"]\n\n# References\n# reference_archive: \"RhapRef_Human_WTA_2023-02.tar.gz\"\n# targeted_reference: [\"BD_Rhapsody_Immune_Response_Panel_Hs.fasta\"]\n# abseq_reference: [\"AbSeq_reference.fasta\"]\n# supplemental_reference: [\"supplemental_reference.fasta\"]\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n# output_raw: \"$id.$key.output_raw\"\n\n# Putative Cell Calling Settings\n# cell_calling_data: \"mRNA\"\n# cell_calling_bioproduct_algorithm: \"Basic\"\n# cell_calling_atac_algorithm: \"Basic\"\n# exact_cell_count: 10000\n# expected_cell_count: 20000\n\n# Intronic Reads Settings\n# exclude_intronic_reads: false\n\n# Multiplex Settings\n# sample_tags_version: \"human\"\n# tag_names: [\"4-mySample\", \"9-myOtherSample\", \"6-alsoThisSample\"]\n\n# VDJ arguments\n# vdj_version: \"human\"\n\n# ATAC options\n# predefined_atac_peaks: \"predefined_peaks.bed\"\n\n# Additional options\nrun_name: \"sample\"\ngenerate_bam: false\n# long_reads: true\n\n# Advanced options\n# custom_star_params: \"--alignIntronMax 6000 --outFilterScoreMinOverLread 0.1 --limitOutSJcollapsed 2000000\"\n# custom_bwa_mem2_params: \"-k 16 -w 200 -r\"\n\n# CWL-runner arguments\nparallel: true\ntimestamps: false\n\n# Undocumented arguments\n# abseq_umi: 123\n# target_analysis: true\n# vdj_jgene_evalue: 123.0\n# vdj_vgene_evalue: 123.0\n# write_filtered_reads: true\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/bd_rhapsody/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html#argument-groups",
    "href": "components/workflows/ingestion/bd_rhapsody.html#argument-groups",
    "title": "BD Rhapsody",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--reads\nReads (optional) - Path to your FASTQ.GZ formatted read files from libraries that may include: - WTA mRNA - Targeted mRNA - AbSeq - Sample Multiplexing - VDJ You may specify as many R1/R2 read pairs as you want.\nList of file, example: \"WTALibrary_S1_L001_R1_001.fastq.gz\", \"WTALibrary_S1_L001_R2_001.fastq.gz\", multiple_sep: \";\"\n\n\n--reads_atac\nPath to your FASTQ.GZ formatted read files from ATAC-Seq libraries. You may specify as many R1/R2/I2 files as you want.\nList of file, example: \"ATACLibrary_S2_L001_R1_001.fastq.gz\", \"ATACLibrary_S2_L001_R2_001.fastq.gz\", \"ATACLibrary_S2_L001_I2_001.fastq.gz\", multiple_sep: \";\"\n\n\n\n\n\nReferences\nAssay type will be inferred from the provided reference(s). Do not provide both reference_archive and targeted_reference at the same time.\nValid reference input combinations: - reference_archive: WTA only - reference_archive & abseq_reference: WTA + AbSeq - reference_archive & supplemental_reference: WTA + extra transgenes - reference_archive & abseq_reference & supplemental_reference: WTA + AbSeq + extra transgenes - reference_archive: WTA + ATAC or ATAC only - reference_archive & supplemental_reference: WTA + ATAC + extra transgenes - targeted_reference: Targeted only - targeted_reference & abseq_reference: Targeted + AbSeq - abseq_reference: AbSeq only\nThe reference_archive can be generated with the reference/build_bdrhap_reference component. Alternatively, BD also provides standard references which can be downloaded from these locations:\n\nHuman: https://bd-rhapsody-public.s3.amazonaws.com/Rhapsody-WTA/Pipeline-version2.x_WTA_references/RhapRef_Human_WTA_2023-02.tar.gz\nMouse: https://bd-rhapsody-public.s3.amazonaws.com/Rhapsody-WTA/Pipeline-version2.x_WTA_references/RhapRef_Mouse_WTA_2023-02.tar.gz\n\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--reference_archive\nPath to Rhapsody WTA Reference in the tar.gz format. Structure of the reference archive: - BD_Rhapsody_Reference_Files/: top level folder - star_index/: sub-folder containing STAR index, that is files created with STAR --runMode genomeGenerate - GTF for gene-transcript-annotation e.g. “gencode.v43.primary_assembly.annotation.gtf”\nfile, example: \"RhapRef_Human_WTA_2023-02.tar.gz\"\n\n\n--targeted_reference\nPath to the targeted reference file in FASTA format.\nList of file, example: \"BD_Rhapsody_Immune_Response_Panel_Hs.fasta\", multiple_sep: \";\"\n\n\n--abseq_reference\nPath to the AbSeq reference file in FASTA format. Only needed if BD AbSeq Ab-Oligos are used.\nList of file, example: \"AbSeq_reference.fasta\", multiple_sep: \";\"\n\n\n--supplemental_reference\nPath to the supplemental reference file in FASTA format. Only needed if there are additional transgene sequences to be aligned against in a WTA assay experiment.\nList of file, example: \"supplemental_reference.fasta\", multiple_sep: \";\"\n\n\n\n\n\nOutputs\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nThe processed output file in h5mu format.\nfile, required, example: \"output.h5mu\"\n\n\n--output_raw\nThe unprocessed output directory containing all the outputs from the pipeline.\nfile, required, example: \"output_dir\"\n\n\n\n\n\nPutative Cell Calling Settings\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--cell_calling_data\nSpecify the dataset to be used for putative cell calling: mRNA, AbSeq, ATAC, mRNA_and_ATAC For putative cell calling using an AbSeq dataset, please provide an AbSeq_Reference fasta file above. For putative cell calling using an ATAC dataset, please provide a WTA+ATAC-Seq Reference_Archive file above. The default data for putative cell calling, will be determined the following way: - If mRNA Reads and ATAC Reads exist: mRNA_and_ATAC - If only ATAC Reads exist: ATAC - Otherwise: mRNA\nstring, example: \"mRNA\"\n\n\n--cell_calling_bioproduct_algorithm\nSpecify the bioproduct algorithm to be used for putative cell calling: Basic or Refined By default, the Basic algorithm will be used for putative cell calling.\nstring, example: \"Basic\"\n\n\n--cell_calling_atac_algorithm\nSpecify the ATAC-seq algorithm to be used for putative cell calling: Basic or Refined By default, the Basic algorithm will be used for putative cell calling.\nstring, example: \"Basic\"\n\n\n--exact_cell_count\nSet a specific number of cells as putative, based on those with the highest error-corrected read count\ninteger, example: 10000\n\n\n--expected_cell_count\nGuide the basic putative cell calling algorithm by providing an estimate of the number of cells expected. Usually this can be the number of cells loaded into the Rhapsody cartridge. If there are multiple inflection points on the second derivative cumulative curve, this will ensure the one selected is near the expected.\ninteger, example: 20000\n\n\n\n\n\nIntronic Reads Settings\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--exclude_intronic_reads\nBy default, the flag is false, and reads aligned to exons and introns are considered and represented in molecule counts. When the flag is set to true, intronic reads will be excluded. The value can be true or false.\nboolean, example: FALSE\n\n\n\n\n\nMultiplex Settings\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--sample_tags_version\nSpecify the version of the Sample Tags used in the run: * If Sample Tag Multiplexing was done, specify the appropriate version: human, mouse, flex, nuclei_includes_mrna, nuclei_atac_only * If this is an SMK + Nuclei mRNA run or an SMK + Multiomic ATAC-Seq (WTA+ATAC-Seq) run (and not an SMK + ATAC-Seq only run), choose the “nuclei_includes_mrna” option. * If this is an SMK + ATAC-Seq only run (and not SMK + Multiomic ATAC-Seq (WTA+ATAC-Seq)), choose the “nuclei_atac_only” option.\nstring, example: \"human\"\n\n\n--tag_names\nSpecify the tag number followed by ‘-’ and the desired sample name to appear in Sample_Tag_Metrics.csv Do not use the special characters.\nList of string, example: \"4-mySample\", \"9-myOtherSample\", \"6-alsoThisSample\", multiple_sep: \";\"\n\n\n\n\n\nVDJ arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--vdj_version\nIf VDJ was done, specify the appropriate option: human, mouse, humanBCR, humanTCR, mouseBCR, mouseTCR\nstring, example: \"human\"\n\n\n\n\n\nATAC options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--predefined_atac_peaks\nAn optional BED file containing pre-established chromatin accessibility peak regions for generating the ATAC cell-by-peak matrix.\nfile, example: \"predefined_peaks.bed\"\n\n\n\n\n\nAdditional options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--run_name\nSpecify a run name to use as the output file base name. Use only letters, numbers, or hyphens. Do not use special characters or spaces.\nstring, default: \"sample\"\n\n\n--generate_bam\nSpecify whether to create the BAM file output\nboolean, default: FALSE\n\n\n--long_reads\nUse STARlong (default: undefined - i.e. autodetects based on read lengths) - Specify if the STARlong aligner should be used instead of STAR. Set to true if the reads are longer than 650bp.\nboolean\n\n\n\n\n\nAdvanced options\nNOTE: Only change these if you are really sure about what you are doing\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--custom_star_params\nModify STAR alignment parameters - Set this parameter to fully override default STAR mapping parameters used in the pipeline. For reference this is the default that is used: Short Reads: --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --outFilterMultimapScoreRange 0 --clip3pAdapterSeq AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA --seedSearchStartLmax 50 --outFilterMatchNmin 25 --limitOutSJcollapsed 2000000 Long Reads: Same as Short Reads + --seedPerReadNmax 10000 This applies to fastqs provided in the Reads user input Do NOT set any non-mapping related params like --genomeDir, --outSAMtype, --outSAMunmapped, --readFilesIn, --runThreadN, etc. We use STAR version 2.7.10b\nstring, example: \"--alignIntronMax 6000 --outFilterScoreMinOverLread 0.1 --limitOutSJcollapsed 2000000\"\n\n\n--custom_bwa_mem2_params\nModify bwa-mem2 alignment parameters - Set this parameter to fully override bwa-mem2 mapping parameters used in the pipeline The pipeline does not specify any custom mapping params to bwa-mem2 so program default values are used This applies to fastqs provided in the Reads_ATAC user input Do NOT set any non-mapping related params like -C, -t, etc. We use bwa-mem2 version 2.2.1\nstring, example: \"-k 16 -w 200 -r\"\n\n\n\n\n\nCWL-runner arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--parallel\nRun jobs in parallel.\nboolean, default: TRUE\n\n\n--timestamps\nAdd timestamps to the errors, warnings, and notifications.\nboolean_true\n\n\n\n\n\nUndocumented arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--abseq_umi\n\ninteger\n\n\n--target_analysis\n\nboolean\n\n\n--vdj_jgene_evalue\ne-value threshold for J gene. The e-value threshold for J gene call by IgBlast/PyIR, default is set as 0.001\ndouble\n\n\n--vdj_vgene_evalue\ne-value threshold for V gene. The e-value threshold for V gene call by IgBlast/PyIR, default is set as 0.001\ndouble\n\n\n--write_filtered_reads\n\nboolean",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html#authors",
    "href": "components/workflows/ingestion/bd_rhapsody.html#authors",
    "title": "BD Rhapsody",
    "section": "Authors",
    "text": "Authors\n\nRobrecht Cannoodt    (maintainer)\nDorien Roosen   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/bd_rhapsody.html#visualisation",
    "href": "components/workflows/ingestion/bd_rhapsody.html#visualisation",
    "title": "BD Rhapsody",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v9(filter)\n    v17(bd_rhapsody_component)\n    v24(cross)\n    v34(cross)\n    v40(filter)\n    v70(concat)\n    v48(from_bdrhap_to_h5mu)\n    v55(cross)\n    v65(cross)\n    v77(cross)\n    v84(cross)\n    v96(cross)\n    v103(cross)\n    v107(Output)\n    v0--&gt;v2\n    v2--&gt;v9\n    v9--&gt;v17\n    v17--&gt;v24\n    v9--&gt;v24\n    v9--&gt;v34\n    v40--&gt;v48\n    v48--&gt;v55\n    v40--&gt;v55\n    v40--&gt;v65\n    v65--&gt;v70\n    v70--&gt;v77\n    v2--&gt;v77\n    v77--&gt;v84\n    v2--&gt;v84\n    v2--&gt;v96\n    v96--&gt;v103\n    v2--&gt;v103\n    v103--&gt;v107\n    v34--&gt;v40\n    v17--&gt;v34\n    v48--&gt;v65\n    v70--&gt;v96\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v9 fill:#e3dcea,stroke:#7a4baa;\n    style v17 fill:#e3dcea,stroke:#7a4baa;\n    style v24 fill:#e3dcea,stroke:#7a4baa;\n    style v34 fill:#e3dcea,stroke:#7a4baa;\n    style v40 fill:#e3dcea,stroke:#7a4baa;\n    style v70 fill:#e3dcea,stroke:#7a4baa;\n    style v48 fill:#e3dcea,stroke:#7a4baa;\n    style v55 fill:#e3dcea,stroke:#7a4baa;\n    style v65 fill:#e3dcea,stroke:#7a4baa;\n    style v77 fill:#e3dcea,stroke:#7a4baa;\n    style v84 fill:#e3dcea,stroke:#7a4baa;\n    style v96 fill:#e3dcea,stroke:#7a4baa;\n    style v103 fill:#e3dcea,stroke:#7a4baa;\n    style v107 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "BD Rhapsody"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html",
    "href": "components/workflows/ingestion/conversion.html",
    "title": "Convert to MuData",
    "section": "",
    "text": "ID: conversion\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Convert to MuData"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html#example-commands",
    "href": "components/workflows/ingestion/conversion.html#example-commands",
    "title": "Convert to MuData",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/ingestion/conversion/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\ninput_type: # please fill in - example: \"foo\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Conversion from h5ad\n# modality: [\"foo\"]\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/conversion/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Convert to MuData"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html#argument-groups",
    "href": "components/workflows/ingestion/conversion.html#argument-groups",
    "title": "Convert to MuData",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"input.h5mu\"\n\n\n--input_type\nType of the input file\nstring, required\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nName or template for the output files.\nfile, example: \"output.h5mu\"\n\n\n\n\n\nConversion from h5ad\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--modality\nName of the modality where the h5ad is stored in the h5mu object.\nList of string, multiple_sep: \";\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Convert to MuData"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html#authors",
    "href": "components/workflows/ingestion/conversion.html#authors",
    "title": "Convert to MuData",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author, maintainer)\nDries De Maeyer   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Convert to MuData"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/conversion.html#visualisation",
    "href": "components/workflows/ingestion/conversion.html#visualisation",
    "title": "Convert to MuData",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v12(filter)\n    v20(from_10xh5_to_h5mu)\n    v27(cross)\n    v37(cross)\n    v117(mix)\n    v48(filter)\n    v56(from_h5ad_to_h5mu)\n    v63(cross)\n    v73(cross)\n    v84(filter)\n    v92(from_10xmtx_to_h5mu)\n    v99(cross)\n    v109(cross)\n    v124(cross)\n    v131(cross)\n    v143(cross)\n    v150(cross)\n    v154(Output)\n    v0--&gt;v2\n    v12--&gt;v20\n    v20--&gt;v27\n    v12--&gt;v27\n    v12--&gt;v37\n    v48--&gt;v56\n    v56--&gt;v63\n    v48--&gt;v63\n    v48--&gt;v73\n    v84--&gt;v92\n    v92--&gt;v99\n    v84--&gt;v99\n    v84--&gt;v109\n    v117--&gt;v124\n    v2--&gt;v124\n    v124--&gt;v131\n    v2--&gt;v131\n    v2--&gt;v143\n    v143--&gt;v150\n    v2--&gt;v150\n    v150--&gt;v154\n    v2--&gt;v12\n    v37--&gt;v117\n    v20--&gt;v37\n    v2--&gt;v48\n    v73--&gt;v117\n    v56--&gt;v73\n    v2--&gt;v84\n    v109--&gt;v117\n    v92--&gt;v109\n    v117--&gt;v143\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v12 fill:#e3dcea,stroke:#7a4baa;\n    style v20 fill:#e3dcea,stroke:#7a4baa;\n    style v27 fill:#e3dcea,stroke:#7a4baa;\n    style v37 fill:#e3dcea,stroke:#7a4baa;\n    style v117 fill:#e3dcea,stroke:#7a4baa;\n    style v48 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v63 fill:#e3dcea,stroke:#7a4baa;\n    style v73 fill:#e3dcea,stroke:#7a4baa;\n    style v84 fill:#e3dcea,stroke:#7a4baa;\n    style v92 fill:#e3dcea,stroke:#7a4baa;\n    style v99 fill:#e3dcea,stroke:#7a4baa;\n    style v109 fill:#e3dcea,stroke:#7a4baa;\n    style v124 fill:#e3dcea,stroke:#7a4baa;\n    style v131 fill:#e3dcea,stroke:#7a4baa;\n    style v143 fill:#e3dcea,stroke:#7a4baa;\n    style v150 fill:#e3dcea,stroke:#7a4baa;\n    style v154 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Convert to MuData"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html",
    "href": "components/workflows/ingestion/make_reference.html",
    "title": "Make reference",
    "section": "",
    "text": "ID: make_reference\nNamespace: workflows/ingestion\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html#example-commands",
    "href": "components/workflows/ingestion/make_reference.html#example-commands",
    "title": "Make reference",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/ingestion/make_reference/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ngenome_fasta: # please fill in - example: \"https:/ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/GRCh38.primary_assembly.genome.fa.gz\"\ntranscriptome_gtf: # please fill in - example: \"https:/ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.annotation.gtf.gz\"\n# ercc: \"https:/assets.thermofisher.com/TFS-Assets/LSG/manuals/ERCC92.zip\"\n\n# STAR Settings\nstar_genome_sa_index_nbases: 14\n\n# BD Rhapsody Settings\nbdrhap_mitochondrial_contigs: [\"chrM\", \"chrMT\", \"M\", \"MT\"]\nbdrhap_filtering_off: false\nbdrhap_wta_only_index: false\n# bdrhap_extra_star_params: \"--limitGenomeGenerateRAM 48000 --genomeSAindexNbases 11\"\n\n# Cellranger ARC options\n# motifs_file: \"path/to/file\"\n# non_nuclear_contigs: [\"foo\"]\n\n# Outputs\ntarget: [\"star\"]\n# output_fasta: \"$id.$key.output_fasta.gz\"\n# output_gtf: \"$id.$key.output_gtf.gz\"\n# output_cellranger: \"$id.$key.output_cellranger.gz\"\n# output_cellranger_arc: \"$id.$key.output_cellranger_arc.gz\"\n# output_bd_rhapsody: \"$id.$key.output_bd_rhapsody.gz\"\n# output_star: \"$id.$key.output_star.gz\"\n\n# Arguments\n# subset_regex: \"(ERCC-00002|chr1)\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/ingestion/make_reference/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html#argument-groups",
    "href": "components/workflows/ingestion/make_reference.html#argument-groups",
    "title": "Make reference",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the reference.\nstring, required, example: \"foo\"\n\n\n--genome_fasta\nReference genome fasta.\nfile, required, example: \"https:/ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/GRCh38.primary_assembly.genome.fa.gz\"\n\n\n--transcriptome_gtf\nReference transcriptome annotation.\nfile, required, example: \"https:/ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.annotation.gtf.gz\"\n\n\n--ercc\nERCC sequence and annotation file.\nfile, example: \"https:/assets.thermofisher.com/TFS-Assets/LSG/manuals/ERCC92.zip\"\n\n\n\n\n\nSTAR Settings\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--star_genome_sa_index_nbases\nLength (bases) of the SA pre-indexing string. Typically between 10 and 15. Longer strings will use much more memory, but allow faster searches. For small genomes, the parameter {genomeSAindexNbases must be scaled down to min(14, log2(GenomeLength)/2 - 1).\ninteger, default: 14\n\n\n\n\n\nBD Rhapsody Settings\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--bdrhap_mitochondrial_contigs\nNames of the Mitochondrial contigs in the provided Reference Genome. Fragments originating from contigs other than these are identified as ‘nuclear fragments’ in the ATACseq analysis pipeline.\nList of string, default: \"chrM\", \"chrMT\", \"M\", \"MT\", multiple_sep: \";\"\n\n\n--bdrhap_filtering_off\nBy default the input Transcript Annotation files are filtered based on the gene_type/gene_biotype attribute. Only features having the following attribute values are kept: - protein_coding - lncRNA - IG_LV_gene - IG_V_gene - IG_V_pseudogene - IG_D_gene - IG_J_gene - IG_J_pseudogene - IG_C_gene - IG_C_pseudogene - TR_V_gene - TR_V_pseudogene - TR_D_gene - TR_J_gene - TR_J_pseudogene - TR_C_gene If you have already pre-filtered the input Annotation files and/or wish to turn-off the filtering, please set this option to True.\nboolean_true\n\n\n--bdrhap_wta_only_index\nBuild a WTA only index, otherwise builds a WTA + ATAC index.\nboolean_true\n\n\n--bdrhap_extra_star_params\nAdditional parameters to pass to STAR when building the genome index. Specify exactly like how you would on the command line.\nstring, example: \"--limitGenomeGenerateRAM 48000 --genomeSAindexNbases 11\"\n\n\n\n\n\nCellranger ARC options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--motifs_file\nPath to file containing transcription factor motifs in JASPAR format.\nfile\n\n\n--non_nuclear_contigs\nName(s) of contig(s) that do not have any chromatin structure, for example, mitochondria or plastids. These contigs are excluded from peak calling since the entire contig will be “open” due to a lack of chromatin structure. Leave empty if there are no such contigs.\nList of string, multiple_sep: \";\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--target\nWhich reference indices to generate.\nList of string, default: \"star\", multiple_sep: \";\"\n\n\n--output_fasta\nOutput genome sequence fasta.\nfile, example: \"genome_sequence.fa.gz\"\n\n\n--output_gtf\nOutput transcriptome annotation gtf.\nfile, example: \"transcriptome_annotation.gtf.gz\"\n\n\n--output_cellranger\nOutput index\nfile, example: \"cellranger_index.tar.gz\"\n\n\n--output_cellranger_arc\nOutput index\nfile, example: \"cellranger_index_arc.tar.gz\"\n\n\n--output_bd_rhapsody\nOutput index\nfile, example: \"bdrhap_index.tar.gz\"\n\n\n--output_star\nOutput index\nfile, example: \"star_index.tar.gz\"\n\n\n\n\n\nArguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--subset_regex\nWill subset the reference chromosomes using the given regex.\nstring, example: \"(ERCC-00002&#124;chr1)\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html#authors",
    "href": "components/workflows/ingestion/make_reference.html#authors",
    "title": "Make reference",
    "section": "Authors",
    "text": "Authors\n\nAngela Oliveira Pisco    (author)\nRobrecht Cannoodt    (author, maintainer)\nWeiwei Schultz (contributor)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/ingestion/make_reference.html#visualisation",
    "href": "components/workflows/ingestion/make_reference.html#visualisation",
    "title": "Make reference",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(make_reference_component)\n    v25(cross)\n    v35(cross)\n    v44(branch)\n    v71(concat)\n    v49(build_cellranger_arc_reference)\n    v56(cross)\n    v66(cross)\n    v75(branch)\n    v102(concat)\n    v80(build_cellranger_reference)\n    v87(cross)\n    v97(cross)\n    v106(branch)\n    v133(concat)\n    v111(build_star_reference)\n    v118(cross)\n    v128(cross)\n    v137(branch)\n    v164(concat)\n    v142(build_bdrhap_reference)\n    v149(cross)\n    v159(cross)\n    v171(cross)\n    v178(cross)\n    v190(cross)\n    v197(cross)\n    v201(Output)\n    v44--&gt;v71\n    v75--&gt;v102\n    v106--&gt;v133\n    v137--&gt;v164\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v44--&gt;v49\n    v49--&gt;v56\n    v44--&gt;v56\n    v44--&gt;v66\n    v66--&gt;v71\n    v75--&gt;v80\n    v80--&gt;v87\n    v75--&gt;v87\n    v75--&gt;v97\n    v97--&gt;v102\n    v106--&gt;v111\n    v111--&gt;v118\n    v106--&gt;v118\n    v106--&gt;v128\n    v128--&gt;v133\n    v137--&gt;v142\n    v142--&gt;v149\n    v137--&gt;v149\n    v137--&gt;v159\n    v159--&gt;v164\n    v164--&gt;v171\n    v2--&gt;v171\n    v171--&gt;v178\n    v2--&gt;v178\n    v2--&gt;v190\n    v190--&gt;v197\n    v2--&gt;v197\n    v197--&gt;v201\n    v18--&gt;v35\n    v35--&gt;v44\n    v49--&gt;v66\n    v71--&gt;v75\n    v80--&gt;v97\n    v102--&gt;v106\n    v111--&gt;v128\n    v133--&gt;v137\n    v142--&gt;v159\n    v164--&gt;v190\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v44 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v75 fill:#e3dcea,stroke:#7a4baa;\n    style v102 fill:#e3dcea,stroke:#7a4baa;\n    style v80 fill:#e3dcea,stroke:#7a4baa;\n    style v87 fill:#e3dcea,stroke:#7a4baa;\n    style v97 fill:#e3dcea,stroke:#7a4baa;\n    style v106 fill:#e3dcea,stroke:#7a4baa;\n    style v133 fill:#e3dcea,stroke:#7a4baa;\n    style v111 fill:#e3dcea,stroke:#7a4baa;\n    style v118 fill:#e3dcea,stroke:#7a4baa;\n    style v128 fill:#e3dcea,stroke:#7a4baa;\n    style v137 fill:#e3dcea,stroke:#7a4baa;\n    style v164 fill:#e3dcea,stroke:#7a4baa;\n    style v142 fill:#e3dcea,stroke:#7a4baa;\n    style v149 fill:#e3dcea,stroke:#7a4baa;\n    style v159 fill:#e3dcea,stroke:#7a4baa;\n    style v171 fill:#e3dcea,stroke:#7a4baa;\n    style v178 fill:#e3dcea,stroke:#7a4baa;\n    style v190 fill:#e3dcea,stroke:#7a4baa;\n    style v197 fill:#e3dcea,stroke:#7a4baa;\n    style v201 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Ingestion",
      "Make reference"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_integration_knn.html",
    "href": "components/workflows/annotation/scgpt_integration_knn.html",
    "title": "scGPT Annotation",
    "section": "",
    "text": "ID: scgpt_integration_knn\nNamespace: workflows/annotation\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_integration_knn.html#example-commands",
    "href": "components/workflows/annotation/scgpt_integration_knn.html#example-commands",
    "title": "scGPT Annotation",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/annotation/scgpt_integration_knn/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Query Input\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"input.h5mu\"\nmodality: \"rna\"\n# input_layer: \"foo\"\n# input_var_gene_names: \"foo\"\ninput_obs_batch_label: # please fill in - example: \"sample\"\noverwrite_existing_key: false\n\n# Reference input\nreference: # please fill in - example: \"reference.h5mu\"\nreference_obs_targets: # please fill in - example: [\"ann_level_1\", \"ann_level_2\", \"ann_level_3\", \"ann_level_4\", \"ann_level_5\", \"ann_finest_level\"]\n# reference_var_gene_names: \"foo\"\nreference_obs_batch_label: # please fill in - example: \"sample\"\n\n# scGPT model\nmodel: # please fill in - example: \"resources_test/scgpt/best_model.pt\"\nmodel_vocab: # please fill in - example: \"resources_test/scgpt/vocab.json\"\nmodel_config: # please fill in - example: \"args.json\"\n# finetuned_checkpoints_key: \"model_state_dict\"\n\n# Padding arguments\npad_token: \"&lt;pad&gt;\"\npad_value: -2\n\n# HVG subset arguments\nn_hvg: 1200\n\n# Tokenization arguments\n# max_seq_len: 123\n\n# Embedding arguments\ndsbn: true\nbatch_size: 64\n\n# Binning arguments\nn_input_bins: 51\n# seed: 123\n\n# Leiden clustering options\nleiden_resolution: [1.0]\n\n# Neighbor classifier arguments\nweights: \"uniform\"\nn_neighbors: 15\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n# output_obs_predictions: [\"foo\"]\n# output_obs_probability: [\"foo\"]\noutput_obsm_integrated: \"X_integrated_scgpt\"\n# output_compression: \"gzip\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/annotation/scgpt_integration_knn/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_integration_knn.html#argument-groups",
    "href": "components/workflows/annotation/scgpt_integration_knn.html#argument-groups",
    "title": "scGPT Annotation",
    "section": "Argument groups",
    "text": "Argument groups\n\nQuery Input\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nInput dataset consisting of the (unlabeled) query observations. The dataset is expected to be pre-processed in the same way as –reference.\nfile, required, example: \"input.h5mu\"\n\n\n--modality\nWhich modality to process. Should match the modality of the –reference dataset.\nstring, default: \"rna\"\n\n\n--input_layer\nMudata layer (key from layers) to use as input data for scGPT integration; if not specified, X is used. Should match the layer name of the reference dataset.\nstring\n\n\n--input_var_gene_names\nThe .var field in the input (query) dataset containing gene names; if not provided, the .var index will be used.\nstring\n\n\n--input_obs_batch_label\nThe .obs field in the input (query) dataset containing the batch labels.\nstring, required, example: \"sample\"\n\n\n--overwrite_existing_key\nIf provided, will overwrite existing fields in the input dataset when data are copied during the reference alignment process.\nboolean_true\n\n\n\n\n\nReference input\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--reference\nReference dataset consisting of observations with cell type labels present in the .obs –reference_obs_batch_label column to train the classifier on. The dataset is expected to be pre-processed in the same way as the –input query dataset(s).\nfile, required, example: \"reference.h5mu\"\n\n\n--reference_obs_targets\nThe .obs key(s) of the target labels to tranfer.\nList of string, required, example: \"ann_level_1\", \"ann_level_2\", \"ann_level_3\", \"ann_level_4\", \"ann_level_5\", \"ann_finest_level\", multiple_sep: \";\"\n\n\n--reference_var_gene_names\nThe .var field in the reference dataset containing gene names; if not provided, the .var index will be used.\nstring\n\n\n--reference_obs_batch_label\nThe .obs field in the reference dataset containing the batch labels.\nstring, required, example: \"sample\"\n\n\n\n\n\nscGPT model\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--model\nThe scGPT model file. Can either be a foundation model or a fine-tuned model. If the model file is a fine-tuned model, it must contain a key for the checkpoints (–finetuned_checkpoints_key).\nfile, required, example: \"resources_test/scgpt/best_model.pt\"\n\n\n--model_vocab\nThe scGPT model vocabulary file.\nfile, required, example: \"resources_test/scgpt/vocab.json\"\n\n\n--model_config\nThe scGPT model configuration file.\nfile, required, example: \"args.json\"\n\n\n--finetuned_checkpoints_key\nKey in the model file containing the pretrained checkpoints. Must be provided when --model is a fine-tuned model.\nstring, example: \"model_state_dict\"\n\n\n\n\n\nPadding arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--pad_token\nToken used for padding.\nstring, default: \"&lt;pad&gt;\"\n\n\n--pad_value\nThe value of the padding token.\ninteger, default: -2\n\n\n\n\n\nHVG subset arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--n_hvg\nNumber of highly variable genes to subset for.\ninteger, default: 1200\n\n\n\n\n\nTokenization arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--max_seq_len\nThe maximum sequence length of the tokenized data. Defaults to the number of features if not provided.\ninteger\n\n\n\n\n\nEmbedding arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--dsbn\nApply domain-specific batch normalization\nboolean, default: TRUE\n\n\n--batch_size\nThe batch size to be used for embedding inference.\ninteger, default: 64\n\n\n\n\n\nBinning arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--n_input_bins\nThe number of bins to discretize the data into; When no value is provided, data won’t be binned.\ninteger, default: 51\n\n\n--seed\nSeed for random number generation used for binning. If not set, no seed is used.\ninteger\n\n\n\n\n\nLeiden clustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nNeighbor classifier arguments\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--weights\nWeight function used in prediction. Possible values are: uniform (all points in each neighborhood are weighted equally) or distance (weight points by the inverse of their distance)\nstring, default: \"uniform\"\n\n\n--n_neighbors\nThe number of neighbors to use in k-neighbor graph structure used for fast approximate nearest neighbor search with PyNNDescent. Larger values will result in more accurate search results at the cost of computation time.\ninteger, default: 15\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nThe query data in .h5mu format with predicted labels predicted from the classifier trained on the reference.\nfile, required, example: \"output.h5mu\"\n\n\n--output_obs_predictions\nIn which .obs slots to store the predicted information. If provided, must have the same length as --reference_obs_targets. If empty, will default to the reference_obs_targets combined with the \"_pred\" suffix.\nList of string, multiple_sep: \";\"\n\n\n--output_obs_probability\nIn which .obs slots to store the probability of the predictions. If provided, must have the same length as --reference_obs_targets. If empty, will default to the reference_obs_targets combined with the \"_probability\" suffix.\nList of string, multiple_sep: \";\"\n\n\n--output_obsm_integrated\nIn which .obsm slot to store the integrated embedding.\nstring, default: \"X_integrated_scgpt\"\n\n\n--output_compression\nThe compression format to be used on the output h5mu object.\nstring, example: \"gzip\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_integration_knn.html#authors",
    "href": "components/workflows/annotation/scgpt_integration_knn.html#authors",
    "title": "scGPT Annotation",
    "section": "Authors",
    "text": "Authors\n\nDorien Roosen   (author, maintainer)\nElizabeth Mlynarski (author)\nWeiwei Schultz (contributor)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/annotation/scgpt_integration_knn.html#visualisation",
    "href": "components/workflows/annotation/scgpt_integration_knn.html#visualisation",
    "title": "scGPT Annotation",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v12(filter)\n    v20(add_id)\n    v27(cross)\n    v37(cross)\n    v43(filter)\n    v51(add_id)\n    v58(cross)\n    v68(cross)\n    v74(filter)\n    v82(duplicate_obs)\n    v89(cross)\n    v99(cross)\n    v105(filter)\n    v113(duplicate_obs)\n    v120(cross)\n    v130(cross)\n    v136(filter)\n    v144(duplicate_var)\n    v151(cross)\n    v161(cross)\n    v167(filter)\n    v175(duplicate_var)\n    v182(cross)\n    v192(cross)\n    v198(filter)\n    v206(concatenate_h5mu)\n    v213(cross)\n    v223(cross)\n    v230(filter)\n    v238(filter)\n    v253(cross)\n    v263(cross)\n    v269(filter)\n    v284(cross)\n    v294(cross)\n    v300(filter)\n    v315(cross)\n    v325(cross)\n    v331(filter)\n    v346(cross)\n    v356(cross)\n    v362(filter)\n    v377(cross)\n    v387(cross)\n    v393(filter)\n    v408(cross)\n    v418(cross)\n    v427(branch)\n    v454(concat)\n    v439(cross)\n    v449(cross)\n    v458(branch)\n    v485(concat)\n    v470(cross)\n    v480(cross)\n    v486(filter)\n    v516(concat)\n    v501(cross)\n    v511(cross)\n    v523(cross)\n    v533(cross)\n    v540(filter)\n    v548(split_h5mu)\n    v555(cross)\n    v565(cross)\n    v574(filter)\n    v604(concat)\n    v582(knn)\n    v589(cross)\n    v599(cross)\n    v610(cross)\n    v617(cross)\n    v629(cross)\n    v636(cross)\n    v640(Output)\n    subgraph group_scgpt_leiden_workflow [scgpt_leiden_workflow]\n        v246(highly_variable_features_scanpy)\n        v277(cross_check_genes)\n        v308(binning)\n        v339(pad_tokenize)\n        v370(embedding)\n        v401(find_neighbors)\n        v432(leiden)\n        v463(move_obsm_to_obs)\n        v494(umap)\n    end\n    v427--&gt;v454\n    v458--&gt;v485\n    v485--&gt;v486\n    v0--&gt;v2\n    v2--&gt;v12\n    v12--&gt;v20\n    v20--&gt;v27\n    v12--&gt;v27\n    v12--&gt;v37\n    v43--&gt;v51\n    v51--&gt;v58\n    v43--&gt;v58\n    v43--&gt;v68\n    v74--&gt;v82\n    v82--&gt;v89\n    v74--&gt;v89\n    v74--&gt;v99\n    v105--&gt;v113\n    v113--&gt;v120\n    v105--&gt;v120\n    v105--&gt;v130\n    v136--&gt;v144\n    v144--&gt;v151\n    v136--&gt;v151\n    v136--&gt;v161\n    v167--&gt;v175\n    v175--&gt;v182\n    v167--&gt;v182\n    v167--&gt;v192\n    v198--&gt;v206\n    v206--&gt;v213\n    v198--&gt;v213\n    v198--&gt;v223\n    v230--&gt;v238\n    v238--&gt;v246\n    v246--&gt;v253\n    v238--&gt;v253\n    v238--&gt;v263\n    v269--&gt;v277\n    v277--&gt;v284\n    v269--&gt;v284\n    v269--&gt;v294\n    v300--&gt;v308\n    v308--&gt;v315\n    v300--&gt;v315\n    v300--&gt;v325\n    v331--&gt;v339\n    v339--&gt;v346\n    v331--&gt;v346\n    v331--&gt;v356\n    v362--&gt;v370\n    v370--&gt;v377\n    v362--&gt;v377\n    v362--&gt;v387\n    v393--&gt;v401\n    v401--&gt;v408\n    v393--&gt;v408\n    v393--&gt;v418\n    v427--&gt;v432\n    v432--&gt;v439\n    v427--&gt;v439\n    v427--&gt;v449\n    v449--&gt;v454\n    v458--&gt;v463\n    v463--&gt;v470\n    v458--&gt;v470\n    v458--&gt;v480\n    v480--&gt;v485\n    v486--&gt;v494\n    v494--&gt;v501\n    v486--&gt;v501\n    v486--&gt;v511\n    v511--&gt;v516\n    v516--&gt;v523\n    v230--&gt;v523\n    v230--&gt;v533\n    v540--&gt;v548\n    v548--&gt;v555\n    v540--&gt;v555\n    v540--&gt;v565\n    v574--&gt;v582\n    v582--&gt;v589\n    v574--&gt;v589\n    v574--&gt;v599\n    v599--&gt;v604\n    v604--&gt;v610\n    v2--&gt;v610\n    v610--&gt;v617\n    v2--&gt;v617\n    v2--&gt;v629\n    v629--&gt;v636\n    v2--&gt;v636\n    v636--&gt;v640\n    v37--&gt;v43\n    v20--&gt;v37\n    v68--&gt;v74\n    v51--&gt;v68\n    v99--&gt;v105\n    v82--&gt;v99\n    v130--&gt;v136\n    v113--&gt;v130\n    v161--&gt;v167\n    v144--&gt;v161\n    v192--&gt;v198\n    v175--&gt;v192\n    v223--&gt;v230\n    v206--&gt;v223\n    v533--&gt;v540\n    v263--&gt;v269\n    v246--&gt;v263\n    v294--&gt;v300\n    v277--&gt;v294\n    v325--&gt;v331\n    v308--&gt;v325\n    v356--&gt;v362\n    v339--&gt;v356\n    v387--&gt;v393\n    v370--&gt;v387\n    v401--&gt;v418\n    v418--&gt;v427\n    v432--&gt;v449\n    v454--&gt;v458\n    v463--&gt;v480\n    v494--&gt;v511\n    v516--&gt;v533\n    v565--&gt;v574\n    v548--&gt;v565\n    v582--&gt;v599\n    v604--&gt;v629\n    style group_scgpt_leiden_workflow fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v12 fill:#e3dcea,stroke:#7a4baa;\n    style v20 fill:#e3dcea,stroke:#7a4baa;\n    style v27 fill:#e3dcea,stroke:#7a4baa;\n    style v37 fill:#e3dcea,stroke:#7a4baa;\n    style v43 fill:#e3dcea,stroke:#7a4baa;\n    style v51 fill:#e3dcea,stroke:#7a4baa;\n    style v58 fill:#e3dcea,stroke:#7a4baa;\n    style v68 fill:#e3dcea,stroke:#7a4baa;\n    style v74 fill:#e3dcea,stroke:#7a4baa;\n    style v82 fill:#e3dcea,stroke:#7a4baa;\n    style v89 fill:#e3dcea,stroke:#7a4baa;\n    style v99 fill:#e3dcea,stroke:#7a4baa;\n    style v105 fill:#e3dcea,stroke:#7a4baa;\n    style v113 fill:#e3dcea,stroke:#7a4baa;\n    style v120 fill:#e3dcea,stroke:#7a4baa;\n    style v130 fill:#e3dcea,stroke:#7a4baa;\n    style v136 fill:#e3dcea,stroke:#7a4baa;\n    style v144 fill:#e3dcea,stroke:#7a4baa;\n    style v151 fill:#e3dcea,stroke:#7a4baa;\n    style v161 fill:#e3dcea,stroke:#7a4baa;\n    style v167 fill:#e3dcea,stroke:#7a4baa;\n    style v175 fill:#e3dcea,stroke:#7a4baa;\n    style v182 fill:#e3dcea,stroke:#7a4baa;\n    style v192 fill:#e3dcea,stroke:#7a4baa;\n    style v198 fill:#e3dcea,stroke:#7a4baa;\n    style v206 fill:#e3dcea,stroke:#7a4baa;\n    style v213 fill:#e3dcea,stroke:#7a4baa;\n    style v223 fill:#e3dcea,stroke:#7a4baa;\n    style v230 fill:#e3dcea,stroke:#7a4baa;\n    style v238 fill:#e3dcea,stroke:#7a4baa;\n    style v246 fill:#e3dcea,stroke:#7a4baa;\n    style v253 fill:#e3dcea,stroke:#7a4baa;\n    style v263 fill:#e3dcea,stroke:#7a4baa;\n    style v269 fill:#e3dcea,stroke:#7a4baa;\n    style v277 fill:#e3dcea,stroke:#7a4baa;\n    style v284 fill:#e3dcea,stroke:#7a4baa;\n    style v294 fill:#e3dcea,stroke:#7a4baa;\n    style v300 fill:#e3dcea,stroke:#7a4baa;\n    style v308 fill:#e3dcea,stroke:#7a4baa;\n    style v315 fill:#e3dcea,stroke:#7a4baa;\n    style v325 fill:#e3dcea,stroke:#7a4baa;\n    style v331 fill:#e3dcea,stroke:#7a4baa;\n    style v339 fill:#e3dcea,stroke:#7a4baa;\n    style v346 fill:#e3dcea,stroke:#7a4baa;\n    style v356 fill:#e3dcea,stroke:#7a4baa;\n    style v362 fill:#e3dcea,stroke:#7a4baa;\n    style v370 fill:#e3dcea,stroke:#7a4baa;\n    style v377 fill:#e3dcea,stroke:#7a4baa;\n    style v387 fill:#e3dcea,stroke:#7a4baa;\n    style v393 fill:#e3dcea,stroke:#7a4baa;\n    style v401 fill:#e3dcea,stroke:#7a4baa;\n    style v408 fill:#e3dcea,stroke:#7a4baa;\n    style v418 fill:#e3dcea,stroke:#7a4baa;\n    style v427 fill:#e3dcea,stroke:#7a4baa;\n    style v454 fill:#e3dcea,stroke:#7a4baa;\n    style v432 fill:#e3dcea,stroke:#7a4baa;\n    style v439 fill:#e3dcea,stroke:#7a4baa;\n    style v449 fill:#e3dcea,stroke:#7a4baa;\n    style v458 fill:#e3dcea,stroke:#7a4baa;\n    style v485 fill:#e3dcea,stroke:#7a4baa;\n    style v463 fill:#e3dcea,stroke:#7a4baa;\n    style v470 fill:#e3dcea,stroke:#7a4baa;\n    style v480 fill:#e3dcea,stroke:#7a4baa;\n    style v486 fill:#e3dcea,stroke:#7a4baa;\n    style v516 fill:#e3dcea,stroke:#7a4baa;\n    style v494 fill:#e3dcea,stroke:#7a4baa;\n    style v501 fill:#e3dcea,stroke:#7a4baa;\n    style v511 fill:#e3dcea,stroke:#7a4baa;\n    style v523 fill:#e3dcea,stroke:#7a4baa;\n    style v533 fill:#e3dcea,stroke:#7a4baa;\n    style v540 fill:#e3dcea,stroke:#7a4baa;\n    style v548 fill:#e3dcea,stroke:#7a4baa;\n    style v555 fill:#e3dcea,stroke:#7a4baa;\n    style v565 fill:#e3dcea,stroke:#7a4baa;\n    style v574 fill:#e3dcea,stroke:#7a4baa;\n    style v604 fill:#e3dcea,stroke:#7a4baa;\n    style v582 fill:#e3dcea,stroke:#7a4baa;\n    style v589 fill:#e3dcea,stroke:#7a4baa;\n    style v599 fill:#e3dcea,stroke:#7a4baa;\n    style v610 fill:#e3dcea,stroke:#7a4baa;\n    style v617 fill:#e3dcea,stroke:#7a4baa;\n    style v629 fill:#e3dcea,stroke:#7a4baa;\n    style v636 fill:#e3dcea,stroke:#7a4baa;\n    style v640 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Annotation",
      "scGPT Annotation"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html",
    "href": "components/workflows/gdo/gdo_singlesample.html",
    "title": "GDO Singlesample",
    "section": "",
    "text": "ID: gdo_singlesample\nNamespace: workflows/gdo\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "GDO Singlesample"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html#example-commands",
    "href": "components/workflows/gdo/gdo_singlesample.html#example-commands",
    "title": "GDO Singlesample",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/gdo/gdo_singlesample/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Input\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\n\n# Output\n# output: \"$id.$key.output.h5mu\"\n\n# Filtering options\n# min_counts: 200\n# max_counts: 5000000\n# min_guides_per_cell: 200\n# max_guides_per_cell: 1500000\n# min_cells_per_guide: 3\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/gdo/gdo_singlesample/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "GDO Singlesample"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html#argument-groups",
    "href": "components/workflows/gdo/gdo_singlesample.html#argument-groups",
    "title": "GDO Singlesample",
    "section": "Argument groups",
    "text": "Argument groups\n\nInput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nInput layer to start from. By default, .X will be used.\nstring\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nFiltering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--min_counts\nMinimum number of counts captured per cell.\ninteger, example: 200\n\n\n--max_counts\nMaximum number of counts captured per cell.\ninteger, example: 5000000\n\n\n--min_guides_per_cell\nMinimum of non-zero values per cell.\ninteger, example: 200\n\n\n--max_guides_per_cell\nMaximum of non-zero values per cell.\ninteger, example: 1500000\n\n\n--min_cells_per_guide\nMinimum of non-zero values per gene.\ninteger, example: 3",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "GDO Singlesample"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html#authors",
    "href": "components/workflows/gdo/gdo_singlesample.html#authors",
    "title": "GDO Singlesample",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "GDO Singlesample"
    ]
  },
  {
    "objectID": "components/workflows/gdo/gdo_singlesample.html#visualisation",
    "href": "components/workflows/gdo/gdo_singlesample.html#visualisation",
    "title": "GDO Singlesample",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(gdo_filter_with_counts)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v71(concat)\n    v49(gdo_do_filter)\n    v56(cross)\n    v66(cross)\n    v78(cross)\n    v85(cross)\n    v97(cross)\n    v104(cross)\n    v108(Output)\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v66--&gt;v71\n    v71--&gt;v78\n    v2--&gt;v78\n    v78--&gt;v85\n    v2--&gt;v85\n    v2--&gt;v97\n    v97--&gt;v104\n    v2--&gt;v104\n    v104--&gt;v108\n    v35--&gt;v41\n    v18--&gt;v35\n    v49--&gt;v66\n    v71--&gt;v97\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v78 fill:#e3dcea,stroke:#7a4baa;\n    style v85 fill:#e3dcea,stroke:#7a4baa;\n    style v97 fill:#e3dcea,stroke:#7a4baa;\n    style v104 fill:#e3dcea,stroke:#7a4baa;\n    style v108 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Gdo",
      "GDO Singlesample"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html",
    "href": "components/workflows/integration/bbknn_leiden.html",
    "title": "Bbknn leiden",
    "section": "",
    "text": "ID: bbknn_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html#example-commands",
    "href": "components/workflows/integration/bbknn_leiden.html#example-commands",
    "title": "Bbknn leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/integration/bbknn_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n\n# Bbknn\nobsm_input: \"X_pca\"\nobs_batch: \"sample_id\"\nuns_output: \"bbknn_integration_neighbors\"\nobsp_distances: \"bbknn_integration_distances\"\nobsp_connectivities: \"bbknn_integration_connectivities\"\nn_neighbors_within_batch: 3\nn_pcs: 50\n# n_trim: 123\n\n# Clustering options\nobs_cluster: \"bbknn_integration_leiden\"\nleiden_resolution: [1.0]\n\n# UMAP options\nobsm_umap: \"X_leiden_bbknn_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/bbknn_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html#argument-groups",
    "href": "components/workflows/integration/bbknn_leiden.html#argument-groups",
    "title": "Bbknn leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n\n\n\nBbknn\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_input\nThe dimensionality reduction in .obsm to use for neighbour detection. Defaults to X_pca.\nstring, default: \"X_pca\"\n\n\n--obs_batch\n.obs column name discriminating between your batches.\nstring, default: \"sample_id\"\n\n\n--uns_output\nMandatory .uns slot to store various neighbor output objects.\nstring, default: \"bbknn_integration_neighbors\"\n\n\n--obsp_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"bbknn_integration_distances\"\n\n\n--obsp_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"bbknn_integration_connectivities\"\n\n\n--n_neighbors_within_batch\nHow many top neighbours to report for each batch; total number of neighbours in the initial k-nearest-neighbours computation will be this number times the number of batches.\ninteger, default: 3\n\n\n--n_pcs\nHow many dimensions (in case of PCA, principal components) to use in the analysis.\ninteger, default: 50\n\n\n--n_trim\nTrim the neighbours of each cell to these many top connectivities. May help with population independence and improve the tidiness of clustering. The lower the value the more independent the individual populations, at the cost of more conserved batch effect. If None (default), sets the parameter value automatically to 10 times neighbors_within_batch times the number of batches. Set to 0 to skip.\ninteger\n\n\n\n\n\nClustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"bbknn_integration_leiden\"\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUMAP options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_leiden_bbknn_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html#authors",
    "href": "components/workflows/integration/bbknn_leiden.html#authors",
    "title": "Bbknn leiden",
    "section": "Authors",
    "text": "Authors\n\nMauro Saporita   (author)\nPovilas Gibas   (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/bbknn_leiden.html#visualisation",
    "href": "components/workflows/integration/bbknn_leiden.html#visualisation",
    "title": "Bbknn leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v40(concat)\n    v18(bbknn)\n    v25(cross)\n    v35(cross)\n    v42(filter)\n    v50(leiden)\n    v57(cross)\n    v67(cross)\n    v73(filter)\n    v81(move_obsm_to_obs)\n    v88(cross)\n    v98(cross)\n    v105(mix)\n    v106(filter)\n    v136(concat)\n    v114(umap)\n    v121(cross)\n    v131(cross)\n    v143(cross)\n    v150(cross)\n    v162(cross)\n    v169(cross)\n    v173(Output)\n    v105--&gt;v106\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v35--&gt;v40\n    v42--&gt;v50\n    v50--&gt;v57\n    v42--&gt;v57\n    v42--&gt;v67\n    v73--&gt;v81\n    v81--&gt;v88\n    v73--&gt;v88\n    v73--&gt;v98\n    v106--&gt;v114\n    v114--&gt;v121\n    v106--&gt;v121\n    v106--&gt;v131\n    v131--&gt;v136\n    v136--&gt;v143\n    v2--&gt;v143\n    v143--&gt;v150\n    v2--&gt;v150\n    v2--&gt;v162\n    v162--&gt;v169\n    v2--&gt;v169\n    v169--&gt;v173\n    v18--&gt;v35\n    v40--&gt;v42\n    v67--&gt;v73\n    v50--&gt;v67\n    v98--&gt;v105\n    v81--&gt;v98\n    v40--&gt;v105\n    v114--&gt;v131\n    v136--&gt;v162\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v40 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v42 fill:#e3dcea,stroke:#7a4baa;\n    style v50 fill:#e3dcea,stroke:#7a4baa;\n    style v57 fill:#e3dcea,stroke:#7a4baa;\n    style v67 fill:#e3dcea,stroke:#7a4baa;\n    style v73 fill:#e3dcea,stroke:#7a4baa;\n    style v81 fill:#e3dcea,stroke:#7a4baa;\n    style v88 fill:#e3dcea,stroke:#7a4baa;\n    style v98 fill:#e3dcea,stroke:#7a4baa;\n    style v105 fill:#e3dcea,stroke:#7a4baa;\n    style v106 fill:#e3dcea,stroke:#7a4baa;\n    style v136 fill:#e3dcea,stroke:#7a4baa;\n    style v114 fill:#e3dcea,stroke:#7a4baa;\n    style v121 fill:#e3dcea,stroke:#7a4baa;\n    style v131 fill:#e3dcea,stroke:#7a4baa;\n    style v143 fill:#e3dcea,stroke:#7a4baa;\n    style v150 fill:#e3dcea,stroke:#7a4baa;\n    style v162 fill:#e3dcea,stroke:#7a4baa;\n    style v169 fill:#e3dcea,stroke:#7a4baa;\n    style v173 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Bbknn leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html",
    "href": "components/workflows/integration/scvi_leiden.html",
    "title": "Scvi leiden",
    "section": "",
    "text": "ID: scvi_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html#example-commands",
    "href": "components/workflows/integration/scvi_leiden.html#example-commands",
    "title": "Scvi leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/integration/scvi_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\n# layer: \"foo\"\nmodality: \"rna\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n# output_model: \"$id.$key.output_model\"\n\n# Neighbour calculation\nuns_neighbors: \"scvi_integration_neighbors\"\nobsp_neighbor_distances: \"scvi_integration_distances\"\nobsp_neighbor_connectivities: \"scvi_integration_connectivities\"\n\n# Scvi integration options\nobs_batch: # please fill in - example: \"foo\"\nobsm_output: \"X_scvi_integrated\"\n# var_input: \"foo\"\n# early_stopping: true\nearly_stopping_monitor: \"elbo_validation\"\nearly_stopping_patience: 45\nearly_stopping_min_delta: 0.0\n# max_epochs: 123\nreduce_lr_on_plateau: true\nlr_factor: 0.6\nlr_patience: 30.0\n\n# Clustering options\nobs_cluster: \"scvi_integration_leiden\"\nleiden_resolution: [1.0]\n\n# Umap options\nobsm_umap: \"X_scvi_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/scvi_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html#argument-groups",
    "href": "components/workflows/integration/scvi_leiden.html#argument-groups",
    "title": "Scvi leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n--output_model\nFolder where the state of the trained model will be saved to.\nfile, required, example: \"output_dir\"\n\n\n\n\n\nNeighbour calculation\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"scvi_integration_neighbors\"\n\n\n--obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"scvi_integration_distances\"\n\n\n--obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"scvi_integration_connectivities\"\n\n\n\n\n\nScvi integration options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_batch\nColumn name discriminating between your batches.\nstring, required\n\n\n--obsm_output\nIn which .obsm slot to store the resulting integrated embedding.\nstring, default: \"X_scvi_integrated\"\n\n\n--var_input\n.var column containing highly variable genes. By default, do not subset genes.\nstring\n\n\n--early_stopping\nWhether to perform early stopping with respect to the validation set.\nboolean\n\n\n--early_stopping_monitor\nMetric logged during validation set epoch.\nstring, default: \"elbo_validation\"\n\n\n--early_stopping_patience\nNumber of validation epochs with no improvement after which training will be stopped.\ninteger, default: 45\n\n\n--early_stopping_min_delta\nMinimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\ndouble, default: 0\n\n\n--max_epochs\nNumber of passes through the dataset, defaults to (20000 / number of cells) * 400 or 400; whichever is smallest.\ninteger\n\n\n--reduce_lr_on_plateau\nWhether to monitor validation loss and reduce learning rate when validation set lr_scheduler_metric plateaus.\nboolean, default: TRUE\n\n\n--lr_factor\nFactor to reduce learning rate.\ndouble, default: 0.6\n\n\n--lr_patience\nNumber of epochs with no improvement after which learning rate will be reduced.\ndouble, default: 30\n\n\n\n\n\nClustering options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"scvi_integration_leiden\"\n\n\n--leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_scvi_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html#authors",
    "href": "components/workflows/integration/scvi_leiden.html#authors",
    "title": "Scvi leiden",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/scvi_leiden.html#visualisation",
    "href": "components/workflows/integration/scvi_leiden.html#visualisation",
    "title": "Scvi leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(scvi)\n    v25(cross)\n    v35(cross)\n    v41(filter)\n    v71(concat)\n    v49(find_neighbors)\n    v56(cross)\n    v66(cross)\n    v73(filter)\n    v81(leiden)\n    v88(cross)\n    v98(cross)\n    v104(filter)\n    v112(move_obsm_to_obs)\n    v119(cross)\n    v129(cross)\n    v136(mix)\n    v137(filter)\n    v167(concat)\n    v145(umap)\n    v152(cross)\n    v162(cross)\n    v174(cross)\n    v181(cross)\n    v193(cross)\n    v200(cross)\n    v204(Output)\n    v136--&gt;v137\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v41--&gt;v49\n    v49--&gt;v56\n    v41--&gt;v56\n    v41--&gt;v66\n    v66--&gt;v71\n    v73--&gt;v81\n    v81--&gt;v88\n    v73--&gt;v88\n    v73--&gt;v98\n    v104--&gt;v112\n    v112--&gt;v119\n    v104--&gt;v119\n    v104--&gt;v129\n    v137--&gt;v145\n    v145--&gt;v152\n    v137--&gt;v152\n    v137--&gt;v162\n    v162--&gt;v167\n    v167--&gt;v174\n    v2--&gt;v174\n    v174--&gt;v181\n    v2--&gt;v181\n    v2--&gt;v193\n    v193--&gt;v200\n    v2--&gt;v200\n    v200--&gt;v204\n    v35--&gt;v41\n    v18--&gt;v35\n    v49--&gt;v66\n    v71--&gt;v73\n    v98--&gt;v104\n    v81--&gt;v98\n    v129--&gt;v136\n    v112--&gt;v129\n    v71--&gt;v136\n    v145--&gt;v162\n    v167--&gt;v193\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v41 fill:#e3dcea,stroke:#7a4baa;\n    style v71 fill:#e3dcea,stroke:#7a4baa;\n    style v49 fill:#e3dcea,stroke:#7a4baa;\n    style v56 fill:#e3dcea,stroke:#7a4baa;\n    style v66 fill:#e3dcea,stroke:#7a4baa;\n    style v73 fill:#e3dcea,stroke:#7a4baa;\n    style v81 fill:#e3dcea,stroke:#7a4baa;\n    style v88 fill:#e3dcea,stroke:#7a4baa;\n    style v98 fill:#e3dcea,stroke:#7a4baa;\n    style v104 fill:#e3dcea,stroke:#7a4baa;\n    style v112 fill:#e3dcea,stroke:#7a4baa;\n    style v119 fill:#e3dcea,stroke:#7a4baa;\n    style v129 fill:#e3dcea,stroke:#7a4baa;\n    style v136 fill:#e3dcea,stroke:#7a4baa;\n    style v137 fill:#e3dcea,stroke:#7a4baa;\n    style v167 fill:#e3dcea,stroke:#7a4baa;\n    style v145 fill:#e3dcea,stroke:#7a4baa;\n    style v152 fill:#e3dcea,stroke:#7a4baa;\n    style v162 fill:#e3dcea,stroke:#7a4baa;\n    style v174 fill:#e3dcea,stroke:#7a4baa;\n    style v181 fill:#e3dcea,stroke:#7a4baa;\n    style v193 fill:#e3dcea,stroke:#7a4baa;\n    style v200 fill:#e3dcea,stroke:#7a4baa;\n    style v204 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Scvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html",
    "href": "components/workflows/integration/totalvi_leiden.html",
    "title": "Totalvi leiden",
    "section": "",
    "text": "ID: totalvi_leiden\nNamespace: workflows/integration\n\n\n\nSource",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html#example-commands",
    "href": "components/workflows/integration/totalvi_leiden.html#example-commands",
    "title": "Totalvi leiden",
    "section": "Example commands",
    "text": "Example commands\nYou can run the pipeline using nextflow run.\n\nView help\nYou can use --help as a parameter to get an overview of the possible parameters.\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -main-script target/nextflow/workflows/integration/totalvi_leiden/main.nf \\\n  --help\n\n\nRun command\n\n\nExample of params.yaml\n\n# Inputs\nid: # please fill in - example: \"foo\"\ninput: # please fill in - example: \"dataset.h5mu\"\nlayer: \"log_normalized\"\nmodality: \"rna\"\nprot_modality: \"prot\"\nreference: # please fill in - example: \"path/to/file\"\n\n# Outputs\n# output: \"$id.$key.output.h5mu\"\n# reference_model_path: \"totalvi_model_reference\"\n# query_model_path: \"totalvi_model_query\"\n\n# General TotalVI Options\nobs_batch: \"sample_id\"\nmax_epochs: 400\nmax_query_epochs: 200\nweight_decay: 0.0\nforce_retrain: false\n# var_input: \"foo\"\n\n# TotalVI integration options RNA\nrna_reference_modality: \"rna\"\nrna_obsm_output: \"X_totalvi\"\n\n# TotalVI integration options ADT\nprot_reference_modality: \"prot\"\nprot_obsm_output: \"X_totalvi\"\n\n# Neighbour calculation RNA\nrna_uns_neighbors: \"totalvi_integration_neighbors\"\nrna_obsp_neighbor_distances: \"totalvi_integration_distances\"\nrna_obsp_neighbor_connectivities: \"totalvi_integration_connectivities\"\n\n# Neighbour calculation ADT\nprot_uns_neighbors: \"totalvi_integration_neighbors\"\nprot_obsp_neighbor_distances: \"totalvi_integration_distances\"\nprot_obsp_neighbor_connectivities: \"totalvi_integration_connectivities\"\n\n# Clustering options RNA\nrna_obs_cluster: \"totalvi_integration_leiden\"\nrna_leiden_resolution: [1.0]\n\n# Clustering options ADT\nprot_obs_cluster: \"totalvi_integration_leiden\"\nprot_leiden_resolution: [1.0]\n\n# Umap options\nobsm_umap: \"X_totalvi_umap\"\n\n# Nextflow input-output arguments\npublish_dir: # please fill in - example: \"output/\"\n# param_list: \"my_params.yaml\"\n\n# Arguments\n\nnextflow run openpipelines-bio/openpipeline \\\n  -r 2.0.1 -latest \\\n  -profile docker \\\n  -main-script target/nextflow/workflows/integration/totalvi_leiden/main.nf \\\n  -params-file params.yaml\n\n\n\n\n\n\nNote\n\n\n\nReplace -profile docker with -profile podman or -profile singularity depending on the desired backend.",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html#argument-groups",
    "href": "components/workflows/integration/totalvi_leiden.html#argument-groups",
    "title": "Totalvi leiden",
    "section": "Argument groups",
    "text": "Argument groups\n\nInputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--id\nID of the sample.\nstring, required, example: \"foo\"\n\n\n--input\nPath to the sample.\nfile, required, example: \"dataset.h5mu\"\n\n\n--layer\nuse specified layer for expression values instead of the .X object from the modality.\nstring, default: \"log_normalized\"\n\n\n--modality\nWhich modality to process.\nstring, default: \"rna\"\n\n\n--prot_modality\nWhich modality to process.\nstring, default: \"prot\"\n\n\n--reference\nInput h5mu file with reference data to train the TOTALVI model.\nfile, required\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--output\nDestination path to the output.\nfile, required, example: \"output.h5mu\"\n\n\n--reference_model_path\nDirectory with the reference model. If not exists, trained model will be saved there\nfile, default: \"totalvi_model_reference\"\n\n\n--query_model_path\nDirectory, where the query model will be saved\nfile, default: \"totalvi_model_query\"\n\n\n\n\n\nGeneral TotalVI Options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obs_batch\n.Obs column name discriminating between your batches.\nstring, default: \"sample_id\"\n\n\n--max_epochs\nNumber of passes through the dataset\ninteger, default: 400\n\n\n--max_query_epochs\nNumber of passes through the dataset, when fine-tuning model for query\ninteger, default: 200\n\n\n--weight_decay\nWeight decay, when fine-tuning model for query\ndouble, default: 0\n\n\n--force_retrain\nIf true, retrain the model and save it to reference_model_path\nboolean_true\n\n\n--var_input\nBoolean .var column to subset data with (e.g. containing highly variable genes). By default, do not subset genes.\nstring\n\n\n\n\n\nTotalVI integration options RNA\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_reference_modality\n\nstring, default: \"rna\"\n\n\n--rna_obsm_output\nIn which .obsm slot to store the normalized RNA from TOTALVI.\nstring, default: \"X_totalvi\"\n\n\n\n\n\nTotalVI integration options ADT\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--prot_reference_modality\nName of the modality containing proteins in the reference\nstring, default: \"prot\"\n\n\n--prot_obsm_output\nIn which .obsm slot to store the normalized protein data from TOTALVI.\nstring, default: \"X_totalvi\"\n\n\n\n\n\nNeighbour calculation RNA\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"totalvi_integration_neighbors\"\n\n\n--rna_obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"totalvi_integration_distances\"\n\n\n--rna_obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"totalvi_integration_connectivities\"\n\n\n\n\n\nNeighbour calculation ADT\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--prot_uns_neighbors\nIn which .uns slot to store various neighbor output objects.\nstring, default: \"totalvi_integration_neighbors\"\n\n\n--prot_obsp_neighbor_distances\nIn which .obsp slot to store the distance matrix between the resulting neighbors.\nstring, default: \"totalvi_integration_distances\"\n\n\n--prot_obsp_neighbor_connectivities\nIn which .obsp slot to store the connectivities matrix between the resulting neighbors.\nstring, default: \"totalvi_integration_connectivities\"\n\n\n\n\n\nClustering options RNA\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--rna_obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"totalvi_integration_leiden\"\n\n\n--rna_leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nClustering options ADT\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--prot_obs_cluster\nPrefix for the .obs keys under which to add the cluster labels. Newly created columns in .obs will be created from the specified value for ‘–obs_cluster’ suffixed with an underscore and one of the resolutions resolutions specified in ‘–leiden_resolution’.\nstring, default: \"totalvi_integration_leiden\"\n\n\n--prot_leiden_resolution\nControl the coarseness of the clustering. Higher values lead to more clusters.\nList of double, default: 1, multiple_sep: \";\"\n\n\n\n\n\nUmap options\n\n\n\n\n\n\n\n\nName\nDescription\nAttributes\n\n\n\n\n--obsm_umap\nIn which .obsm slot to store the resulting UMAP embedding.\nstring, default: \"X_totalvi_umap\"",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html#authors",
    "href": "components/workflows/integration/totalvi_leiden.html#authors",
    "title": "Totalvi leiden",
    "section": "Authors",
    "text": "Authors\n\nDries Schaumont    (author)",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "components/workflows/integration/totalvi_leiden.html#visualisation",
    "href": "components/workflows/integration/totalvi_leiden.html#visualisation",
    "title": "Totalvi leiden",
    "section": "Visualisation",
    "text": "Visualisation\n\n\n\n\n\nflowchart TB\n    v0(Channel.fromList)\n    v2(filter)\n    v10(filter)\n    v18(totalvi)\n    v25(cross)\n    v35(cross)\n    v42(filter)\n    v72(concat)\n    v50(find_neighbors)\n    v57(cross)\n    v67(cross)\n    v74(filter)\n    v82(leiden)\n    v89(cross)\n    v99(cross)\n    v105(filter)\n    v113(move_obsm_to_obs)\n    v120(cross)\n    v130(cross)\n    v137(mix)\n    v138(filter)\n    v146(umap)\n    v153(cross)\n    v163(cross)\n    v170(filter)\n    v200(concat)\n    v185(cross)\n    v195(cross)\n    v202(filter)\n    v217(cross)\n    v227(cross)\n    v233(filter)\n    v248(cross)\n    v258(cross)\n    v265(mix)\n    v266(filter)\n    v281(cross)\n    v291(cross)\n    v297(filter)\n    v327(concat)\n    v305(publish)\n    v312(cross)\n    v322(cross)\n    v334(cross)\n    v341(cross)\n    v353(cross)\n    v360(cross)\n    v364(Output)\n    subgraph group_totalvi_leiden [totalvi_leiden]\n        v178(find_neighbors)\n        v210(leiden)\n        v241(move_obsm_to_obs)\n        v274(umap)\n    end\n    v137--&gt;v138\n    v265--&gt;v266\n    v0--&gt;v2\n    v2--&gt;v10\n    v10--&gt;v18\n    v18--&gt;v25\n    v10--&gt;v25\n    v10--&gt;v35\n    v42--&gt;v50\n    v50--&gt;v57\n    v42--&gt;v57\n    v42--&gt;v67\n    v67--&gt;v72\n    v74--&gt;v82\n    v82--&gt;v89\n    v74--&gt;v89\n    v74--&gt;v99\n    v105--&gt;v113\n    v113--&gt;v120\n    v105--&gt;v120\n    v105--&gt;v130\n    v138--&gt;v146\n    v146--&gt;v153\n    v138--&gt;v153\n    v138--&gt;v163\n    v170--&gt;v178\n    v178--&gt;v185\n    v170--&gt;v185\n    v170--&gt;v195\n    v195--&gt;v200\n    v202--&gt;v210\n    v210--&gt;v217\n    v202--&gt;v217\n    v202--&gt;v227\n    v233--&gt;v241\n    v241--&gt;v248\n    v233--&gt;v248\n    v233--&gt;v258\n    v266--&gt;v274\n    v274--&gt;v281\n    v266--&gt;v281\n    v266--&gt;v291\n    v297--&gt;v305\n    v305--&gt;v312\n    v297--&gt;v312\n    v297--&gt;v322\n    v322--&gt;v327\n    v327--&gt;v334\n    v2--&gt;v334\n    v334--&gt;v341\n    v2--&gt;v341\n    v2--&gt;v353\n    v353--&gt;v360\n    v2--&gt;v360\n    v360--&gt;v364\n    v35--&gt;v42\n    v18--&gt;v35\n    v50--&gt;v67\n    v72--&gt;v74\n    v99--&gt;v105\n    v82--&gt;v99\n    v130--&gt;v137\n    v113--&gt;v130\n    v72--&gt;v137\n    v163--&gt;v170\n    v146--&gt;v163\n    v178--&gt;v195\n    v200--&gt;v202\n    v227--&gt;v233\n    v210--&gt;v227\n    v258--&gt;v265\n    v241--&gt;v258\n    v200--&gt;v265\n    v291--&gt;v297\n    v274--&gt;v291\n    v305--&gt;v322\n    v327--&gt;v353\n    style group_totalvi_leiden fill:#F0F0F0,stroke:#969696;\n    style v0 fill:#e3dcea,stroke:#7a4baa;\n    style v2 fill:#e3dcea,stroke:#7a4baa;\n    style v10 fill:#e3dcea,stroke:#7a4baa;\n    style v18 fill:#e3dcea,stroke:#7a4baa;\n    style v25 fill:#e3dcea,stroke:#7a4baa;\n    style v35 fill:#e3dcea,stroke:#7a4baa;\n    style v42 fill:#e3dcea,stroke:#7a4baa;\n    style v72 fill:#e3dcea,stroke:#7a4baa;\n    style v50 fill:#e3dcea,stroke:#7a4baa;\n    style v57 fill:#e3dcea,stroke:#7a4baa;\n    style v67 fill:#e3dcea,stroke:#7a4baa;\n    style v74 fill:#e3dcea,stroke:#7a4baa;\n    style v82 fill:#e3dcea,stroke:#7a4baa;\n    style v89 fill:#e3dcea,stroke:#7a4baa;\n    style v99 fill:#e3dcea,stroke:#7a4baa;\n    style v105 fill:#e3dcea,stroke:#7a4baa;\n    style v113 fill:#e3dcea,stroke:#7a4baa;\n    style v120 fill:#e3dcea,stroke:#7a4baa;\n    style v130 fill:#e3dcea,stroke:#7a4baa;\n    style v137 fill:#e3dcea,stroke:#7a4baa;\n    style v138 fill:#e3dcea,stroke:#7a4baa;\n    style v146 fill:#e3dcea,stroke:#7a4baa;\n    style v153 fill:#e3dcea,stroke:#7a4baa;\n    style v163 fill:#e3dcea,stroke:#7a4baa;\n    style v170 fill:#e3dcea,stroke:#7a4baa;\n    style v200 fill:#e3dcea,stroke:#7a4baa;\n    style v178 fill:#e3dcea,stroke:#7a4baa;\n    style v185 fill:#e3dcea,stroke:#7a4baa;\n    style v195 fill:#e3dcea,stroke:#7a4baa;\n    style v202 fill:#e3dcea,stroke:#7a4baa;\n    style v210 fill:#e3dcea,stroke:#7a4baa;\n    style v217 fill:#e3dcea,stroke:#7a4baa;\n    style v227 fill:#e3dcea,stroke:#7a4baa;\n    style v233 fill:#e3dcea,stroke:#7a4baa;\n    style v241 fill:#e3dcea,stroke:#7a4baa;\n    style v248 fill:#e3dcea,stroke:#7a4baa;\n    style v258 fill:#e3dcea,stroke:#7a4baa;\n    style v265 fill:#e3dcea,stroke:#7a4baa;\n    style v266 fill:#e3dcea,stroke:#7a4baa;\n    style v274 fill:#e3dcea,stroke:#7a4baa;\n    style v281 fill:#e3dcea,stroke:#7a4baa;\n    style v291 fill:#e3dcea,stroke:#7a4baa;\n    style v297 fill:#e3dcea,stroke:#7a4baa;\n    style v327 fill:#e3dcea,stroke:#7a4baa;\n    style v305 fill:#e3dcea,stroke:#7a4baa;\n    style v312 fill:#e3dcea,stroke:#7a4baa;\n    style v322 fill:#e3dcea,stroke:#7a4baa;\n    style v334 fill:#e3dcea,stroke:#7a4baa;\n    style v341 fill:#e3dcea,stroke:#7a4baa;\n    style v353 fill:#e3dcea,stroke:#7a4baa;\n    style v360 fill:#e3dcea,stroke:#7a4baa;\n    style v364 fill:#e3dcea,stroke:#7a4baa;",
    "crumbs": [
      "Reference",
      "Workflows",
      "Integration",
      "Totalvi leiden"
    ]
  },
  {
    "objectID": "more_information/cheat_sheets.html",
    "href": "more_information/cheat_sheets.html",
    "title": "Cheat sheets",
    "section": "",
    "text": "Figure 1: Cheat sheet for developing modular pipeline components with Viash, including a sample Viash component (left) and common commands used throughout the various stages of a development cycle (right).",
    "crumbs": [
      "Fundamentals",
      "More information",
      "Cheat sheets"
    ]
  },
  {
    "objectID": "more_information/cheat_sheets.html#viash",
    "href": "more_information/cheat_sheets.html#viash",
    "title": "Cheat sheets",
    "section": "",
    "text": "Figure 1: Cheat sheet for developing modular pipeline components with Viash, including a sample Viash component (left) and common commands used throughout the various stages of a development cycle (right).",
    "crumbs": [
      "Fundamentals",
      "More information",
      "Cheat sheets"
    ]
  },
  {
    "objectID": "more_information/index.html",
    "href": "more_information/index.html",
    "title": "More information",
    "section": "",
    "text": "Cheat sheets: Cheat sheets for various tools\n  \n  \n  \n    Code of conduct: Our DEI values\n  \n  \n  \n    FAQ: Frequently Asked Questions\n  \n  \n\n\nNo matching items",
    "crumbs": [
      "Fundamentals",
      "More information"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html",
    "href": "user_guide/running_pipelines.html",
    "title": "Running pipelines",
    "section": "",
    "text": "Dependening on whether you want to run a workflow locally or on cloud infrastructure, using Nextflow Tower or not, you will need to use different commands.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html#run-locally-from-the-cli",
    "href": "user_guide/running_pipelines.html#run-locally-from-the-cli",
    "title": "Running pipelines",
    "section": "Run locally from the CLI",
    "text": "Run locally from the CLI\nYou can run a workflow from the command line using the following command:\nnextflow run openpipelines-bio/openpipeline \\\n  -main-script target/nextflow/workflows/integration/multimodal_integration/main.nf \\\n  -revision 0.12.1 \\\n  -latest \\\n  -profile docker \\\n  --publish_dir foo/ \\\n  --input \"bar\" \\\n  --output \"test.txt\"\nDoing so will run the workflow locally using a Docker container.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html#on-cloud-infrastructure",
    "href": "user_guide/running_pipelines.html#on-cloud-infrastructure",
    "title": "Running pipelines",
    "section": "On cloud infrastructure",
    "text": "On cloud infrastructure\nYou can use a similar command to run the workflow on cloud infrastructure, such as AWS Batch or Google Cloud Platform. However, this requires you to create a separate Nextflow config file for each cloud provider. See the Nextflow documentation for more information.\nnextflow run openpipelines-bio/openpipeline \\\n  -main-script target/nextflow/workflows/integration/multimodal_integration/main.nf \\\n  -revision 0.12.1 \\\n  -latest \\\n  --publish_dir foo/ \\\n  --input \"bar\" \\\n  --output \"test.txt\" \\\n  -c configs/my_hpc.config",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html#using-the-nextflow-tower-cli",
    "href": "user_guide/running_pipelines.html#using-the-nextflow-tower-cli",
    "title": "Running pipelines",
    "section": "Using the Nextflow Tower CLI",
    "text": "Using the Nextflow Tower CLI\nIf you have access to a Nextflow Tower instance in which a Compute Environment has already been set up, you can run a workflow from the Tower CLI. The command is very similar to the command to run a workflow from the CLI, but you need to: * Use tw launch instead of nextflow run * Specify the workspace ID and compute environment ID * Rename arguments: -revision to --revision, -latest to --pull-latest, -main-script to --main-script, -c to --config * Store workflow arguments in a separate yaml file (if this was not already the case).\nExample:\ntw launch openpipelines-bio/openpipeline \\\n  --revision 0.12.1 \\\n  --pull-latest \\\n  --main-script target/nextflow/workflows/integration/multimodal_integration/main.nf \\\n  --workspace &lt;your workspace id&gt; \\\n  --compute-env &lt;your compute environment id&gt; \\\n  --params-file params.yaml \\\n  --config configs/my_hpc.config",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  },
  {
    "objectID": "user_guide/running_pipelines.html#using-the-nextflow-tower-web-ui",
    "href": "user_guide/running_pipelines.html#using-the-nextflow-tower-web-ui",
    "title": "Running pipelines",
    "section": "Using the Nextflow Tower Web UI",
    "text": "Using the Nextflow Tower Web UI\nIf you have access to a Nextflow Tower instance in which a Compute Environment has already been set up, you can run a workflow from the Tower UI. To do so, go to the “Launchpad” and click on the button “launch a run without configuration”.\n\nNext, fill in the required fields and click on “Launch run”.\n\nCompute environment: Select the compute environment you want to run the workflow on.\nPipeline to launch: Fill in openpipelines-bio/openpipeline.\nRevision number: The release number of the pipeline you want to run, e.g. 0.12.1. You can find the release number on the GitHub releases page.\nWork directory: The bucket path where the scratch data is stored.\nPipeline parameters: The YAML or JSON of the parameters that are passed to the pipeline. See the Components page for more information about the parameters of each pipeline.",
    "crumbs": [
      "Fundamentals",
      "User guide",
      "Running pipelines"
    ]
  }
]